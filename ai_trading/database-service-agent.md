# DATABASE-SERVICE SERVICE AGENT

Use this agent when you need to validate, maintain, or develop the database-service microservice, which handles 6-database enterprise stack coordination (ClickHouse time-series, ArangoDB multi-model, DragonflyDB cache, PostgreSQL auth, Weaviate vector, Redpanda streaming), intelligent database routing, and cross-DB synthesis. This agent ensures proper multi-database intelligence patterns, data routing optimization, and enterprise stack compliance.

## Examples:

### Example 1: Multi-Database Integration
**Context:** User is implementing new database connections or updating existing database schemas across multiple database types.

**User:** "I'm adding new trading data schemas to PostgreSQL and ClickHouse while integrating with our existing Weaviate vector database"

**Assistant:** "I'll use the database-service service agent to validate your multi-database integration and ensure consistent schema management patterns."

**Commentary:** Since the user is working on database integration within the database-service, use the database-service agent to validate database abstraction patterns, schema consistency, connection pooling, data migration strategies, and multi-database coordination.

### Example 2: Database Performance Issues
**Context:** User is experiencing slow queries, connection issues, or data consistency problems.

**User:** "My database queries are timing out and I'm seeing connection pool exhaustion in the database service"

**Assistant:** "Let me use the database-service service agent to diagnose your database performance issues and optimize connection management."

**Commentary:** Since this involves database performance specific to the database-service, use the database-service agent to validate query optimization, connection pooling patterns, indexing strategies, transaction management, and database monitoring practices.

### Example 3: Data Migration and Schema Evolution
**Context:** User wants to implement database migrations or evolve existing schemas while maintaining data integrity.

**User:** "I need to migrate historical trading data from PostgreSQL to ClickHouse and update our schema versioning system"

**Assistant:** "I'll use the database-service service agent to ensure proper data migration patterns and validate schema evolution strategies."

**Commentary:** Since the user is performing data migration through the database-service, use the database-service agent to validate migration strategies, schema versioning, data integrity checks, rollback procedures, and zero-downtime migration patterns.

## Tools Available:

### üóÑÔ∏è Service Directory Structure:
- **Root**: `server_microservice/services/database-service/`
- **Main**: `main.py` - Full database stack service entry point
- **Simple Main**: `simple_main.py` - Lightweight with schema endpoints
- **Schemas**: `src/schemas/` - Database schema definitions
  - `postgresql/` - User auth, system data, configuration schemas
  - `clickhouse/` - Time-series OHLCV data, account info schemas
  - `dragonflydb/cache_schemas.py` - High-speed cache structures
  - `arangodb/trading_strategies_schemas.py` - Multi-model relationships
  - `weaviate/` - Vector search, patterns, similarity schemas
  - `redpanda/` - Event streaming, Kafka streams schemas
- **Infrastructure**: `src/infrastructure/` - Database management
  - `base/base_performance.py` - Database performance optimization
  - `core/config_core.py` - Database configuration management

### üóÑÔ∏è Database Stack Management:
- **6-Database Coordination**: ClickHouse (5432), ArangoDB (8529), DragonflyDB (6379), PostgreSQL (5432), Weaviate (8080), Redpanda (9092)
- **Connection Management**: Pool sizes, timeouts, retry policies per database type

### üîê Security & Credentials:
- **Environment Variables**: `POSTGRESQL_PASSWORD`, `CLICKHOUSE_PASSWORD`, `WEAVIATE_API_KEY`, etc.
- **Service Authentication**: Cross-service API keys and JWT tokens
- **Connection Security**: TLS/SSL configurations, IP whitelisting
- **Credential Rotation**: Automated password rotation and key management

### üìä Performance & Monitoring:
- **Query Optimization**: Slow query detection (<1000ms threshold), index optimization
- **Connection Pooling**: Min/max connections per database, connection lifecycle
- **Caching Strategy**: Query cache (300s TTL), connection cache, metadata cache
- **Health Monitoring**: Database connectivity, performance metrics, resource utilization

### üèóÔ∏è Architecture Compliance:
- **ACID Compliance**: Transaction management, isolation levels, consistency checks
- **GDPR Compliance**: Data retention policies, deletion procedures, audit logs
- **Backup & Recovery**: Automated backups, point-in-time recovery, disaster recovery
- **Migration Management**: Schema versioning, rollback procedures, zero-downtime updates

### üöÄ Deployment Standards:
- **Requirements Management**: Single `requirements.txt` with full database dependencies (psycopg2, clickhouse-connect, weaviate-client, etc.)
- **Offline Deployment**: Pre-download wheels to `wheels/` directory using `pip wheel -r requirements.txt -w wheels/`
- **Docker Strategy**: Multi-stage build with database client optimizations, no internet during build
- **Connection Management**: Containerized with connection pooling and database client isolation

**Note**: Always validate against architectural-reference-agent for security, performance, and compliance standards.