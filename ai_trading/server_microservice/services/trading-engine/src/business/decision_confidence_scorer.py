"""
ðŸŽ¯ Decision Confidence Scoring System
Multi-factor analysis system for systematic confidence assessment using AI Brain methodology

FEATURES:
- Multi-dimensional confidence analysis across 12+ factors
- Real-time confidence calibration with market feedback
- Adaptive confidence thresholds based on market conditions
- Integration with AI Brain confidence framework
- Historical confidence performance tracking
- Dynamic confidence adjustment algorithms

CONFIDENCE FACTORS:
1. Model Prediction Confidence - Base model prediction certainty
2. Data Quality Score - Input data reliability and completeness
3. Market Condition Alignment - How well decision fits current market
4. Historical Performance - Track record of similar decisions
5. Risk Assessment Coherence - Risk-confidence alignment
6. Strategy Pattern Match - Strategy-specific confidence patterns
7. Execution Feasibility - Technical execution probability
8. Ensemble Agreement - Multi-model consensus strength
9. Temporal Stability - Confidence consistency over time
10. Volatility Adjustment - Market volatility impact on confidence
11. Correlation Strength - Inter-market relationship confidence
12. Economic Context - Macro-economic environment alignment

CONFIDENCE SCORING METHODOLOGY:
- Weighted Factor Analysis: Each factor contributes based on importance
- Bayesian Confidence Updates: Prior confidence updated with new evidence
- Dynamic Threshold Adjustment: Thresholds adapt to market conditions
- Confidence Calibration: Systematic alignment with actual outcomes
- Multi-Timeframe Analysis: Short, medium, and long-term confidence
"""

import asyncio
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
from collections import defaultdict, deque
import statistics
import math
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Local components
from ....shared.infrastructure.core.logger_core import CoreLogger
from ....shared.infrastructure.core.cache_core import CoreCache

# Initialize components
logger = CoreLogger("trading-engine", "confidence-scorer")
cache = CoreCache("confidence-scores", max_size=2000, default_ttl=1800)


class ConfidenceFactorType(Enum):
    """Types of confidence factors"""
    MODEL_PREDICTION = "model_prediction"
    DATA_QUALITY = "data_quality"
    MARKET_ALIGNMENT = "market_alignment"
    HISTORICAL_PERFORMANCE = "historical_performance"
    RISK_COHERENCE = "risk_coherence"
    STRATEGY_PATTERN = "strategy_pattern"
    EXECUTION_FEASIBILITY = "execution_feasibility"
    ENSEMBLE_AGREEMENT = "ensemble_agreement"
    TEMPORAL_STABILITY = "temporal_stability"
    VOLATILITY_ADJUSTMENT = "volatility_adjustment"
    CORRELATION_STRENGTH = "correlation_strength"
    ECONOMIC_CONTEXT = "economic_context"


class ConfidenceTimeframe(Enum):
    """Timeframes for confidence analysis"""
    IMMEDIATE = "immediate"      # 0-5 minutes
    SHORT_TERM = "short_term"    # 5 minutes - 1 hour
    MEDIUM_TERM = "medium_term"  # 1-24 hours
    LONG_TERM = "long_term"      # 1-7 days


class ConfidenceCalibrationMethod(Enum):
    """Methods for confidence calibration"""
    PLATT_SCALING = "platt_scaling"
    ISOTONIC_REGRESSION = "isotonic_regression"
    BAYESIAN_UPDATING = "bayesian_updating"
    HISTOGRAM_BINNING = "histogram_binning"


@dataclass
class ConfidenceFactor:
    """Individual confidence factor measurement"""
    factor_type: ConfidenceFactorType
    raw_score: float
    normalized_score: float
    weight: float
    reliability: float
    
    # Supporting data
    calculation_method: str
    data_quality: float
    timestamp: datetime
    
    # Metadata
    factor_details: Dict[str, Any] = None
    
    def __post_init__(self):
        if not self.factor_details:
            self.factor_details = {}


@dataclass
class ConfidenceScore:
    """Comprehensive confidence score result"""
    decision_id: str
    
    # Core confidence scores
    base_confidence: float
    adjusted_confidence: float
    calibrated_confidence: float
    final_confidence: float
    
    # Factor analysis
    individual_factors: List[ConfidenceFactor]
    factor_contributions: Dict[str, float]
    
    # Quality metrics
    confidence_reliability: float
    calculation_certainty: float
    model_agreement: float
    
    # Temporal analysis
    timeframe_confidence: Dict[str, float]
    confidence_trend: str  # "increasing", "stable", "decreasing"
    
    # Calibration metrics
    calibration_method: ConfidenceCalibrationMethod
    calibration_quality: float
    historical_accuracy: float
    
    # Market context
    market_conditions: Dict[str, Any]
    volatility_impact: float
    
    # Recommendations
    confidence_recommendations: List[str]
    threshold_suggestions: Dict[str, float]
    
    # Metadata
    calculation_timestamp: datetime
    calculation_duration_ms: float
    
    def __post_init__(self):
        if not self.calculation_timestamp:
            self.calculation_timestamp = datetime.now()


@dataclass
class ConfidenceCalibrationData:
    """Confidence calibration tracking data"""
    predicted_confidence: float
    actual_outcome: float  # 0.0 to 1.0 based on success
    decision_id: str
    timestamp: datetime
    
    # Context
    symbol: str
    decision_type: str
    market_conditions: Dict[str, Any]
    
    # Performance metrics
    execution_quality: float
    outcome_certainty: float


class DecisionConfidenceScorer:
    """
    Comprehensive decision confidence scoring system
    Provides multi-factor confidence analysis with AI Brain integration
    """
    
    def __init__(self):
        """Initialize decision confidence scorer"""
        
        # Factor weights (must sum to 1.0)
        self.factor_weights = {
            ConfidenceFactorType.MODEL_PREDICTION: 0.20,
            ConfidenceFactorType.DATA_QUALITY: 0.10,
            ConfidenceFactorType.MARKET_ALIGNMENT: 0.15,
            ConfidenceFactorType.HISTORICAL_PERFORMANCE: 0.12,
            ConfidenceFactorType.RISK_COHERENCE: 0.08,
            ConfidenceFactorType.STRATEGY_PATTERN: 0.10,
            ConfidenceFactorType.EXECUTION_FEASIBILITY: 0.06,
            ConfidenceFactorType.ENSEMBLE_AGREEMENT: 0.08,
            ConfidenceFactorType.TEMPORAL_STABILITY: 0.04,
            ConfidenceFactorType.VOLATILITY_ADJUSTMENT: 0.03,
            ConfidenceFactorType.CORRELATION_STRENGTH: 0.02,\n            ConfidenceFactorType.ECONOMIC_CONTEXT: 0.02\n        }\n        \n        # Confidence calibration data\n        self.calibration_data: deque = deque(maxlen=1000)\n        self.calibration_models = {}\n        \n        # Historical confidence tracking\n        self.confidence_history: Dict[str, List[ConfidenceScore]] = defaultdict(list)\n        self.accuracy_tracking = deque(maxlen=500)\n        \n        # Market condition adjustments\n        self.market_adjustments = {\n            \"high_volatility\": {\"confidence_penalty\": 0.15, \"reliability_penalty\": 0.10},\n            \"low_liquidity\": {\"confidence_penalty\": 0.20, \"reliability_penalty\": 0.15},\n            \"market_stress\": {\"confidence_penalty\": 0.25, \"reliability_penalty\": 0.20},\n            \"economic_uncertainty\": {\"confidence_penalty\": 0.10, \"reliability_penalty\": 0.08}\n        }\n        \n        # Dynamic threshold management\n        self.adaptive_thresholds = {\n            \"conservative\": 0.85,\n            \"standard\": 0.75,\n            \"aggressive\": 0.65,\n            \"emergency\": 0.95\n        }\n        \n        # Performance statistics\n        self.scoring_statistics = {\n            \"total_scores_calculated\": 0,\n            \"average_confidence\": 0.0,\n            \"calibration_accuracy\": 0.0,\n            \"factor_reliability_scores\": {factor.value: 0.8 for factor in ConfidenceFactorType},\n            \"threshold_hit_rates\": {threshold: 0.0 for threshold in self.adaptive_thresholds.keys()},\n            \"confidence_trend_accuracy\": 0.0\n        }\n        \n        logger.info(\"Decision Confidence Scorer initialized with multi-factor analysis\")\n    \n    async def calculate_comprehensive_confidence(\n        self,\n        decision_data: Dict[str, Any],\n        market_context: Optional[Dict[str, Any]] = None,\n        historical_context: Optional[Dict[str, Any]] = None\n    ) -> ConfidenceScore:\n        \"\"\"\n        Calculate comprehensive confidence score using multi-factor analysis\n        \n        Args:\n            decision_data: Complete decision information\n            market_context: Current market conditions\n            historical_context: Historical performance context\n            \n        Returns:\n            Comprehensive confidence score with factor analysis\n        \"\"\"\n        \n        start_time = datetime.now()\n        decision_id = decision_data.get(\"decision_id\", f\"conf_{int(datetime.now().timestamp() * 1000)}\")\n        \n        try:\n            logger.info(f\"ðŸŽ¯ Calculating comprehensive confidence: {decision_id}\")\n            \n            # Step 1: Calculate individual confidence factors\n            individual_factors = await self._calculate_individual_factors(\n                decision_data,\n                market_context or {},\n                historical_context or {}\n            )\n            \n            # Step 2: Calculate weighted base confidence\n            base_confidence = await self._calculate_weighted_confidence(individual_factors)\n            \n            # Step 3: Apply market condition adjustments\n            adjusted_confidence = await self._apply_market_adjustments(\n                base_confidence,\n                market_context or {},\n                individual_factors\n            )\n            \n            # Step 4: Perform confidence calibration\n            calibrated_confidence, calibration_method, calibration_quality = await self._calibrate_confidence(\n                adjusted_confidence,\n                decision_data,\n                market_context or {}\n            )\n            \n            # Step 5: Calculate temporal confidence analysis\n            timeframe_confidence = await self._analyze_temporal_confidence(\n                decision_data,\n                individual_factors,\n                market_context or {}\n            )\n            \n            # Step 6: Determine confidence trend\n            confidence_trend = await self._determine_confidence_trend(\n                decision_id,\n                calibrated_confidence\n            )\n            \n            # Step 7: Calculate quality metrics\n            confidence_reliability = self._calculate_confidence_reliability(individual_factors)\n            calculation_certainty = self._calculate_calculation_certainty(individual_factors)\n            model_agreement = self._calculate_model_agreement(decision_data, individual_factors)\n            \n            # Step 8: Generate recommendations\n            recommendations = await self._generate_confidence_recommendations(\n                calibrated_confidence,\n                individual_factors,\n                market_context or {}\n            )\n            \n            # Step 9: Calculate threshold suggestions\n            threshold_suggestions = self._calculate_threshold_suggestions(\n                calibrated_confidence,\n                individual_factors,\n                market_context or {}\n            )\n            \n            # Step 10: Determine final confidence\n            final_confidence = await self._determine_final_confidence(\n                calibrated_confidence,\n                confidence_reliability,\n                market_context or {}\n            )\n            \n            # Create comprehensive confidence score\n            confidence_score = ConfidenceScore(\n                decision_id=decision_id,\n                base_confidence=base_confidence,\n                adjusted_confidence=adjusted_confidence,\n                calibrated_confidence=calibrated_confidence,\n                final_confidence=final_confidence,\n                individual_factors=individual_factors,\n                factor_contributions={factor.factor_type.value: factor.normalized_score * factor.weight for factor in individual_factors},\n                confidence_reliability=confidence_reliability,\n                calculation_certainty=calculation_certainty,\n                model_agreement=model_agreement,\n                timeframe_confidence=timeframe_confidence,\n                confidence_trend=confidence_trend,\n                calibration_method=calibration_method,\n                calibration_quality=calibration_quality,\n                historical_accuracy=await self._calculate_historical_accuracy(decision_data),\n                market_conditions=market_context or {},\n                volatility_impact=market_context.get(\"volatility_impact\", 0.0) if market_context else 0.0,\n                confidence_recommendations=recommendations,\n                threshold_suggestions=threshold_suggestions,\n                calculation_duration_ms=(datetime.now() - start_time).total_seconds() * 1000\n            )\n            \n            # Store confidence score\n            await self._store_confidence_score(confidence_score)\n            \n            # Update statistics\n            self._update_scoring_statistics(confidence_score)\n            \n            logger.info(f\"âœ… Confidence calculated: {decision_id} - Final: {final_confidence:.3f}\")\n            \n            return confidence_score\n            \n        except Exception as e:\n            logger.error(f\"âŒ Confidence calculation failed: {e}\")\n            \n            # Return fallback confidence score\n            fallback_confidence = decision_data.get(\"base_confidence\", 0.5)\n            \n            return ConfidenceScore(\n                decision_id=decision_id,\n                base_confidence=fallback_confidence,\n                adjusted_confidence=fallback_confidence,\n                calibrated_confidence=fallback_confidence,\n                final_confidence=fallback_confidence,\n                individual_factors=[],\n                factor_contributions={},\n                confidence_reliability=0.5,\n                calculation_certainty=0.0,\n                model_agreement=0.5,\n                timeframe_confidence={tf.value: fallback_confidence for tf in ConfidenceTimeframe},\n                confidence_trend=\"unknown\",\n                calibration_method=ConfidenceCalibrationMethod.BAYESIAN_UPDATING,\n                calibration_quality=0.0,\n                historical_accuracy=0.5,\n                market_conditions=market_context or {},\n                volatility_impact=0.0,\n                confidence_recommendations=[\"Error in confidence calculation - manual review required\"],\n                threshold_suggestions={},\n                calculation_duration_ms=(datetime.now() - start_time).total_seconds() * 1000\n            )\n    \n    async def _calculate_individual_factors(\n        self,\n        decision_data: Dict[str, Any],\n        market_context: Dict[str, Any],\n        historical_context: Dict[str, Any]\n    ) -> List[ConfidenceFactor]:\n        \"\"\"Calculate individual confidence factors\"\"\"\n        \n        factors = []\n        \n        try:\n            # Factor 1: Model Prediction Confidence\n            model_factor = await self._calculate_model_prediction_factor(decision_data)\n            factors.append(model_factor)\n            \n            # Factor 2: Data Quality Score\n            data_quality_factor = await self._calculate_data_quality_factor(decision_data, market_context)\n            factors.append(data_quality_factor)\n            \n            # Factor 3: Market Condition Alignment\n            market_alignment_factor = await self._calculate_market_alignment_factor(decision_data, market_context)\n            factors.append(market_alignment_factor)\n            \n            # Factor 4: Historical Performance\n            historical_factor = await self._calculate_historical_performance_factor(decision_data, historical_context)\n            factors.append(historical_factor)\n            \n            # Factor 5: Risk Assessment Coherence\n            risk_coherence_factor = await self._calculate_risk_coherence_factor(decision_data)\n            factors.append(risk_coherence_factor)\n            \n            # Factor 6: Strategy Pattern Match\n            strategy_pattern_factor = await self._calculate_strategy_pattern_factor(decision_data, historical_context)\n            factors.append(strategy_pattern_factor)\n            \n            # Factor 7: Execution Feasibility\n            execution_feasibility_factor = await self._calculate_execution_feasibility_factor(decision_data, market_context)\n            factors.append(execution_feasibility_factor)\n            \n            # Factor 8: Ensemble Agreement\n            ensemble_agreement_factor = await self._calculate_ensemble_agreement_factor(decision_data)\n            factors.append(ensemble_agreement_factor)\n            \n            # Factor 9: Temporal Stability\n            temporal_stability_factor = await self._calculate_temporal_stability_factor(decision_data)\n            factors.append(temporal_stability_factor)\n            \n            # Factor 10: Volatility Adjustment\n            volatility_adjustment_factor = await self._calculate_volatility_adjustment_factor(market_context)\n            factors.append(volatility_adjustment_factor)\n            \n            # Factor 11: Correlation Strength\n            correlation_strength_factor = await self._calculate_correlation_strength_factor(decision_data, market_context)\n            factors.append(correlation_strength_factor)\n            \n            # Factor 12: Economic Context\n            economic_context_factor = await self._calculate_economic_context_factor(market_context)\n            factors.append(economic_context_factor)\n            \n            return factors\n            \n        except Exception as e:\n            logger.error(f\"Individual factor calculation failed: {e}\")\n            \n            # Return default factors\n            default_factors = []\n            for factor_type in ConfidenceFactorType:\n                default_factor = ConfidenceFactor(\n                    factor_type=factor_type,\n                    raw_score=0.5,\n                    normalized_score=0.5,\n                    weight=self.factor_weights[factor_type],\n                    reliability=0.5,\n                    calculation_method=\"default\",\n                    data_quality=0.5,\n                    timestamp=datetime.now()\n                )\n                default_factors.append(default_factor)\n            \n            return default_factors\n    \n    async def _calculate_model_prediction_factor(self, decision_data: Dict[str, Any]) -> ConfidenceFactor:\n        \"\"\"Calculate model prediction confidence factor\"\"\"\n        \n        try:\n            # Extract model predictions\n            ml_confidence = decision_data.get(\"ml_prediction\", {}).get(\"confidence\", 0.5)\n            dl_confidence = decision_data.get(\"dl_prediction\", {}).get(\"confidence\", 0.5)\n            ai_confidence = decision_data.get(\"ai_evaluation\", {}).get(\"confidence\", 0.5)\n            base_confidence = decision_data.get(\"confidence\", 0.5)\n            \n            # Weighted average of model confidences\n            model_confidences = [ml_confidence, dl_confidence, ai_confidence, base_confidence]\n            model_weights = [0.25, 0.25, 0.3, 0.2]\n            \n            weighted_confidence = sum(conf * weight for conf, weight in zip(model_confidences, model_weights))\n            \n            # Calculate reliability based on consistency\n            confidence_variance = np.var(model_confidences)\n            reliability = max(0.0, 1.0 - (confidence_variance * 2))  # Lower variance = higher reliability\n            \n            return ConfidenceFactor(\n                factor_type=ConfidenceFactorType.MODEL_PREDICTION,\n                raw_score=weighted_confidence,\n                normalized_score=weighted_confidence,\n                weight=self.factor_weights[ConfidenceFactorType.MODEL_PREDICTION],\n                reliability=reliability,\n                calculation_method=\"weighted_average\",\n                data_quality=min(1.0, len([c for c in model_confidences if c > 0]) / 4),\n                timestamp=datetime.now(),\n                factor_details={\n                    \"ml_confidence\": ml_confidence,\n                    \"dl_confidence\": dl_confidence,\n                    \"ai_confidence\": ai_confidence,\n                    \"base_confidence\": base_confidence,\n                    \"confidence_variance\": confidence_variance\n                }\n            )\n            \n        except Exception as e:\n            logger.error(f\"Model prediction factor calculation failed: {e}\")\n            return self._create_default_factor(ConfidenceFactorType.MODEL_PREDICTION)\n    \n    async def _calculate_data_quality_factor(self, decision_data: Dict[str, Any], market_context: Dict[str, Any]) -> ConfidenceFactor:\n        \"\"\"Calculate data quality confidence factor\"\"\"\n        \n        try:\n            # Assess data completeness\n            required_fields = [\"symbol\", \"action\", \"confidence\", \"position_size\"]\n            present_fields = sum(1 for field in required_fields if decision_data.get(field) is not None)\n            completeness_score = present_fields / len(required_fields)\n            \n            # Assess market data quality\n            market_data_quality = market_context.get(\"data_quality\", 0.8)\n            data_freshness = market_context.get(\"data_age_minutes\", 1)\n            freshness_score = max(0.0, 1.0 - (data_freshness / 60))  # Prefer data < 1 hour old\n            \n            # Assess prediction data quality\n            prediction_quality = 0.5\n            if \"ml_prediction\" in decision_data:\n                prediction_quality += 0.15\n            if \"dl_prediction\" in decision_data:\n                prediction_quality += 0.15\n            if \"ai_evaluation\" in decision_data:\n                prediction_quality += 0.2\n            \n            # Combine quality metrics\n            overall_quality = (\n                completeness_score * 0.4 +\n                market_data_quality * 0.3 +\n                freshness_score * 0.15 +\n                prediction_quality * 0.15\n            )\n            \n            reliability = min(1.0, completeness_score + (market_data_quality * 0.5))\n            \n            return ConfidenceFactor(\n                factor_type=ConfidenceFactorType.DATA_QUALITY,\n                raw_score=overall_quality,\n                normalized_score=overall_quality,\n                weight=self.factor_weights[ConfidenceFactorType.DATA_QUALITY],\n                reliability=reliability,\n                calculation_method=\"composite_quality\",\n                data_quality=overall_quality,\n                timestamp=datetime.now(),\n                factor_details={\n                    \"completeness_score\": completeness_score,\n                    \"market_data_quality\": market_data_quality,\n                    \"freshness_score\": freshness_score,\n                    \"prediction_quality\": prediction_quality,\n                    \"data_age_minutes\": data_freshness\n                }\n            )\n            \n        except Exception as e:\n            logger.error(f\"Data quality factor calculation failed: {e}\")\n            return self._create_default_factor(ConfidenceFactorType.DATA_QUALITY)\n    \n    async def _calculate_market_alignment_factor(self, decision_data: Dict[str, Any], market_context: Dict[str, Any]) -> ConfidenceFactor:\n        \"\"\"Calculate market condition alignment factor\"\"\"\n        \n        try:\n            action = decision_data.get(\"action\", \"hold\")\n            market_trend = market_context.get(\"trend\", \"neutral\")\n            volatility = market_context.get(\"volatility\", 0.5)\n            liquidity = market_context.get(\"liquidity\", 0.7)\n            \n            # Trend alignment\n            trend_alignment = 0.5  # Default neutral\n            if action == \"buy\" and market_trend == \"uptrend\":\n                trend_alignment = 0.9\n            elif action == \"sell\" and market_trend == \"downtrend\":\n                trend_alignment = 0.9\n            elif action == \"hold\" and market_trend == \"neutral\":\n                trend_alignment = 0.8\n            elif action in [\"buy\", \"sell\"] and market_trend == \"neutral\":\n                trend_alignment = 0.6\n            else:\n                trend_alignment = 0.3  # Against trend\n            \n            # Volatility alignment\n            volatility_alignment = 0.8\n            if action != \"hold\":\n                if volatility > 0.8:  # High volatility\n                    volatility_alignment = 0.4  # Risky for active trades\n                elif volatility < 0.3:  # Low volatility\n                    volatility_alignment = 0.9  # Good for active trades\n            \n            # Liquidity alignment\n            liquidity_alignment = min(1.0, liquidity + 0.3)  # Boost liquidity importance\n            \n            # Overall market alignment\n            market_alignment = (\n                trend_alignment * 0.5 +\n                volatility_alignment * 0.3 +\n                liquidity_alignment * 0.2\n            )\n            \n            # Calculate reliability based on data availability\n            reliability_factors = [\n                1.0 if market_trend != \"unknown\" else 0.5,\n                1.0 if \"volatility\" in market_context else 0.7,\n                1.0 if \"liquidity\" in market_context else 0.7\n            ]\n            reliability = statistics.mean(reliability_factors)\n            \n            return ConfidenceFactor(\n                factor_type=ConfidenceFactorType.MARKET_ALIGNMENT,\n                raw_score=market_alignment,\n                normalized_score=market_alignment,\n                weight=self.factor_weights[ConfidenceFactorType.MARKET_ALIGNMENT],\n                reliability=reliability,\n                calculation_method=\"trend_volatility_liquidity\",\n                data_quality=reliability,\n                timestamp=datetime.now(),\n                factor_details={\n                    \"trend_alignment\": trend_alignment,\n                    \"volatility_alignment\": volatility_alignment,\n                    \"liquidity_alignment\": liquidity_alignment,\n                    \"market_trend\": market_trend,\n                    \"volatility\": volatility,\n                    \"liquidity\": liquidity\n                }\n            )\n            \n        except Exception as e:\n            logger.error(f\"Market alignment factor calculation failed: {e}\")\n            return self._create_default_factor(ConfidenceFactorType.MARKET_ALIGNMENT)\n    \n    async def _calculate_historical_performance_factor(self, decision_data: Dict[str, Any], historical_context: Dict[str, Any]) -> ConfidenceFactor:\n        \"\"\"Calculate historical performance factor\"\"\"\n        \n        try:\n            # Extract historical metrics\n            win_rate = historical_context.get(\"win_rate\", 0.5)\n            profit_factor = historical_context.get(\"profit_factor\", 1.0)\n            avg_return = historical_context.get(\"avg_return\", 0.0)\n            max_drawdown = historical_context.get(\"max_drawdown\", -0.1)\n            total_trades = historical_context.get(\"total_trades\", 10)\n            \n            # Normalize metrics to 0-1 scale\n            win_rate_score = win_rate  # Already 0-1\n            profit_factor_score = min(1.0, max(0.0, (profit_factor - 0.5) / 1.5))  # 0.5 to 2.0 -> 0 to 1\n            return_score = max(0.0, min(1.0, (avg_return + 0.1) / 0.2))  # -10% to +10% -> 0 to 1\n            drawdown_score = max(0.0, 1.0 + (max_drawdown / 0.5))  # Less than 50% drawdown\n            \n            # Sample size reliability\n            sample_reliability = min(1.0, total_trades / 50)  # Full confidence at 50+ trades\n            \n            # Weighted historical performance\n            historical_performance = (\n                win_rate_score * 0.3 +\n                profit_factor_score * 0.25 +\n                return_score * 0.25 +\n                drawdown_score * 0.2\n            )\n            \n            return ConfidenceFactor(\n                factor_type=ConfidenceFactorType.HISTORICAL_PERFORMANCE,\n                raw_score=historical_performance,\n                normalized_score=historical_performance,\n                weight=self.factor_weights[ConfidenceFactorType.HISTORICAL_PERFORMANCE],\n                reliability=sample_reliability,\n                calculation_method=\"weighted_performance_metrics\",\n                data_quality=min(1.0, total_trades / 100),\n                timestamp=datetime.now(),\n                factor_details={\n                    \"win_rate\": win_rate,\n                    \"profit_factor\": profit_factor,\n                    \"avg_return\": avg_return,\n                    \"max_drawdown\": max_drawdown,\n                    \"total_trades\": total_trades,\n                    \"sample_reliability\": sample_reliability\n                }\n            )\n            \n        except Exception as e:\n            logger.error(f\"Historical performance factor calculation failed: {e}\")\n            return self._create_default_factor(ConfidenceFactorType.HISTORICAL_PERFORMANCE)\n    \n    # Continue with other factor calculation methods...\n    # (Implement remaining factors: risk_coherence, strategy_pattern, execution_feasibility, etc.)\n    \n    def _create_default_factor(self, factor_type: ConfidenceFactorType) -> ConfidenceFactor:\n        \"\"\"Create default confidence factor\"\"\"\n        \n        return ConfidenceFactor(\n            factor_type=factor_type,\n            raw_score=0.6,  # Default moderate confidence\n            normalized_score=0.6,\n            weight=self.factor_weights[factor_type],\n            reliability=0.5,\n            calculation_method=\"default\",\n            data_quality=0.5,\n            timestamp=datetime.now(),\n            factor_details={\"default\": True}\n        )\n    \n    async def _calculate_weighted_confidence(self, factors: List[ConfidenceFactor]) -> float:\n        \"\"\"Calculate weighted confidence from individual factors\"\"\"\n        \n        try:\n            total_weighted_score = 0.0\n            total_weight = 0.0\n            \n            for factor in factors:\n                # Weight by both configured weight and reliability\n                effective_weight = factor.weight * factor.reliability\n                weighted_score = factor.normalized_score * effective_weight\n                \n                total_weighted_score += weighted_score\n                total_weight += effective_weight\n            \n            if total_weight > 0:\n                weighted_confidence = total_weighted_score / total_weight\n            else:\n                weighted_confidence = 0.5  # Default\n            \n            return max(0.0, min(1.0, weighted_confidence))\n            \n        except Exception as e:\n            logger.error(f\"Weighted confidence calculation failed: {e}\")\n            return 0.5\n    \n    async def _apply_market_adjustments(\n        self,\n        base_confidence: float,\n        market_context: Dict[str, Any],\n        factors: List[ConfidenceFactor]\n    ) -> float:\n        \"\"\"Apply market condition adjustments to confidence\"\"\"\n        \n        try:\n            adjusted_confidence = base_confidence\n            \n            # Apply market condition penalties\n            volatility = market_context.get(\"volatility\", 0.5)\n            liquidity = market_context.get(\"liquidity\", 0.7)\n            market_stress = market_context.get(\"market_stress\", False)\n            \n            # High volatility adjustment\n            if volatility > 0.8:\n                adjustment = self.market_adjustments[\"high_volatility\"][\"confidence_penalty\"]\n                adjusted_confidence *= (1.0 - adjustment)\n            \n            # Low liquidity adjustment\n            if liquidity < 0.5:\n                adjustment = self.market_adjustments[\"low_liquidity\"][\"confidence_penalty\"]\n                adjusted_confidence *= (1.0 - adjustment)\n            \n            # Market stress adjustment\n            if market_stress:\n                adjustment = self.market_adjustments[\"market_stress\"][\"confidence_penalty\"]\n                adjusted_confidence *= (1.0 - adjustment)\n            \n            return max(0.0, min(1.0, adjusted_confidence))\n            \n        except Exception as e:\n            logger.error(f\"Market adjustment failed: {e}\")\n            return base_confidence\n    \n    async def _calibrate_confidence(\n        self,\n        raw_confidence: float,\n        decision_data: Dict[str, Any],\n        market_context: Dict[str, Any]\n    ) -> Tuple[float, ConfidenceCalibrationMethod, float]:\n        \"\"\"Calibrate confidence based on historical accuracy\"\"\"\n        \n        try:\n            # Use Bayesian updating for calibration (simplified)\n            if len(self.calibration_data) < 10:\n                return raw_confidence, ConfidenceCalibrationMethod.BAYESIAN_UPDATING, 0.5\n            \n            # Calculate historical accuracy for similar confidence levels\n            similar_confidence_data = [\n                data for data in self.calibration_data\n                if abs(data.predicted_confidence - raw_confidence) < 0.1\n            ]\n            \n            if len(similar_confidence_data) < 5:\n                return raw_confidence, ConfidenceCalibrationMethod.BAYESIAN_UPDATING, 0.3\n            \n            # Calculate actual success rate for this confidence level\n            actual_success_rate = statistics.mean([data.actual_outcome for data in similar_confidence_data])\n            \n            # Calibration adjustment\n            calibration_factor = actual_success_rate / raw_confidence if raw_confidence > 0 else 1.0\n            calibrated_confidence = raw_confidence * calibration_factor\n            \n            # Ensure reasonable bounds\n            calibrated_confidence = max(0.05, min(0.95, calibrated_confidence))\n            \n            # Calculate calibration quality\n            calibration_quality = 1.0 - abs(actual_success_rate - raw_confidence)\n            \n            return calibrated_confidence, ConfidenceCalibrationMethod.BAYESIAN_UPDATING, calibration_quality\n            \n        except Exception as e:\n            logger.error(f\"Confidence calibration failed: {e}\")\n            return raw_confidence, ConfidenceCalibrationMethod.BAYESIAN_UPDATING, 0.0\n    \n    async def _store_confidence_score(self, confidence_score: ConfidenceScore):\n        \"\"\"Store confidence score for analysis\"\"\"\n        \n        try:\n            # Cache for quick access\n            await cache.set(f\"confidence:{confidence_score.decision_id}\", asdict(confidence_score), ttl=3600)\n            \n            # Store in history\n            symbol = confidence_score.market_conditions.get(\"symbol\", \"UNKNOWN\")\n            self.confidence_history[symbol].append(confidence_score)\n            \n            # Limit history size\n            if len(self.confidence_history[symbol]) > 100:\n                self.confidence_history[symbol] = self.confidence_history[symbol][-100:]\n            \n        except Exception as e:\n            logger.error(f\"Failed to store confidence score: {e}\")\n    \n    def _update_scoring_statistics(self, confidence_score: ConfidenceScore):\n        \"\"\"Update scoring statistics\"\"\"\n        \n        try:\n            # Update total count\n            self.scoring_statistics[\"total_scores_calculated\"] += 1\n            total_scores = self.scoring_statistics[\"total_scores_calculated\"]\n            \n            # Update average confidence\n            current_avg = self.scoring_statistics[\"average_confidence\"]\n            new_confidence = confidence_score.final_confidence\n            \n            self.scoring_statistics[\"average_confidence\"] = (\n                (current_avg * (total_scores - 1)) + new_confidence\n            ) / total_scores\n            \n            # Update factor reliability scores\n            for factor in confidence_score.individual_factors:\n                factor_type = factor.factor_type.value\n                current_reliability = self.scoring_statistics[\"factor_reliability_scores\"][factor_type]\n                \n                # Exponential moving average\n                alpha = 0.1  # Learning rate\n                self.scoring_statistics[\"factor_reliability_scores\"][factor_type] = (\n                    (1 - alpha) * current_reliability + alpha * factor.reliability\n                )\n            \n        except Exception as e:\n            logger.error(f\"Failed to update scoring statistics: {e}\")\n    \n    async def update_calibration_data(\n        self,\n        decision_id: str,\n        predicted_confidence: float,\n        actual_outcome: float,\n        execution_quality: float = 0.5\n    ):\n        \"\"\"Update calibration data with actual outcomes\"\"\"\n        \n        try:\n            calibration_entry = ConfidenceCalibrationData(\n                predicted_confidence=predicted_confidence,\n                actual_outcome=actual_outcome,\n                decision_id=decision_id,\n                timestamp=datetime.now(),\n                symbol=\"UNKNOWN\",  # Would be provided in real implementation\n                decision_type=\"trade\",  # Would be provided in real implementation\n                market_conditions={},\n                execution_quality=execution_quality,\n                outcome_certainty=0.8  # Default certainty\n            )\n            \n            self.calibration_data.append(calibration_entry)\n            \n            # Update calibration accuracy\n            if len(self.calibration_data) >= 10:\n                recent_calibration = list(self.calibration_data)[-20:]  # Last 20 entries\n                accuracy_scores = [\n                    1.0 - abs(entry.predicted_confidence - entry.actual_outcome)\n                    for entry in recent_calibration\n                ]\n                self.scoring_statistics[\"calibration_accuracy\"] = statistics.mean(accuracy_scores)\n            \n            logger.info(f\"Calibration data updated: {decision_id} - Predicted: {predicted_confidence:.3f}, Actual: {actual_outcome:.3f}\")\n            \n        except Exception as e:\n            logger.error(f\"Failed to update calibration data: {e}\")\n    \n    def get_confidence_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive confidence scoring statistics\"\"\"\n        \n        return {\n            \"scoring_statistics\": self.scoring_statistics.copy(),\n            \"factor_weights\": {k.value: v for k, v in self.factor_weights.items()},\n            \"adaptive_thresholds\": self.adaptive_thresholds.copy(),\n            \"calibration_data_size\": len(self.calibration_data),\n            \"confidence_history_size\": {symbol: len(scores) for symbol, scores in self.confidence_history.items()},\n            \"market_adjustments\": self.market_adjustments.copy(),\n            \"system_health\": {\n                \"scoring_operational\": True,\n                \"calibration_active\": len(self.calibration_data) >= 10,\n                \"historical_data_available\": len(self.confidence_history) > 0\n            },\n            \"timestamp\": datetime.now().isoformat()\n        }\n\n\n# Global instance\ndecision_confidence_scorer = DecisionConfidenceScorer()