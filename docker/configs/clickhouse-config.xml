<?xml version="1.0"?>
<!-- ClickHouse Configuration for AI Trading Platform -->
<!-- Phase 1: Optimized Log Storage with Security and Performance -->

<yandex>
    <!-- Server identification -->
    <display_name>AI Trading Platform - Log Storage</display_name>

    <!-- Logging configuration -->
    <logger>
        <level>information</level>
        <log>/var/log/clickhouse-server/clickhouse-server.log</log>
        <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>
        <size>1000M</size>
        <count>10</count>
        <console>1</console>
    </logger>

    <!-- Query logging -->
    <query_log>
        <database>system</database>
        <table>query_log</table>
        <ttl>event_date + INTERVAL 30 DAY</ttl>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
    </query_log>

    <!-- Part log for monitoring -->
    <part_log>
        <database>system</database>
        <table>part_log</table>
        <ttl>event_date + INTERVAL 30 DAY</ttl>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
    </part_log>

    <!-- Network configuration -->
    <listen_host>0.0.0.0</listen_host>
    <http_port>8123</http_port>
    <tcp_port>9000</tcp_port>
    <interserver_http_port>9009</interserver_http_port>

    <!-- Security settings -->
    <max_connections>4096</max_connections>
    <keep_alive_timeout>3</keep_alive_timeout>
    <max_concurrent_queries>100</max_concurrent_queries>
    <uncompressed_cache_size>8589934592</uncompressed_cache_size>
    <mark_cache_size>5368709120</mark_cache_size>

    <!-- Performance optimization for log storage -->
    <merge_tree>
        <max_suspicious_broken_parts>5</max_suspicious_broken_parts>
        <parts_to_delay_insert>150</parts_to_delay_insert>
        <parts_to_throw_insert>300</parts_to_throw_insert>
        <max_delay_to_insert>1</max_delay_to_insert>
        <max_parts_in_total>100000</max_parts_in_total>
        <merge_max_block_size>8192</merge_max_block_size>
        <max_bytes_to_merge_at_max_space_in_pool>161061273600</max_bytes_to_merge_at_max_space_in_pool>
        <max_replicated_merges_in_queue>16</max_replicated_merges_in_queue>
        <number_of_free_entries_in_pool_to_lower_max_size_of_merge>8</number_of_free_entries_in_pool_to_lower_max_size_of_merge>
        <max_number_of_merges_with_ttl_in_pool>2</max_number_of_merges_with_ttl_in_pool>
    </merge_tree>

    <!-- Storage policy for tiered storage -->
    <storage_configuration>
        <disks>
            <!-- Hot storage: High-performance SSD -->
            <hot>
                <type>local</type>
                <path>/var/lib/clickhouse/hot/</path>
                <keep_free_space_bytes>10737418240</keep_free_space_bytes>
            </hot>

            <!-- Warm storage: Standard SSD -->
            <warm>
                <type>local</type>
                <path>/var/lib/clickhouse/warm/</path>
                <keep_free_space_bytes>5368709120</keep_free_space_bytes>
            </warm>

            <!-- Cold storage: High compression -->
            <cold>
                <type>local</type>
                <path>/var/lib/clickhouse/cold/</path>
                <keep_free_space_bytes>2147483648</keep_free_space_bytes>
            </cold>
        </disks>

        <policies>
            <!-- Hot data policy -->
            <hot_policy>
                <volumes>
                    <hot_volume>
                        <disk>hot</disk>
                        <max_data_part_size_bytes>10737418240</max_data_part_size_bytes>
                    </hot_volume>
                </volumes>
            </hot_policy>

            <!-- Tiered policy for cost optimization -->
            <tiered_policy>
                <volumes>
                    <hot_volume>
                        <disk>hot</disk>
                        <max_data_part_size_bytes>1073741824</max_data_part_size_bytes>
                        <move_factor>0.1</move_factor>
                    </hot_volume>
                    <warm_volume>
                        <disk>warm</disk>
                        <max_data_part_size_bytes>5368709120</max_data_part_size_bytes>
                        <move_factor>0.1</move_factor>
                    </warm_volume>
                    <cold_volume>
                        <disk>cold</disk>
                    </cold_volume>
                </volumes>
                <move_factor>0.1</move_factor>
            </tiered_policy>
        </policies>
    </storage_configuration>

    <!-- Compression settings for cost optimization -->
    <compression>
        <case>
            <method>lz4</method>
            <min_part_size>10000000</min_part_size>
            <min_part_size_ratio>0.01</min_part_size_ratio>
        </case>
        <case>
            <method>zstd</method>
            <level>1</level>
            <min_part_size>100000000</min_part_size>
            <min_part_size_ratio>0.01</min_part_size_ratio>
        </case>
    </compression>

    <!-- Memory and performance settings -->
    <max_memory_usage>10000000000</max_memory_usage>
    <use_uncompressed_cache>1</use_uncompressed_cache>
    <load_balancing>random</load_balancing>

    <!-- Background processing threads -->
    <background_pool_size>16</background_pool_size>
    <background_merges_mutations_concurrency_ratio>2</background_merges_mutations_concurrency_ratio>
    <background_move_pool_size>8</background_move_pool_size>
    <background_schedule_pool_size>16</background_schedule_pool_size>

    <!-- Query complexity limits -->
    <max_query_size>268435456</max_query_size>
    <interactive_delay>1000000</interactive_delay>
    <connect_timeout>10</connect_timeout>
    <receive_timeout>300</receive_timeout>
    <send_timeout>300</send_timeout>

    <!-- Insert settings for log ingestion -->
    <max_insert_block_size>1048576</max_insert_block_size>
    <max_insert_threads>4</max_insert_threads>
    <max_block_size>65536</max_block_size>

    <!-- Timezone -->
    <timezone>UTC</timezone>

    <!-- Distributed processing -->
    <distributed_ddl>
        <path>/clickhouse/task_queue/ddl</path>
    </distributed_ddl>

    <!-- Format schemas path -->
    <format_schema_path>/var/lib/clickhouse/format_schemas/</format_schema_path>

    <!-- Dictionaries configuration -->
    <dictionaries_config>*_dictionary.xml</dictionaries_config>

    <!-- User files path -->
    <user_files_path>/var/lib/clickhouse/user_files/</user_files_path>

    <!-- Access control -->
    <access_control_path>/var/lib/clickhouse/access/</access_control_path>

    <!-- Top level domains lists -->
    <top_level_domains_path>/opt/clickhouse-server/share/top_level_domains/</top_level_domains_path>

    <!-- Custom settings for AI Trading Platform -->
    <custom_settings_prefixes>aitrading</custom_settings_prefixes>

    <!-- Prometheus metrics -->
    <prometheus>
        <endpoint>/metrics</endpoint>
        <port>9363</port>
        <metrics>true</metrics>
        <events>true</events>
        <asynchronous_metrics>true</asynchronous_metrics>
    </prometheus>

    <!-- OpenTelemetry configuration -->
    <opentelemetry>
        <trace_processors>
            <batch>
                <endpoint>http://jaeger:14268/api/traces</endpoint>
            </batch>
        </trace_processors>
    </opentelemetry>

    <!-- Security hardening -->
    <max_server_memory_usage_to_ram_ratio>0.9</max_server_memory_usage_to_ram_ratio>
    <mlock_executable>true</mlock_executable>

    <!-- Named collections for external data sources -->
    <named_collections>
        <aitrading_postgres>
            <host>postgres-main</host>
            <port>5432</port>
            <database>aitrading_prod</database>
            <schema>public</schema>
        </aitrading_postgres>
    </named_collections>

    <!-- Profiles for different use cases -->
    <profiles>
        <default>
            <max_memory_usage>10000000000</max_memory_usage>
            <use_uncompressed_cache>1</use_uncompressed_cache>
            <load_balancing>random</load_balancing>
            <max_query_size>268435456</max_query_size>
        </default>

        <!-- Profile for log ingestion -->
        <log_ingestion>
            <max_memory_usage>5000000000</max_memory_usage>
            <max_insert_block_size>1048576</max_insert_block_size>
            <max_insert_threads>8</max_insert_threads>
            <insert_quorum>0</insert_quorum>
            <insert_quorum_timeout>60000</insert_quorum_timeout>
        </log_ingestion>

        <!-- Profile for analytics queries -->
        <analytics>
            <max_memory_usage>20000000000</max_memory_usage>
            <max_threads>8</max_threads>
            <max_query_size>1073741824</max_query_size>
            <max_ast_elements>500000</max_ast_elements>
        </analytics>

        <!-- Profile for real-time queries -->
        <realtime>
            <max_memory_usage>1000000000</max_memory_usage>
            <max_execution_time>5</max_execution_time>
            <max_rows_to_read>1000000</max_rows_to_read>
            <timeout_before_checking_execution_speed>5</timeout_before_checking_execution_speed>
        </realtime>
    </profiles>

    <!-- Quotas for resource management -->
    <quotas>
        <default>
            <interval>
                <duration>3600</duration>
                <queries>0</queries>
                <errors>0</errors>
                <result_rows>0</result_rows>
                <read_rows>0</read_rows>
                <execution_time>0</execution_time>
            </interval>
        </default>

        <!-- Log ingestion quota -->
        <log_ingestion>
            <interval>
                <duration>3600</duration>
                <queries>10000</queries>
                <errors>1000</errors>
                <result_rows>0</result_rows>
                <read_rows>0</read_rows>
                <execution_time>3600</execution_time>
            </interval>
        </log_ingestion>

        <!-- Analytics quota -->
        <analytics>
            <interval>
                <duration>3600</duration>
                <queries>1000</queries>
                <errors>100</errors>
                <result_rows>10000000</result_rows>
                <read_rows>100000000</read_rows>
                <execution_time>1800</execution_time>
            </interval>
        </analytics>
    </quotas>

    <!-- Include users configuration -->
    <users_config>users.xml</users_config>

    <!-- Include macros for cluster configuration -->
    <macros incl="macros" optional="true" />

    <!-- Include cluster configuration -->
    <remote_servers incl="clickhouse_remote_servers" optional="true" />

    <!-- Include ZooKeeper configuration if clustering is needed -->
    <zookeeper incl="zookeeper-servers" optional="true" />

    <!-- Builtin configurations -->
    <builtin_dictionaries_reload_interval>3600</builtin_dictionaries_reload_interval>
    <max_session_timeout>3600</max_session_timeout>
    <default_session_timeout>60</default_session_timeout>

    <!-- Path to configuration file with users, access rights, profiles of settings, quotas. -->
    <users_config>users.xml</users_config>

    <!-- Default database -->
    <default_database>aitrading_logs</default_database>

    <!-- Path to temporary data for processing hard queries. -->
    <tmp_path>/var/lib/clickhouse/tmp/</tmp_path>

    <!-- Path to folder where users and roles created by SQL commands are stored. -->
    <user_directories>
        <users_xml>
            <path>/etc/clickhouse-server/users.xml</path>
        </users_xml>
        <local_directory>
            <path>/var/lib/clickhouse/access/</path>
        </local_directory>
    </user_directories>
</yandex>