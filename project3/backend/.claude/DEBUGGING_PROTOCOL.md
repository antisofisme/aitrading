# ğŸ§  Systematic Debugging Protocol
**MANDATORY FOR ALL BUG FIXES AND FEATURE DEVELOPMENT**

> Based on 2025 research: Multi-Agent Collaboration, Runtime Debugging, and Test-Driven Development

---

## ğŸš¨ **CRITICAL RULE**

**NEVER start coding immediately. ALWAYS follow this 5-phase protocol.**

Research shows: **"Thinking before debugging prevents fixing local issues while missing higher-level architectural problems."**

---

## ğŸ“‹ **Phase 1: ARCHITECTURE MAPPING (15-30 min minimum)**

### Before touching ANY code, build complete mental model:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MANDATORY CHECKLIST:                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â–¡ Read ALL files involved in the issue                 â”‚
â”‚ â–¡ Draw component interaction diagram                   â”‚
â”‚ â–¡ Document complete data flow (input â†’ output)         â”‚
â”‚ â–¡ Identify ALL interfaces/schemas between components   â”‚
â”‚ â–¡ List assumptions and constraints                     â”‚
â”‚ â–¡ Find potential failure points                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Example Mental Model Template:

```
Component A (file_a.py)
â”œâ”€ Input: {data structure}
â”œâ”€ Processing: [step 1, step 2, step 3]
â”œâ”€ Output: {data structure}
â”œâ”€ Dependencies: [Component B, Service X]
â”œâ”€ Interface: Protocol/Schema used
â””â”€ Failure Points: [timeout, schema mismatch, null values]
    â†“
Component B (file_b.py)
â”œâ”€ Input: {received from A}
â”œâ”€ Schema Validation: [field1, field2, field3]
â”œâ”€ Storage: Database table schema
â””â”€ Constraints: [unique keys, foreign keys, data types]
```

**Tools to Use:**
- `Read` all related files first
- `Grep` to find all usages of key functions/variables
- `Glob` to find all test files
- Draw ASCII diagrams in thinking block

---

## ğŸ”¬ **Phase 2: HYPOTHESIS GENERATION (Minimum 3 hypotheses)**

### Generate ranked hypotheses BEFORE any debugging:

```python
# Template for each hypothesis:
Hypothesis {N}: {Clear statement of what might be wrong}
â”œâ”€ Likelihood: High/Medium/Low
â”œâ”€ Evidence Needed: [What data would prove/disprove this]
â”œâ”€ Verification Method: [Exact steps to test]
â”œâ”€ Expected Behavior: [What should happen if correct]
â”œâ”€ Actual Behavior: [What is happening]
â””â”€ Root Cause If True: [Architectural issue that caused this]
```

### Example (from actual bug):

```
Hypothesis 1: ClickHouse schema mismatch in duplicate check
â”œâ”€ Likelihood: HIGH
â”œâ”€ Evidence: Error mentions "Unknown expression identifier 'timestamp_ms'"
â”œâ”€ Verification: Check live_aggregates table schema vs query
â”œâ”€ Expected: Query uses field that exists in table
â”œâ”€ Actual: Query uses timestamp_ms, table has time field
â””â”€ Root Cause: Code written before schema migration, not updated

Hypothesis 2: NATS subject pattern mismatch
â”œâ”€ Likelihood: MEDIUM
â”œâ”€ Evidence: Messages published but not received
â”œâ”€ Verification: Log publisher subject and subscriber pattern
â”œâ”€ Expected: market.XAUUSD.5m matches market.> subscription
â”œâ”€ Actual: (needs verification)
â””â”€ Root Cause: Inconsistent naming convention

Hypothesis 3: Message format incompatibility
â”œâ”€ Likelihood: LOW
â”œâ”€ Evidence: No parsing errors in logs
â”œâ”€ Verification: Log message payload structure
â”œâ”€ Expected: All required fields present
â”œâ”€ Actual: (needs verification)
â””â”€ Root Cause: Schema evolution without migration
```

**ANTI-PATTERN:**
```
âŒ "Let me add debug logging to see what's happening"
âŒ "Let me try changing this and see if it works"
âŒ "Maybe it's a race condition, let me add sleep()"
```

**CORRECT PATTERN:**
```
âœ… Generate 3-5 specific hypotheses ranked by likelihood
âœ… Design targeted verification for top hypothesis
âœ… Test hypothesis systematically
```

---

## ğŸ§ª **Phase 3: TEST-FIRST VERIFICATION**

### Write verification BEFORE fixing anything:

```python
# 1. Create verification script or test
# This becomes your "Definition of Done"

async def verify_pipeline_fix():
    """
    Verification for: Bars from polygon reaching ClickHouse

    Success Criteria:
    - Polygon publishes bars to NATS
    - Data-bridge receives bars from NATS
    - ClickHouse inserts bars without errors
    - Verification query finds expected bars
    """

    # Test 1: Publisher working
    assert polygon_stats['published_nats'] > 0

    # Test 2: NATS routing working
    assert data_bridge_stats['aggregate_messages'] > 0

    # Test 3: ClickHouse insertion working
    bars_in_db = clickhouse.execute("SELECT count() FROM live_aggregates WHERE ...")
    assert bars_in_db > 0

    # Test 4: Data integrity
    sample_bar = clickhouse.execute("SELECT * FROM live_aggregates LIMIT 1")
    assert sample_bar['symbol'] == expected_symbol
    assert sample_bar['time'] is not None

    return True

# 2. Run verification (SHOULD FAIL initially)
result = verify_pipeline_fix()  # âŒ FAIL - Expected behavior

# 3. Make fix

# 4. Run verification again
result = verify_pipeline_fix()  # âœ… PASS - Fix successful
```

**Benefits:**
- Clear success criteria before starting
- Prevents "looks like it works" false positives
- Creates regression test for future
- Forces understanding of expected behavior

---

## ğŸ”§ **Phase 4: INCREMENTAL FIXES (ONE AT A TIME)**

### CRITICAL: Fix one thing, verify immediately

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WORKFLOW:                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Test Hypothesis #1 (highest likelihood)             â”‚
â”‚    â”œâ”€ Make MINIMAL change (1 file, 1 function)         â”‚
â”‚    â”œâ”€ Document what changed and why                    â”‚
â”‚    â”œâ”€ Rebuild/restart affected service ONLY            â”‚
â”‚    â””â”€ Run verification immediately                     â”‚
â”‚                                                          â”‚
â”‚ 2. If PASS â†’ Done. If FAIL â†’ Revert and test next      â”‚
â”‚                                                          â”‚
â”‚ 3. Test Hypothesis #2                                  â”‚
â”‚    â””â”€ Repeat process                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Example (Correct Approach):

```bash
# Fix 1: Schema mismatch in duplicate check
Edit clickhouse_writer.py (change timestamp_ms â†’ time)
Rebuild data-bridge ONLY
Test: Check logs for schema errors
Result: âœ… PASS - No more schema errors

# Fix 2: (Not needed - Fix 1 solved it)
```

### ANTI-PATTERNS (NEVER DO):

```
âŒ Fix multiple files simultaneously
âŒ Make "while I'm here" improvements
âŒ Batch multiple changes before testing
âŒ Assume fix worked without verification
âŒ Add logging as "temporary" without removal plan
```

### CORRECT PATTERNS:

```
âœ… Fix ONE file at a time
âœ… Test immediately after each change
âœ… Revert if fix doesn't work
âœ… Document each change in git commit
âœ… Remove debug code after issue resolved
```

---

## ğŸ“Š **Phase 5: ROOT CAUSE ANALYSIS**

### After successful fix, ask "Why did this happen?"

```
Fix Applied: Changed duplicate check to use 'time' field
    â†“
Symptom: Query failed with "Unknown expression identifier"
    â†“
Immediate Cause: Code used timestamp_ms, schema had time
    â†“
Root Cause: Schema migration happened, code not updated
    â†“
Prevention:
â”œâ”€ Add integration test for schema compatibility
â”œâ”€ Add schema version checking in startup
â”œâ”€ Document schema changes in CHANGELOG
â””â”€ Add pre-commit hook to check for hardcoded field names
```

**Template:**

```
ROOT CAUSE ANALYSIS:
â”œâ”€ What broke? [Specific symptom]
â”œâ”€ Why did it break? [Immediate cause]
â”œâ”€ Why did that happen? [Ask "why" 3-5 times]
â”œâ”€ What prevented catching this earlier?
â””â”€ Prevention measures:
    â”œâ”€ Code change needed
    â”œâ”€ Test to add
    â”œâ”€ Documentation to update
    â””â”€ Process improvement
```

---

## ğŸ¤– **When to Use Specialized Agents**

For complex issues, delegate to specialized agents AFTER completing Phase 1-2:

```javascript
// âŒ WRONG: Delegate without mental model
Task("debugger", "Fix the pipeline", "debugger")

// âœ… CORRECT: Delegate with complete context
Task("error-detective", `
ARCHITECTURE:
${mental_model_diagram}

HYPOTHESES GENERATED:
1. Schema mismatch (HIGH likelihood)
   Evidence: ${evidence}
2. NATS routing issue (MEDIUM)
   Evidence: ${evidence}

TASK: Test hypothesis #1 systematically with verification
`, "error-detective")
```

**Agent Types:**
- `error-detective` - Log analysis, error correlation
- `tdd-orchestrator` - Test-first development
- `debugger` - General debugging (use with detailed context)
- `code-reviewer` - Pre-merge review
- `architect-review` - Architectural validation

---

## ğŸ“ˆ **Metrics for Success**

Track these to measure improvement:

```
Before Protocol:
â”œâ”€ Time to fix: 3-4 hours (trial and error)
â”œâ”€ False fixes: 3-5 attempts
â”œâ”€ Regressions: 20% of fixes break something else
â””â”€ Root cause found: 30%

After Protocol:
â”œâ”€ Time to fix: 30-60 min (systematic)
â”œâ”€ False fixes: 0-1 attempts
â”œâ”€ Regressions: <5%
â””â”€ Root cause found: 90%
```

---

## ğŸ¯ **Quick Reference Checklist**

Before ANY coding session:

```
â–¡ Phase 1: Built complete mental model (15-30 min)
â–¡ Phase 2: Generated 3+ ranked hypotheses
â–¡ Phase 3: Written verification test (fails initially)
â–¡ Phase 4: Fixed ONE thing, verified immediately
â–¡ Phase 5: Documented root cause and prevention
```

**Estimated Time Investment:**
- Protocol overhead: +30 minutes upfront
- Time saved: -2 to 3 hours of trial-and-error
- **Net savings: 1.5 to 2.5 hours per bug**

---

## ğŸ“š **References**

Research papers (2025):
- "Multi-Agent Collaboration and Runtime Debugging for LLM Code Generation"
- "Test-Driven Development: Mental Models for Code Understanding"
- "The Debugging Mindset: Think Before Fixing"

Best practices:
- Incremental development with immediate verification
- Hypothesis-driven debugging
- Root cause analysis over symptom fixing

---

**Last Updated:** 2025-10-19
**Version:** 1.0
**Status:** MANDATORY for all debugging tasks
