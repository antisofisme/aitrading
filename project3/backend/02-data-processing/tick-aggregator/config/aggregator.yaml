# =======================================================================
# TICK AGGREGATOR CONFIGURATION
# =======================================================================
# Purpose: Aggregate tick data from TimescaleDB into OHLCV candles
# Version: 1.0.0
# Last Updated: 2025-10-06
# =======================================================================

# Service Configuration
service:
  name: "tick-aggregator"
  instance_id: "aggregator-1"
  log_level: "INFO"

# Database Configuration (DEPRECATED - config loaded from environment variables)
# These values are not used; kept for backward compatibility
database:
  host: "${POSTGRES_HOST}"
  port: ${POSTGRES_PORT}
  database: "${POSTGRES_DB}"
  user: "${POSTGRES_USER}"
  password: "${POSTGRES_PASSWORD}"
  pool_size: 5
  tenant_id: "system"  # Application-level tenant

# Aggregation Configuration
aggregation:
  # Technical Indicators Configuration
  technical_indicators:
    indicators:
      sma:
        enabled: true
        periods: [7, 14, 21, 50, 200]

      ema:
        enabled: true
        periods: [7, 14, 21, 50, 200]

      rsi:
        enabled: true
        period: 14

      macd:
        enabled: true
        fast_period: 12
        slow_period: 26
        signal_period: 9

      bollinger:
        enabled: true
        period: 20
        std_dev: 2.0

      atr:
        enabled: true
        period: 14

      adx:
        enabled: true
        period: 14

      stochastic:
        enabled: true
        k_period: 14
        d_period: 3
        smooth_k: 3

      cci:
        enabled: true
        period: 20

      mfi:
        enabled: true
        period: 14

      obv:
        enabled: true

      adl:
        enabled: true

      vwap:
        enabled: true

  # Timeframes to aggregate (maps to schedule)
  # CRITICAL TIMING FIX: Enhanced lookback windows to catch late-arriving ticks
  # Buffer Strategy: interval + 2x buffer (was 1x, now 2x for robustness)
  timeframes:
    - name: "5m"
      interval_minutes: 5
      cron: "*/5 * * * *"  # Every 5 minutes
      lookback_minutes: 15  # IMPROVED: 5 + 10min buffer (was 10min, now 15min)
      # Safety: Catches ticks up to 10 minutes late (2x interval)

    - name: "15m"
      interval_minutes: 15
      cron: "*/15 * * * *"  # Every 15 minutes
      lookback_minutes: 30  # IMPROVED: 15 + 15min buffer (was 20min, now 30min)
      # Safety: Catches ticks up to 15 minutes late

    - name: "30m"
      interval_minutes: 30
      cron: "*/30 * * * *"  # Every 30 minutes
      lookback_minutes: 50  # IMPROVED: 30 + 20min buffer (was 35min, now 50min)
      # Safety: Catches ticks up to 20 minutes late

    - name: "1h"
      interval_minutes: 60
      cron: "0 * * * *"  # Every hour at :00
      lookback_minutes: 90  # IMPROVED: 60 + 30min buffer (was 65min, now 90min)
      # Safety: Catches ticks up to 30 minutes late

    - name: "4h"
      interval_minutes: 240
      cron: "0 */4 * * *"  # Every 4 hours
      lookback_minutes: 300  # IMPROVED: 240 + 60min buffer (was 245min, now 300min)
      # Safety: Catches ticks up to 1 hour late

    - name: "1d"
      interval_minutes: 1440
      cron: "0 0 * * *"  # Daily at 00:00 UTC
      lookback_minutes: 1500  # IMPROVED: 1440 + 60min buffer (was 1445min, now 1500min)
      # Safety: Catches ticks up to 1 hour late

    - name: "1w"
      interval_minutes: 10080
      cron: "0 0 * * 1"  # Weekly on Monday 00:00 UTC
      lookback_minutes: 10200  # IMPROVED: 10080 + 120min buffer (was 10085min, now 10200min)
      # Safety: Catches ticks up to 2 hours late

  # Batch configuration
  batch_size: 10000  # Ticks per query
  max_symbols_per_batch: 10  # Process N symbols in parallel

# NATS Configuration (with cluster support for High Availability)
nats:
  # NATS Cluster URLs (automatic failover)
  cluster_urls:
    - "nats://nats-1:4222"
    - "nats://nats-2:4222"
    - "nats://nats-3:4222"

  # Legacy single URL (fallback) - comma-separated for cluster
  url: "nats://nats-1:4222,nats://nats-2:4222,nats://nats-3:4222"

  max_reconnect_attempts: -1  # Infinite reconnection attempts
  reconnect_time_wait: 2      # seconds between reconnect attempts
  ping_interval: 120          # seconds for keepalive ping

  subjects:
    aggregates: "bars.{symbol}.{timeframe}"  # e.g., bars.EURUSD.5m

# Kafka Configuration (will be fetched from Central Hub)
kafka:
  brokers:
    - "suho-kafka:9092"
  topics:
    aggregate_archive: "aggregate_archive"
  compression_type: "lz4"

# Monitoring
monitoring:
  report_interval_seconds: 300  # Report every 5 minutes
  enable_metrics: true
  log_performance: true

# State Management
state:
  # Track last aggregation timestamp per timeframe/symbol
  persist_state: true
  state_file: "/tmp/aggregator_state.json"  # Will be volume-mounted
