# ğŸ—„ï¸ Database Flow - Data Storage Strategy

> **Version**: 1.8.0 (Draft - Bertahap)
> **Last Updated**: 2025-10-17
> **Status**: In Discussion - Konsep Awal

---

## <ï¿½ Prinsip Utama: Pemisahan Storage Berdasarkan Use Case

### 1ï¿½ **Live Data** ï¿½ TimescaleDB (Fast Writes)
- **Purpose**: Real-time trading, operational data
- **Optimization**: Fast writes, time-series queries
- **Retention**: 90 hari (auto-cleanup)
- **Use Case**: Live monitoring, recent analysis

### 2ï¿½ **Historical Data** ï¿½ ClickHouse (Long-term Storage)
- **Purpose**: Analytics, backtesting, ML training
- **Optimization**: Compression, analytical queries
- **Retention**: Unlimited (cost-effective storage)
- **Use Case**: Historical analysis, model training

---

## =ï¿½ Data Sources & Storage

**Final Architecture**: 3 Data Sources

| Source | Type | Storage | Retention | Cost | Status |
|--------|------|---------|-----------|------|--------|
| **Polygon Live** | Real-time tick | TimescaleDB | 3 days | Paid (subscription) | âœ… Active |
| **Polygon Historical** | Gap filling (M1/M5) | â†’ live_aggregates | On-demand (< 7 days) | Paid (per request) | âœ… Active |
| **Dukascopy Historical** | Historical tick | ClickHouse | Unlimited | FREE | âœ… Active |

---

### **Source 1: Polygon.io Live WebSocket**

**Data Type**: Real-time tick data (bid/ask quotes)

**Storage**:
```
Polygon WebSocket â†’ Live Collector â†’ TimescaleDB.live_ticks
```

**Database**: TimescaleDB
**Table**: `live_ticks`
**Retention**: 3 hari (auto-cleanup)

**Kolom Inti** (6 kolom):
- `time` - timestamp with time zone (primary key)
- `symbol` - varchar(20) (EUR/USD, XAU/USD, dll)
- `bid` - numeric(18,5)
- `ask` - numeric(18,5)
- `timestamp_ms` - bigint (unix timestamp untuk precision)
- `ingested_at` - timestamp with time zone

**Kolom Derived** (âš ï¸ **TIDAK DISIMPAN** - calculated saat query jika dibutuhkan):
- `mid` = (bid + ask) / 2 - Mid price (calculated from bid/ask)
- `spread` = ask - bid - Spread (calculated from bid/ask)

**Why NOT Stored?**
- âœ… Dapat dihitung instant dari bid/ask
- âœ… Menghemat storage (tidak perlu kolom tambahan)
- âœ… Spread metrics sudah di-aggregate di `live_aggregates` table

**Kolom Dihapus** (redundant):
- ~~`tick_id`~~ - tidak perlu, cukup composite key (time + symbol)
- ~~`tenant_id`~~ - single tenant
- ~~`exchange`~~ - jelas dari context (Polygon)
- ~~`source`~~ - jelas dari tabel
- ~~`event_type`~~ - semua tick
- ~~`use_case`~~ - jelas dari tabel

**Karakteristik**:
- Write speed: ~100-1,000 ticks/second per symbol
- Query pattern: Recent data (last minutes/hours)
- Auto-delete setelah 3 hari (operational window only)

---

### **Source 2: Dukascopy Historical Downloads**

**Data Type**: Historical tick data (bid/ask dari binary files)

**Storage**:
```
Dukascopy Binary Files â†’ Historical Downloader â†’ ClickHouse.historical_ticks
```

**Database**: ClickHouse
**Table**: `historical_ticks`
**Retention**: Unlimited

**Kolom Inti** (6 kolom - **sama dengan live**):
- `time` - DateTime64(3, 'UTC') (primary key)
- `symbol` - String (EUR/USD, XAU/USD, dll)
- `bid` - Decimal(18,5)
- `ask` - Decimal(18,5)
- `timestamp_ms` - UInt64 (unix timestamp untuk precision)
- `ingested_at` - DateTime64(3, 'UTC')

**Kolom Derived** (âš ï¸ **TIDAK DISIMPAN** - calculated saat query jika dibutuhkan):
- `mid` = (bid + ask) / 2 - Mid price (calculated from bid/ask)
- `spread` = ask - bid - Spread (calculated from bid/ask)

**Why NOT Stored?**
- âœ… Dapat dihitung instant dari bid/ask
- âœ… Menghemat storage (tidak perlu kolom tambahan)
- âœ… Spread metrics sudah di-aggregate di `historical_aggregates` table

**Kolom Dihapus** (redundant):
- ~~`tick_id`~~ - tidak perlu, cukup composite key (time + symbol)
- ~~`tenant_id`~~ - single tenant
- ~~`exchange`~~ - jelas dari context (Dukascopy)
- ~~`source`~~ - jelas dari tabel
- ~~`event_type`~~ - semua tick
- ~~`use_case`~~ - jelas dari tabel

**Karakteristik**:
- Write speed: Batch processing (jutaan tick sekaligus)
- Query pattern: Historical analysis (backtest, ML training)
- Highly compressed (10:1 ratio)
- Long-term storage (tahun-tahun ke belakang)

**About Dukascopy**:
- **Provider**: Dukascopy Bank SA (Swiss regulated bank)
- **Data Quality**: High-quality, widely used by researchers/traders
- **Cost**: **FREE** (gratis untuk historical tick data)
- **Update Schedule**: Daily updates
- **Delay**: **1-2 hari** (T-1 atau T-2)
  - Contoh: Data 15 Oktober tersedia tanggal 16-17 Oktober
- **Coverage**: Multi-year historical data

---

### **Source 3: Polygon Historical API** âš ï¸ **GAP FILLER**

**Data Type**: Aggregated candles (M1/M5) untuk gap filling

**Purpose**: **Fill gaps** di live_aggregates (< 2 days old)

**Storage Flow**:
```
Gap Detector â†’ Polygon Historical API â†’ ClickHouse.live_aggregates
                                        (insert missing candles)
```

**Target Table**: **live_aggregates** (tidak punya tabel sendiri)
**Trigger**: On-demand (saat gap terdeteksi)
**Cost**: Pay-per-request (hanya saat fill gaps)

**Why Needed**:
- Live stream bisa terputus (app restart, connection issues)
- Dukascopy delay 1-2 hari (tidak bisa fill recent gaps)
- Polygon Historical available immediately untuk recent data

**Gap Filling Strategy**:
```python
def fill_gap_in_live_aggregates(gap):
    """
    Fill detected gap dengan Polygon Historical (< 7 days only)
    """
    if gap.age < timedelta(days=7):
        # Recent gap: Use Polygon Historical (available now)
        data = polygon_api.get_aggregates(
            symbol=gap.symbol,
            timeframe='5m',
            start=gap.start,
            end=gap.end
        )
        clickhouse.insert('live_aggregates', data)
    else:
        # Old gap (> 7 days): SKIP - out of live retention window
        # Data already available in historical_aggregates (Dukascopy)
        logger.warning(f"Gap too old ({gap.age} days), skipping fill: {gap}")
```

**Karakteristik**:
- Update: On-demand (bukan continuous)
- Data type: Aggregates only (M1/M5), bukan raw tick
- No separate table (langsung ke live_aggregates)
- Use case: Gap recovery, data continuity
- Cost efficient: Only pay when gaps detected

---

## ğŸ”§ Gap Detection & Filling

**Service**: `gap-filler-service` (scheduled job)

### Gap Detection Logic

```python
class GapDetector:
    def detect_gaps(self, timeframe='5m', max_gap_minutes=15):
        """
        Detect gaps in live_aggregates
        """
        query = """
        SELECT
            symbol,
            time as gap_start,
            LEAD(time) OVER (PARTITION BY symbol ORDER BY time) as gap_end,
            dateDiff('minute', time, gap_end) as gap_minutes
        FROM live_aggregates
        WHERE timeframe = {timeframe}
        HAVING gap_minutes > {max_gap}
        ORDER BY symbol, time
        """

        gaps = clickhouse.query(query)
        return [Gap(**g) for g in gaps]
```

### Gap Filling Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            SCHEDULED JOB (every hour)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Detect Gaps          â”‚
        â”‚   (> 15 min missing)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Check Gap Age         â”‚
        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚           â”‚
      < 7 days      > 7 days
             â”‚           â”‚
             â–¼           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Polygon    â”‚  â”‚  SKIP        â”‚
    â”‚  Historical â”‚  â”‚  (too old)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ Fill gap
           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Insert to            â”‚
        â”‚  live_aggregates      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Note: Gaps > 7 days tidak perlu diisi karena:
- Out of live retention window (7 days)
- Data sudah ada di historical_aggregates
```

### Gap Filling Example

**Scenario**: App down 10:00-12:00 (2 hours)

```sql
-- Before gap filling
SELECT time, symbol FROM live_aggregates
WHERE symbol = 'C:EURUSD' AND timeframe = '5m'
ORDER BY time;

2025-10-17 09:55:00  C:EURUSD
-- GAP: 24 missing candles (10:00 - 11:55)
2025-10-17 12:00:00  C:EURUSD

-- Gap detected: 120 minutes missing

-- Fill with Polygon Historical
gaps = detect_gaps()
for gap in gaps:
    if gap.minutes > 15:  # Alert threshold
        fill_gap_with_polygon_historical(gap)

-- After gap filling
2025-10-17 09:55:00  C:EURUSD
2025-10-17 10:00:00  C:EURUSD  â† Filled from Polygon
2025-10-17 10:05:00  C:EURUSD  â† Filled from Polygon
...
2025-10-17 11:55:00  C:EURUSD  â† Filled from Polygon
2025-10-17 12:00:00  C:EURUSD  â† Back to live stream
```

**Monitoring**:
- Alert if gap > 15 minutes detected
- Log all gap fills untuk audit
- Track Polygon API usage (cost monitoring)

---


## ğŸ“° External Data Sources (ML Features)

**Purpose**: Menyediakan context data untuk ML model (economic events, market conditions)

**Total Sources**: 3 external data sources (scraped/fetched dari luar)

**Storage**: ClickHouse (analytics database)
**Flow**: `External Collector â†’ NATS/Kafka â†’ ClickHouse`

---

### **Source 4: Economic Calendar** â­â­â­â­â­

**Data Type**: Scheduled economic events & releases (NFP, GDP, Interest Rate, dll)

**Storage**:
```
MQL5 Scraper â†’ External Collector â†’ ClickHouse.external_economic_calendar
```

**Database**: ClickHouse
**Table**: `external_economic_calendar`
**Retention**: Unlimited

**Kolom Inti** (8 kolom):
- `event_time` - DateTime64(3, 'UTC') â­ **PRIMARY** (kapan event release)
- `currency` - String (USD, EUR, GBP, JPY)
- `event_name` - String (Non-Farm Payrolls, GDP, Interest Rate Decision)
- `impact` - Enum8('low', 'medium', 'high') (impact level untuk ML)
- `forecast` - Nullable(String) (predicted value)
- `previous` - Nullable(String) (previous value)
- `actual` - Nullable(String) (actual released value)
- `scraped_at` - DateTime64(3, 'UTC') (tracking only)

**Primary Index**: (event_time, currency)

**Timestamp Concept** (PENTING untuk ML):
```
event_time: 2025-10-06 14:30:00 UTC  â† NFP dirilis (ML pakai ini untuk join)
scraped_at: 2025-10-06 15:00:00 UTC  â† Kapan scraper ambil (tracking)
```

**ML Usage**:
```sql
-- Join dengan price data pada waktu event
SELECT
    e.event_time,
    e.event_name,
    e.impact,
    p.close AS price_before,
    LEAD(p.close) OVER (ORDER BY p.time) AS price_after
FROM external_economic_calendar e
LEFT JOIN aggregates p
    ON p.timestamp BETWEEN e.event_time - INTERVAL 5 MINUTE
                       AND e.event_time + INTERVAL 5 MINUTE
WHERE e.currency = 'USD' AND e.impact = 'high'
```

**Karakteristik**:
- Write frequency: Hourly scraping
- Data consistency: Very high (scheduled events)
- ML impact: â­â­â­â­â­ (highest impact untuk price movement)

---

### **Source 5: FRED Economic Indicators** â­â­â­â­â­

**Data Type**: Official US economic indicators (GDP, Unemployment, CPI, Interest Rates)

**Storage**:
```
FRED API â†’ External Collector â†’ ClickHouse.external_fred_indicators
```

**Database**: ClickHouse
**Table**: `external_fred_indicators`
**Retention**: Unlimited

**Kolom Inti** (6 kolom):
- `release_time` - DateTime64(3, 'UTC') â­ **PRIMARY** (kapan data dirilis)
- `series_id` - String (GDP, UNRATE, CPIAUCSL, DFF, DGS10, DEXUSEU, DEXJPUS)
- `series_name` - String (Gross Domestic Product, Unemployment Rate, dll)
- `value` - Float64 (indicator value)
- `unit` - String (Billions, Percent, Index)
- `scraped_at` - DateTime64(3, 'UTC') (tracking only)

**Primary Index**: (release_time, series_id)

**Timestamp Concept**:
```
release_time: 2025-10-06 08:30:00 UTC  â† GDP Q3 dirilis (ML pakai ini)
scraped_at: 2025-10-06 12:00:00 UTC    â† Kapan scraper ambil
```

**Karakteristik**:
- Write frequency: 4 hours (API polling)
- Data consistency: Very high (official government data)
- ML impact: â­â­â­â­â­ (macro trend prediction)

---

### **Source 6: Commodity Prices** â­â­â­â­

**Data Type**: Commodity prices (Gold, Oil, Silver, Copper, Natural Gas)

**Storage**:
```
Yahoo Finance API â†’ External Collector â†’ ClickHouse.external_commodity_prices
```

**Database**: ClickHouse
**Table**: `external_commodity_prices`
**Retention**: Unlimited

**Kolom Inti** (8 kolom):
- `price_time` - DateTime64(3, 'UTC') â­ **PRIMARY** (waktu harga tercatat)
- `symbol` - String (GC=F, CL=F, SI=F, HG=F, NG=F)
- `commodity_name` - String (Gold, Crude Oil, Silver, Copper, Natural Gas)
- `price` - Decimal(18,5) (current price)
- `currency` - String (USD)
- `change_pct` - Float64 (perubahan % dari previous close)
- `volume` - UInt64 (trading volume)
- `scraped_at` - DateTime64(3, 'UTC') (tracking only)

**Primary Index**: (price_time, symbol)

**Timestamp Concept**:
```
price_time: 2025-10-06 14:30:00 UTC  â† Harga gold saat ini (ML pakai ini)
scraped_at: 2025-10-06 14:30:15 UTC  â† Kapan scraper ambil
```

**Karakteristik**:
- Write frequency: 30 minutes (API polling)
- Data consistency: High (Yahoo Finance API)
- ML impact: â­â­â­â­ (correlation analysis dengan forex)

---

### ~~**Source 7: Crypto Sentiment**~~ âŒ **ARCHIVED**
### ~~**Source 8: Fear & Greed Index**~~ âŒ **ARCHIVED**
### ~~**Source 9: Market Sessions**~~ âš ï¸ **MOVED TO CALCULATED FEATURES**

**Status**: **TIDAK DIGUNAKAN** - Relevance rendah atau sudah dipindahkan

**Alasan**:
- Crypto sentiment tidak relevan untuk forex/gold pairs
- Fear & Greed Index = subjective, konsistensi rendah
- **Market Sessions** = calculated feature (tidak perlu external source), pindah ke Feature Engineering

---

## ğŸ§® Calculated Features (Feature Engineering)

**Purpose**: Features yang di-calculate on-the-fly dari timestamp dan candle data (tidak perlu external source)

**Storage**: **TIDAK DISIMPAN** - calculated saat feature engineering
**Flow**: `Aggregates â†’ Feature Engineering Service â†’ ML Features`

---

### **Feature 1: Market Sessions** â­â­â­â­â­

**Data Type**: Trading session status (London, New York, Tokyo, Sydney)

**Calculation**:
```python
def calculate_market_session(timestamp_utc):
    """
    Pure calculation dari UTC time (tidak perlu scraping)
    """
    hour = timestamp_utc.hour
    
    sessions = {
        'sydney': (22, 7),    # 22:00 - 07:00 UTC
        'tokyo': (0, 9),      # 00:00 - 09:00 UTC
        'london': (8, 16),    # 08:00 - 16:00 UTC
        'newyork': (13, 22)   # 13:00 - 22:00 UTC
    }
    
    active = [name for name, (start, end) in sessions.items() 
              if is_time_in_range(hour, start, end)]
    
    return {
        'active_sessions': active,              # ['london', 'newyork']
        'active_count': len(active),            # 2
        'is_overlap': len(active) > 1,          # True
        'liquidity_level': get_liquidity(active) # 'high'
    }
```

**Features Generated** (5 features):
- `active_sessions` - List session aktif
- `active_count` - Jumlah session (0-4)
- `is_overlap` - Binary flag overlap
- `liquidity_level` - Enum(very_low, low, medium, high, very_high)
- `is_london_newyork_overlap` - Binary flag untuk overlap paling liquid

**ML Impact**: â­â­â­â­â­ (volatility & liquidity prediction)

**Why Calculated, Not Stored?**
- Session times = fixed schedule (tidak berubah)
- Calculation = instant (< 1ms)
- No external dependency (pure math dari UTC time)
- Lebih flexible (mudah adjust session times)

---

### **Feature 2: Calendar Features** â­â­â­â­â­

**Data Type**: Day of week, week of month, month boundaries, dst holidays

**Calculation**:
```python
def add_calendar_features(df, timestamp_col='time'):
    """
    Calendar features dari timestamp
    """
    # Day features
    df['day_of_week'] = df[timestamp_col].dt.dayofweek  # 0=Monday, 6=Sunday
    df['day_name'] = df[timestamp_col].dt.day_name()    # 'Monday', 'Tuesday'
    
    # Binary flags (important trading days)
    df['is_monday'] = (df['day_of_week'] == 0).astype(int)
    df['is_friday'] = (df['day_of_week'] == 4).astype(int)
    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
    
    # Week features
    df['week_of_month'] = (df[timestamp_col].dt.day - 1) // 7 + 1
    df['week_of_year'] = df[timestamp_col].dt.isocalendar().week
    
    # Month boundaries (impact trading behavior)
    df['is_month_start'] = (df[timestamp_col].dt.day <= 5).astype(int)
    df['is_month_end'] = (df[timestamp_col].dt.day >= 25).astype(int)
    df['day_of_month'] = df[timestamp_col].dt.day
    
    return df
```

**Features Generated** (10 features):
- `day_of_week` - 0-6 (Monday-Sunday)
- `day_name` - String name
- `is_monday`, `is_friday`, `is_weekend` - Binary flags
- `week_of_month` - 1-5
- `week_of_year` - 1-52
- `is_month_start`, `is_month_end` - Binary flags
- `day_of_month` - 1-31

**ML Impact**: â­â­â­â­â­ (behavioral patterns per hari)

**Why Important?**
- Monday = Opening volatility (gap dari weekend)
- Friday = Position closing (avoid weekend risk)
- Month-end = Institutional rebalancing
- Week 1 = NFP release (first Friday of month)

---

### **Feature 3: Time Features** â­â­â­â­

**Data Type**: Hour, minute, quarter (intraday patterns)

**Calculation**:
```python
def add_time_features(df, timestamp_col='time'):
    """
    Intraday time features
    """
    df['hour_utc'] = df[timestamp_col].dt.hour           # 0-23
    df['minute'] = df[timestamp_col].dt.minute           # 0-59
    df['quarter_hour'] = df['minute'] // 15              # 0-3
    
    # Binary flags untuk key hours
    df['is_market_open'] = df['hour_utc'].between(0, 22).astype(int)  # Forex 24/5
    df['is_london_open'] = df['hour_utc'].between(8, 16).astype(int)
    df['is_ny_open'] = df['hour_utc'].between(13, 22).astype(int)
    
    return df
```

**Features Generated** (6 features):
- `hour_utc`, `minute`, `quarter_hour` - Time components
- `is_market_open`, `is_london_open`, `is_ny_open` - Binary flags

**ML Impact**: â­â­â­â­ (intraday volatility patterns)

---

## ğŸ”„ Feature Engineering Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         AGGREGATED CANDLES (live/historical)            â”‚
â”‚  time, symbol, timeframe, OHLC, tick_count, spreads     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Feature Engineering Service  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚           â”‚           â”‚
        â–¼           â–¼           â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚Calculatedâ”‚ â”‚External â”‚ â”‚Technical â”‚
   â”‚ Featuresâ”‚ â”‚  Data   â”‚ â”‚Indicatorsâ”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
        â”‚           â”‚           â”‚
        â”‚  Market   â”‚ Economic  â”‚  RSI
        â”‚  Sessions â”‚ Calendar  â”‚  MACD
        â”‚  Calendar â”‚ FRED      â”‚  Bollinger
        â”‚  Time     â”‚ Commodity â”‚  Fibonacci
        â”‚           â”‚           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  COMPLETE ML FEATURES â”‚
        â”‚   (72 features total) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **Summary: Storage Strategy**

| Feature Type | Storage | Calculation | Example |
|-------------|---------|-------------|---------|
| **Raw Ticks** | TimescaleDB/ClickHouse | Streamed/Batch | bid, ask, spread |
| **Aggregates** | ClickHouse | Tick Aggregator | OHLC, tick_count |
| **External Data** | ClickHouse | Scraper/API | Economic events, FRED |
| **Calculated Features** | **NONE** | Feature Engineering | Market sessions, calendar |
| **Technical Indicators** | **NONE** | Feature Engineering | RSI, MACD, Bollinger |

**Principle**: Only store raw/external data. Calculate derived features on-the-fly untuk flexibility.

---

## ğŸ“Š Aggregated Candles (OHLC Data)

**Purpose**: Time-series candles untuk trading analysis dan ML training

**Storage**: ClickHouse (analytics database)
**Flow**: `Raw Ticks â†’ Tick Aggregator â†’ ClickHouse`

**Timeframes**: 7 levels - 5m, 15m, 30m, 1h, 4h, 1d, 1w

**Aggregation Strategy**: **Flat Aggregation** (semua timeframe di-aggregate langsung dari raw ticks untuk accuracy)

---

### **Table 1: live_aggregates** (dari Polygon Live Ticks)

**Data Flow**:
```
TimescaleDB.live_ticks â†’ Tick Aggregator â†’ ClickHouse.live_aggregates
```

**Database**: ClickHouse
**Retention**: 7 hari (operational window)
**Update**: Real-time streaming aggregation + gap filling

**Schema** (15 kolom):
```sql
CREATE TABLE suho_analytics.live_aggregates
(
    -- Primary keys
    time DateTime64(3, 'UTC'),                          -- Candle open time (UTC aligned)
    symbol String,                                      -- C:EURUSD, C:XAUUSD, dll
    timeframe Enum8('5m'=1, '15m'=2, '30m'=3, '1h'=4, 
                    '4h'=5, '1d'=6, '1w'=7),           -- Timeframe identifier
    
    -- OHLC (mid prices: (bid+ask)/2)
    open Decimal(18,5),                                 -- First tick mid price
    high Decimal(18,5),                                 -- Highest mid price
    low Decimal(18,5),                                  -- Lowest mid price
    close Decimal(18,5),                                -- Last tick mid price
    
    -- Volume metrics
    tick_count UInt32,                                  -- Volume proxy (total ticks)
    
    -- Spread metrics (âš ï¸ DISIMPAN karena hasil aggregasi dari banyak tick)
    avg_spread Decimal(18,5),                          -- Average bid-ask spread (dari 847+ ticks)
    max_spread Decimal(18,5),                          -- Maximum spread (volatility spike detection)
    min_spread Decimal(18,5),                          -- Minimum spread (tightest liquidity)
    
    -- Volatility metrics
    price_range Decimal(18,5),                         -- high - low (intrabar range)
    pct_change Decimal(10,5),                          -- (close - open) / open * 100
    
    -- Quality flags
    is_complete UInt8,                                 -- 1 = closed candle, 0 = partial
    
    -- Metadata
    created_at DateTime64(3, 'UTC')                    -- Aggregation timestamp
)
ENGINE = MergeTree()
PARTITION BY (toYYYYMM(time), timeframe)
ORDER BY (symbol, timeframe, time);
```

**Primary Index**: (symbol, timeframe, time)

**Partition Strategy**: 
- Partitioned by month AND timeframe (e.g., `202510_4` = October 2025, 1h candles)
- Enables fast queries per timeframe
- Easy retention management (drop old partitions)

**Aggregation Example** (EUR/USD 5m candle):
```
Raw Ticks (14:00:00 - 14:05:00): 847 ticks
  â†’ 14:00:01: bid=1.10501, ask=1.10503 (spread=0.00002)
  â†’ 14:00:45: bid=1.10510, ask=1.10512 (spread=0.00002)
  â†’ 14:04:59: bid=1.10508, ask=1.10510 (spread=0.00002)
  â†’ ... (844 more ticks with varying spreads)

Result (DISIMPAN di database):
{
  time: "2025-10-17 14:00:00",
  symbol: "C:EURUSD",
  timeframe: "5m",
  open: 1.10502,           // First tick mid
  high: 1.10515,           // Highest mid
  low: 1.10498,            // Lowest mid
  close: 1.10509,          // Last tick mid
  tick_count: 847,         // Total ticks in 5 min

  // Spread metrics (aggregated dari 847 individual tick spreads)
  avg_spread: 0.00002,     // mean([spread1, spread2, ..., spread847])
  max_spread: 0.00004,     // max([spread1, spread2, ..., spread847]) - Volatility spike
  min_spread: 0.00001,     // min([spread1, spread2, ..., spread847]) - Tightest liquidity

  price_range: 0.00017,    // 1.10515 - 1.10498
  pct_change: 0.0063,      // (1.10509-1.10502)/1.10502*100
  is_complete: 1           // Closed candle
}
```

**Why Spread Metrics DISIMPAN?**
- âŒ **Tidak bisa di-recalculate** tanpa akses ke 847 raw tick data
- âœ… Hasil aggregasi dari banyak tick (not single calculation)
- âœ… Critical untuk ML (volatility & liquidity features)
- âœ… Dibutuhkan untuk historical analysis (tick data mungkin sudah dihapus)

**Karakteristik**:
- Write frequency: Continuous (new candles every timeframe interval)
- Query pattern: Recent data (last hours/days/week)
- Use Case: Live monitoring, operational alerts, real-time analysis, swing trading
- Retention: 7 days (sufficient for scalping, day trading, swing trading)

---

### **Table 2: historical_aggregates** (dari Dukascopy Historical Ticks)

**Data Flow**:
```
ClickHouse.historical_ticks â†’ Historical Aggregator â†’ ClickHouse.historical_aggregates
```

**Database**: ClickHouse
**Retention**: Unlimited (long-term storage)
**Update**: Batch processing (backfill historical)

**Schema** (15 kolom - **identik dengan live_aggregates**):
```sql
CREATE TABLE suho_analytics.historical_aggregates
(
    -- Primary keys
    time DateTime64(3, 'UTC'),
    symbol String,
    timeframe Enum8('5m'=1, '15m'=2, '30m'=3, '1h'=4, 
                    '4h'=5, '1d'=6, '1w'=7),
    
    -- OHLC
    open Decimal(18,5),
    high Decimal(18,5),
    low Decimal(18,5),
    close Decimal(18,5),
    
    -- Volume metrics
    tick_count UInt32,
    
    -- Spread metrics
    avg_spread Decimal(18,5),
    max_spread Decimal(18,5),
    min_spread Decimal(18,5),
    
    -- Volatility metrics
    price_range Decimal(18,5),
    pct_change Decimal(10,5),
    
    -- Quality flags
    is_complete UInt8,
    
    -- Metadata
    created_at DateTime64(3, 'UTC')
)
ENGINE = MergeTree()
PARTITION BY (toYYYYMM(time), timeframe)
ORDER BY (symbol, timeframe, time);
```

**Karakteristik**:
- Write frequency: Batch (daily backfill dari Dukascopy)
- Query pattern: Historical analysis (any timeframe, multi-year)
- Data quality: Complete (no gaps)
- Use Case: ML training, historical backtesting, research
- Compression: High (ClickHouse compression ~10:1)

---

### **Timeframe Synchronization**

Semua timeframes aligned dengan UTC boundaries:

```
Timeline UTC:
14:00:00 â”€â”€â”€â”€â”€â”€â†’ 14:05:00 â”€â”€â”€â”€â”€â”€â†’ 14:10:00 â”€â”€â”€â”€â”€â”€â†’ 14:15:00 â”€â”€â”€â”€â”€â”€â†’ 14:30:00
   â”‚                â”‚                â”‚                â”‚                â”‚
   5m #1            5m #2            5m #3            5m #4            5m #5 & 5m #6
   
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              15m candle (14:15:00)
   
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   30m candle (14:00:00)
```

**Data Example**:
```sql
time                symbol      timeframe   open      high      low       close     tick_count
2025-10-17 14:00:00 C:EURUSD   5m          1.10502   1.10515   1.10498   1.10509   847
2025-10-17 14:05:00 C:EURUSD   5m          1.10510   1.10520   1.10505   1.10518   923
2025-10-17 14:10:00 C:EURUSD   5m          1.10519   1.10525   1.10512   1.10520   891

2025-10-17 14:00:00 C:EURUSD   15m         1.10502   1.10525   1.10498   1.10520   2661

2025-10-17 14:00:00 C:EURUSD   1h          1.10502   1.10545   1.10498   1.10540   10234
```

---

### **Query Pattern: Union Live + Historical for ML**

Untuk ML training, gabungkan kedua tabel:

```sql
-- Complete dataset: historical + recent live data
SELECT * FROM (
    -- Historical data (complete, compressed)
    SELECT * FROM suho_analytics.historical_aggregates
    WHERE time < '2025-10-15'
      AND symbol = 'C:EURUSD'
      AND timeframe = '1h'
    
    UNION ALL
    
    -- Recent live data (operational)
    SELECT * FROM suho_analytics.live_aggregates
    WHERE time >= '2025-10-15'
      AND symbol = 'C:EURUSD'
      AND timeframe = '1h'
)
ORDER BY time;
```

**Benefits**:
- Complete historical coverage (unlimited retention)
- Fresh live data (real-time updates)
- Efficient storage (historical compressed, live hot)
- Optimal for ML training (all data in one query)

---

### **Why 15 Columns?**

| Column | Purpose | ML Impact |
|--------|---------|-----------|
| **time, symbol, timeframe** | Primary keys | Essential for indexing |
| **open, high, low, close** | Price action | â­â­â­â­â­ Core features |
| **tick_count** | Activity/volume proxy | â­â­â­â­ Liquidity indicator |
| **avg_spread, max_spread, min_spread** | Market conditions | â­â­â­â­ Volatility/liquidity |
| **price_range** | Volatility | â­â­â­â­ Breakout detection |
| **pct_change** | Momentum | â­â­â­â­â­ Direction prediction |
| **is_complete** | Quality flag | â­â­â­ Filter incomplete data |
| **created_at** | Tracking | â­ Debugging only |

**Total**: 15 kolom (lean, focused, ML-optimized)

---

### **Aggregation Process Flow**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAW TICK DATA                                â”‚
â”‚  TimescaleDB.live_ticks (live) or ClickHouse.historical_ticks  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ Tick Aggregator â”‚
                  â”‚   (Flat Mode)   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚               â”‚               â”‚
           â–¼               â–¼               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  5m, 15m   â”‚  â”‚  30m, 1h   â”‚  â”‚  4h, 1d, 1wâ”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â”‚               â”‚               â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  ClickHouse Aggregates Table  â”‚
          â”‚  (live or historical)         â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Flat Aggregation Logic**:
```python
def aggregate_ticks_to_all_timeframes(ticks, timestamp):
    """
    Aggregate raw ticks to ALL timeframes in parallel
    Each timeframe calculated directly from raw ticks (not cascading)
    """
    results = []
    
    for tf in ['5m', '15m', '30m', '1h', '4h', '1d', '1w']:
        candle_start = round_to_timeframe(timestamp, tf)
        candle_ticks = filter_ticks_for_timeframe(ticks, candle_start, tf)
        
        candle = {
            'time': candle_start,
            'timeframe': tf,
            'open': candle_ticks[0].mid_price,
            'high': max(t.mid_price for t in candle_ticks),
            'low': min(t.mid_price for t in candle_ticks),
            'close': candle_ticks[-1].mid_price,
            'tick_count': len(candle_ticks),
            'avg_spread': mean(t.spread for t in candle_ticks),
            'max_spread': max(t.spread for t in candle_ticks),
            'min_spread': min(t.spread for t in candle_ticks),
            'price_range': max_price - min_price,
            'pct_change': (close - open) / open * 100,
            'is_complete': 1 if candle_closed else 0,
        }
        results.append(candle)
    
    return results
```

**Why Flat vs Hierarchical?**
- âœ… Most accurate (no cascading errors)
- âœ… tick_count accurate (actual ticks, not summed)
- âœ… Spread metrics precise (calculated from raw data)
- âŒ More computation (7x aggregation per batch)

For ML training accuracy, flat aggregation is worth the computational cost.

---
## = Perbedaan Storage Strategy

| Aspek | TimescaleDB (Live) | ClickHouse (Historical) |
|-------|-------------------|------------------------|
| **Use Case** | Real-time trading | Historical analysis |
| **Write Pattern** | Streaming (real-time) | Batch (backfill) |
| **Write Speed** | Fast (optimized for inserts) | Very fast (batch inserts) |
| **Query Pattern** | Recent data (hot data) | Any timeframe (cold data) |
| **Retention** | 3 hari (ticks), 7 hari (aggregates) | Unlimited |
| **Compression** | Moderate | High (10:1) |
| **Storage Cost** | Higher (SSD) | Lower (compressed) |
| **Data Age** | 0-7 days | All historical data |
| **Trading Style** | Scalping, Day, Swing | Position, ML training |

---

## =ï¿½ Next Steps (Belum Dibahas)

- [x] Flow dari tick ke aggregates (candle generation) âœ… COMPLETED
- [ ] Flow dari aggregates ke ML features (feature engineering service)
- [ ] Query patterns untuk each use case
- [ ] Data lifecycle management
- [ ] Backup & recovery strategy

---

**Status**: Dokumentasi bertahap - akan dilanjutkan sesuai diskusi

**Version History**:
- v1.8.0 (2025-10-17): Clarified storage strategy - Tick tables: mid/spread NOT stored (derived on-the-fly), Aggregates: spread metrics STORED (aggregated from many ticks, cannot recalculate)
- v1.7.0 (2025-10-17): Optimized live retention - live_ticks: 3 days, live_aggregates: 7 days (covers scalping/day/swing), gap filling < 7 days, Dukascopy does NOT fill live gaps
- v1.6.0 (2025-10-17): Added Gap Filling strategy - Polygon Historical for recent gaps, renamed market_ticks â†’ live_ticks, added gap detection workflow
- v1.5.0 (2025-10-17): Separated External Data (3 sources) vs Calculated Features (Market Sessions, Calendar, Time) - Market Sessions moved to Feature Engineering
- v1.4.0 (2025-10-17): Added aggregates tables (live + historical) with 15 columns, flat aggregation strategy, 7 timeframes
- v1.3.0 (2025-10-17): Added 4 external data tables with timestamp concept for ML (Economic Calendar, FRED, Market Sessions, Commodity Prices)
- v1.2.0 (2025-10-17): Finalized 2-source architecture - Polygon Live + Dukascopy Historical only, archived Polygon Historical
- v1.1.0 (2025-10-17): Simplified schema - reduced from 14 to 6 core columns, removed redundant fields
- v1.0.0 (2025-10-17): Initial draft - Live vs Historical storage concept
