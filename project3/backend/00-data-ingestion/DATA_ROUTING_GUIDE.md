# ğŸ“Š Data Routing & Schema Management Guide

## Overview
Dokumentasi lengkap tentang **dari mana ke mana** data disimpan di sistem.

---

## ğŸ—‚ï¸ SCHEMA ARCHITECTURE - Hybrid Database Approach

### Database Distribution Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA ROUTING ARCHITECTURE                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  DATA COLLECTORS (Sources)                              â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  1. Polygon Live Collector       (Real-time ticks)      â”‚   â”‚
â”‚  â”‚  2. Polygon Historical Downloader (Historical OHLCV)    â”‚   â”‚
â”‚  â”‚  3. External Data Collector      (Economic calendar)    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                        â”‚             â”‚             â”‚
â”‚           â†“                        â†“             â†“             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ClickHouse     â”‚   â”‚  ClickHouse     â”‚   â”‚ PostgreSQL/  â”‚ â”‚
â”‚  â”‚  ticks          â”‚   â”‚  aggregates     â”‚   â”‚ TimescaleDB  â”‚ â”‚
â”‚  â”‚  (Forex data)   â”‚   â”‚  (OHLCV bars)   â”‚   â”‚ market_contextâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ DATA ROUTING TABLE

| Collector | Data Type | Target Database | Table Name | Purpose |
|-----------|-----------|-----------------|------------|---------|
| **Polygon Live** | Forex Ticks (Real-time) | ClickHouse | `ticks` | High-frequency quote data |
| **Polygon Live** | Aggregate Bars (OHLCV) | ClickHouse | `aggregates` | Time-series OHLCV candles |
| **Polygon Historical** | Historical OHLCV | ClickHouse | `aggregates` | Backtesting data |
| **External Collector** | Economic Calendar | PostgreSQL/TimescaleDB | `market_context` | Low-frequency context data |
| **External Collector** | Sentiment Data | PostgreSQL/TimescaleDB | `market_context` | Market sentiment |
| **External Collector** | Economic Indicators | PostgreSQL/TimescaleDB | `market_context` | FRED, CoinGecko data |

---

## ğŸ¯ ROUTING LOGIC - HIGH vs LOW FREQUENCY

### **From Schema SQL (Lines 457-475):**

```sql
-- =========================================================================
-- DATA ROUTING LOGIC (for collectors)
-- =========================================================================

-- HIGH FREQUENCY â†’ market_ticks table:
-- - Yahoo Finance OHLCV data
-- - Dukascopy historical ticks
-- - Broker MT5/API real-time feeds
-- - All price/volume time series data

-- LOW FREQUENCY â†’ market_context table:
-- - FRED economic indicators
-- - CoinGecko crypto prices/sentiment
-- - Fear & Greed Index
-- - Exchange rates (for reference)
-- - Market session data
-- - News events
-- - Economic calendar events
```

### Decision Tree:

```
Data Type?
    â”‚
    â”œâ”€ Price/Volume/Tick? â”€â”€â†’ HIGH FREQUENCY
    â”‚                         â”œâ”€ Real-time? â†’ ClickHouse `ticks`
    â”‚                         â””â”€ Historical? â†’ ClickHouse `aggregates`
    â”‚
    â””â”€ Economic/Sentiment? â”€â”€â†’ LOW FREQUENCY
                              â””â”€ PostgreSQL `market_context`
```

---

## ğŸ—ƒï¸ DETAILED SCHEMA MAPPING

### 1ï¸âƒ£ ClickHouse `ticks` Table

**Source**: Polygon Live Collector (WebSocket + REST)

**Purpose**: Real-time forex quotes for pattern analysis

**Schema**:
```sql
CREATE TABLE ticks (
    symbol String,                    -- "EUR/USD", "XAU/USD"
    timestamp DateTime64(3, 'UTC'),   -- Millisecond precision
    timestamp_ms UInt64,              -- Unix timestamp
    bid Decimal(18, 5),               -- Bid price
    ask Decimal(18, 5),               -- Ask price
    mid Decimal(18, 5),               -- Mid price
    spread Decimal(10, 5),            -- Spread in pips
    exchange UInt16,                  -- Exchange ID
    source String,                    -- "polygon_websocket"
    event_type String,                -- "quote"
    ingested_at DateTime64(3, 'UTC')
)
ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (symbol, timestamp)
TTL toDateTime(timestamp) + INTERVAL 90 DAY;  -- 90 days retention
```

**Data Flow**:
```
Polygon.io WebSocket
    â†’ Live Collector (websocket_client.py)
    â†’ NATS Publisher (nats_publisher.py)
    â†’ Data Bridge (kafka_subscriber.py)
    â†’ ClickHouse Writer
    â†’ ticks table
```

---

### 2ï¸âƒ£ ClickHouse `aggregates` Table

**Source**:
- Polygon Live Collector (Aggregate WebSocket)
- Polygon Historical Downloader (REST API)

**Purpose**: OHLCV bars for technical analysis & backtesting

**Schema**:
```sql
CREATE TABLE aggregates (
    symbol String,                    -- "EUR/USD"
    timeframe String,                 -- "1m", "5m", "1h", "1d"
    timestamp DateTime64(3, 'UTC'),   -- Bar start time
    timestamp_ms UInt64,
    open Decimal(18, 5),              -- OHLCV
    high Decimal(18, 5),
    low Decimal(18, 5),
    close Decimal(18, 5),
    volume UInt64,                    -- Tick volume
    vwap Decimal(18, 5),              -- Volume-weighted average
    range_pips Decimal(10, 5),        -- High-low range
    start_time DateTime64(3, 'UTC'),  -- Bar start
    end_time DateTime64(3, 'UTC'),    -- Bar end
    source String,                    -- "polygon_aggregate" / "polygon_historical"
    ingested_at DateTime64(3, 'UTC')
)
ENGINE = MergeTree()
PARTITION BY (symbol, toYYYYMM(timestamp))
ORDER BY (symbol, timeframe, timestamp)
TTL toDateTime(timestamp) + INTERVAL 365 DAY;  -- 1 year retention
```

**Data Flow (Live)**:
```
Polygon.io Aggregate WebSocket (CAS stream)
    â†’ Live Collector (aggregate_client.py)
    â†’ NATS Publisher
    â†’ Data Bridge
    â†’ ClickHouse Writer
    â†’ aggregates table
```

**Data Flow (Historical)**:
```
Polygon.io REST API
    â†’ Historical Downloader (main.py)
    â†’ ClickHouse Writer (clickhouse_writer.py)
    â†’ aggregates table
```

---

### 3ï¸âƒ£ PostgreSQL/TimescaleDB `market_context` Table

**Source**: External Data Collector (Scrapers + APIs)

**Purpose**: Low-frequency economic/sentiment data for ML context

**Schema**:
```sql
CREATE TABLE market_context (
    id BIGSERIAL PRIMARY KEY,

    -- Identification
    event_id VARCHAR(100),            -- Unique event ID
    external_id VARCHAR(100),         -- External system ID

    -- Classification
    data_type VARCHAR(30) NOT NULL,   -- 'economic_calendar', 'sentiment', etc.
    symbol VARCHAR(50) NOT NULL,      -- Event name or indicator
    timestamp BIGINT NOT NULL,        -- Unix milliseconds
    time_status VARCHAR(20),          -- 'historical', 'scheduled', 'future'

    -- Values
    value DECIMAL(20,8),              -- Numeric value
    value_text VARCHAR(100),          -- Text value

    -- Event-specific
    actual_value DECIMAL(20,8),       -- Actual result
    forecast_value DECIMAL(20,8),     -- Forecast
    previous_value DECIMAL(20,8),     -- Previous value
    event_date TIMESTAMP,             -- Scheduled date
    release_date TIMESTAMP,           -- Release date

    -- Context
    category VARCHAR(50),             -- 'inflation', 'gdp', etc.
    importance VARCHAR(20),           -- 'high', 'medium', 'low'

    -- Market impact
    currency_impact TEXT[],           -- Affected currencies
    forex_impact VARCHAR(30),         -- Impact direction
    market_impact_score DECIMAL(3,2), -- 0.0-1.0

    -- Source
    source VARCHAR(50) NOT NULL,      -- 'mql5', 'fred', 'coingecko'
    reliability VARCHAR(20),          -- 'high', 'medium', 'low'
    metadata JSONB,                   -- Full API response

    created_at TIMESTAMP DEFAULT NOW()
);

-- Indexes
CREATE INDEX idx_context_type_time ON market_context (data_type, timestamp DESC);
CREATE INDEX idx_context_symbol_time ON market_context (symbol, timestamp DESC);
CREATE UNIQUE INDEX idx_context_event_dedup ON market_context (event_id, symbol, event_date);
```

**Data Flow**:
```
External APIs (MQL5, FRED, CoinGecko)
    â†’ External Collector (scrapers/*.py)
    â†’ Transformation Pipeline (NEW - needs to be built)
    â†’ PostgreSQL Writer (NEW - needs to be built)
    â†’ market_context table
```

---

## ğŸ”„ COMPLETE DATA FLOW DIAGRAM

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA INGESTION SOURCES                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                      â”‚                      â”‚
         â†“                      â†“                      â†“
    Polygon.io            Polygon.io           External APIs
    WebSocket             REST API             (MQL5, FRED)
    (Real-time)           (Historical)         (Economic)
         â”‚                      â”‚                      â”‚
         â†“                      â†“                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Live Collector  â”‚   â”‚ Historical      â”‚   â”‚ External         â”‚
â”‚                 â”‚   â”‚ Downloader      â”‚   â”‚ Collector        â”‚
â”‚ Output:         â”‚   â”‚                 â”‚   â”‚                  â”‚
â”‚ - Tick quotes   â”‚   â”‚ Output:         â”‚   â”‚ Output:          â”‚
â”‚ - Aggregates    â”‚   â”‚ - OHLCV bars    â”‚   â”‚ - Calendar eventsâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                      â”‚                      â”‚
         â†“                      â†“                      â†“
    NATS/Kafka             Direct Write          Transform
    Message Queue           to DB                Pipeline
         â”‚                      â”‚                      â”‚
         â†“                      â†“                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Bridge     â”‚   â”‚ ClickHouse      â”‚   â”‚ PostgreSQL       â”‚
â”‚ (Subscriber)    â”‚   â”‚ Writer          â”‚   â”‚ Writer           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                      â”‚                      â”‚
         â†“                      â†“                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATABASE LAYER                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ ClickHouse (OLAP - Analytics Database)                 â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚ â€¢ ticks (90 days TTL)        - Real-time quotes        â”‚    â”‚
â”‚  â”‚ â€¢ aggregates (365 days TTL)  - OHLCV bars              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ PostgreSQL/TimescaleDB (OLTP - Transactional DB)       â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚ â€¢ market_context (5 years)   - Economic/Sentiment data â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ DragonflyDB (Cache Layer)                              â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚ â€¢ tick:latest:{symbol} (1h TTL)  - Latest ticks        â”‚    â”‚
â”‚  â”‚ â€¢ candle:latest:{symbol} (1h)    - Latest candles      â”‚    â”‚
â”‚  â”‚ â€¢ indicator:{symbol} (5m)        - Calculated values   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Schema File Locations

### Active Schemas (To be created):
```
backend/00-data-ingestion/schemas/
â”œâ”€â”€ clickhouse_forex.sql           # ClickHouse ticks + aggregates
â””â”€â”€ postgresql_market_context.sql  # PostgreSQL market_context
```

### Archived Schemas (Reference):
```
backend/00-data-ingestion/_archived/
â”œâ”€â”€ clickhouse-consumer/sql/schema.sql           # Old ClickHouse schema
â””â”€â”€ external-data/database/database_schema_hybrid.sql  # Hybrid schema (BEST REFERENCE)
```

### Architecture Documentation:
```
backend/docs/architecture/
â””â”€â”€ END_TO_END_ARCHITECTURE.md    # Complete system architecture
```

---

## ğŸ”§ Implementation Status

| Component | Schema Ready | Writer Ready | Integration Complete |
|-----------|--------------|--------------|---------------------|
| **Polygon Live â†’ ClickHouse ticks** | âœ… Yes | âœ… Via Data Bridge | âœ… Production Ready |
| **Polygon Live â†’ ClickHouse aggregates** | âš ï¸ Missing fields | âš ï¸ Needs update | âš ï¸ Needs fix |
| **Polygon Historical â†’ ClickHouse** | âš ï¸ Missing fields | âš ï¸ Needs update | âš ï¸ Needs fix |
| **External â†’ PostgreSQL context** | âœ… Yes | âŒ Not built | âŒ Major work needed |

---

## ğŸ¯ Next Steps

### Priority 1: Fix Historical Downloader Output
```python
# File: polygon-historical-downloader/src/clickhouse_writer.py
# Add missing fields to match aggregates schema:
{
    ...existing fields,
    'timeframe': '1m',  # From config
    'range_pips': (high - low) * 10000,
    'start_time': timestamp,
    'end_time': timestamp + timeframe_duration,
    'event_type': 'ohlcv'
}
```

### Priority 2: Build External Collector Writer
```python
# New file: external-data-collector/src/writers/postgresql_writer.py
class PostgreSQLWriter:
    async def write_market_context(self, event: dict):
        # Transform scraped data to market_context format
        transformed = transform_to_market_context(event)
        # Insert to PostgreSQL
        await self.pg_pool.execute(INSERT_SQL, transformed)
```

### Priority 3: Create Active Schema Directory
```bash
mkdir -p backend/00-data-ingestion/schemas
# Move and update schemas from archived
```

---

## ğŸ“Š Performance Considerations

### Data Retention

| Database | Table | Retention | Reason |
|----------|-------|-----------|--------|
| ClickHouse | `ticks` | 90 days | High volume, recent data analysis |
| ClickHouse | `aggregates` | 365 days | Backtesting, lower volume |
| PostgreSQL | `market_context` | 5 years | Low volume, historical context |
| DragonflyDB | Cache | 1 hour - 15 min | Real-time only |

### Storage Estimates

- **ticks**: ~100 MB/day (compressed)
- **aggregates**: ~20 MB/day
- **market_context**: ~1 MB/day
- **Total**: ~4-5 GB/month

---

## ğŸ“š References

1. **Schema SQL**: `/00-data-ingestion/_archived/external-data/database/database_schema_hybrid.sql` (Lines 457-475)
2. **Architecture**: `/docs/architecture/END_TO_END_ARCHITECTURE.md`
3. **Verification**: `/00-data-ingestion/DATA_SCHEMA_VERIFICATION.md`
4. **Data Manager**: `/01-core-infrastructure/central-hub/DATA_MANAGER_SPEC.md`

---

**Last Updated**: 2025-10-06
**Status**: Documentation Complete - Implementation In Progress
