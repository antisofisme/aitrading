# ğŸ”„ Complete Data Flow Architecture

**Last Updated**: 2025-10-06
**Version**: 3.0.0

## ğŸ“Š Overview

This document describes the **complete end-to-end data flow** from Polygon.io to databases, ensuring data persistence and proper storage.

**Key Architecture Change (v3.0.0)**:
- Live data now flows through **tick data + Aggregator Service** (not direct 1s bars)
- Aggregator queries TimescaleDB and creates 7 timeframes (M5-W1)
- This provides **FASTEST** historical data updates vs REST API

---

## ğŸŸ¢ LIVE DATA FLOW (Tick Data â†’ Aggregated)

### **Source**: Polygon WebSocket Tick/Quote Stream

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: Data Collection (Tick Data)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Polygon WebSocket (Tick/Quote Stream)                      â”‚
â”‚ - Real-time bid/ask quotes (raw ticks)                     â”‚
â”‚ - Delay: <100ms                                            â”‚
â”‚ - Symbols: EUR/USD, GBP/USD, XAU/USD, etc.                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: Live Collector (Tick Publisher)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Service: polygon-live-collector                             â”‚
â”‚ Location: /00-data-ingestion/polygon-live-collector/       â”‚
â”‚                                                              â”‚
â”‚ Function:                                                    â”‚
â”‚ - Parse tick data (bid/ask) from WebSocket                 â”‚
â”‚ - Calculate mid price and spread                            â”‚
â”‚ - Publish to NATS/Kafka (NO direct DB write)              â”‚
â”‚                                                              â”‚
â”‚ Output Format (Tick Data):                                   â”‚
â”‚ {                                                            â”‚
â”‚   'symbol': 'EUR/USD',                                      â”‚
â”‚   'bid': 1.0848,                                            â”‚
â”‚   'ask': 1.0850,                                            â”‚
â”‚   'mid': 1.0849,                                            â”‚
â”‚   'spread': 0.20,  (pips)                                   â”‚
â”‚   'timestamp_ms': 1706198400123,                            â”‚
â”‚   'exchange': 1,                                             â”‚
â”‚   'source': 'polygon_websocket',                            â”‚
â”‚   'event_type': 'quote'                                     â”‚
â”‚ }                                                            â”‚
â”‚                                                              â”‚
â”‚ NOTE: CAS (1s aggregates) DISABLED - using tick data       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: Message Queue (NATS/Kafka)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ NATS Subject: ticks.EURUSD                                  â”‚
â”‚ Kafka Topic: tick_archive                                   â”‚
â”‚                                                              â”‚
â”‚ Message Metadata:                                            â”‚
â”‚ - _source: 'polygon_websocket'                             â”‚
â”‚ - ingested_at: timestamp                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: Data Bridge (Tick Routing)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Service: data-bridge                                        â”‚
â”‚ Location: /02-data-processing/data-bridge/                 â”‚
â”‚                                                              â”‚
â”‚ Function:                                                    â”‚
â”‚ - Subscribe from NATS subject: ticks.>                      â”‚
â”‚ - Filter: source = 'polygon_websocket' â†’ TimescaleDB       â”‚
â”‚ - Batch write via Database Manager (1000 ticks/batch)      â”‚
â”‚ - Add tenant_id: 'system' (application-level)              â”‚
â”‚                                                              â”‚
â”‚ Configuration:                                               â”‚
â”‚ - Batch size: 1000 ticks                                    â”‚
â”‚ - Batch timeout: 5 seconds                                  â”‚
â”‚ - Tenant: 'system' (shared across all user tenants)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 5: TimescaleDB Storage (Tick Data)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Database: suho_trading                                      â”‚
â”‚ Table: market_ticks                                         â”‚
â”‚ Schema Location: /central-hub/shared/schemas/timescaledb/  â”‚
â”‚                  01_market_ticks.sql                        â”‚
â”‚                                                              â”‚
â”‚ Data Stored:                                                 â”‚
â”‚ - time: TIMESTAMPTZ (tick timestamp)                       â”‚
â”‚ - tenant_id: 'system'                                       â”‚
â”‚ - symbol: 'EUR/USD'                                         â”‚
â”‚ - bid, ask, mid, spread                                     â”‚
â”‚ - timestamp_ms, exchange                                     â”‚
â”‚ - source: 'polygon_websocket'                               â”‚
â”‚                                                              â”‚
â”‚ Retention: 90 days (auto-delete via retention policy)      â”‚
â”‚ Compression: After 7 days (10-20x space savings)           â”‚
â”‚                                                              â”‚
â”‚ âœ… TICK DATA PERSISTED IN TIMESCALEDB                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 6: Tick Aggregator Service (NEW!)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Service: tick-aggregator                                    â”‚
â”‚ Location: /02-data-processing/tick-aggregator/             â”‚
â”‚                                                              â”‚
â”‚ Function:                                                    â”‚
â”‚ - Query ticks from TimescaleDB (batch, scheduled)          â”‚
â”‚ - Aggregate to OHLCV for 7 timeframes                      â”‚
â”‚ - Calculate VWAP, range_pips, body_pips                    â”‚
â”‚ - Publish aggregates to NATS/Kafka                         â”‚
â”‚                                                              â”‚
â”‚ Schedule:                                                    â”‚
â”‚ - M5: Every 5 minutes                                       â”‚
â”‚ - M15: Every 15 minutes                                     â”‚
â”‚ - M30: Every 30 minutes                                     â”‚
â”‚ - H1: Every hour                                            â”‚
â”‚ - H4: Every 4 hours                                         â”‚
â”‚ - D1: Daily at 00:00 UTC                                    â”‚
â”‚ - W1: Weekly on Monday 00:00 UTC                           â”‚
â”‚                                                              â”‚
â”‚ Output Format (Aggregated):                                  â”‚
â”‚ {                                                            â”‚
â”‚   'symbol': 'EUR/USD',                                      â”‚
â”‚   'timeframe': '5m',                                        â”‚
â”‚   'timestamp_ms': 1706198400000,                            â”‚
â”‚   'open': 1.0850, 'high': 1.0855,                          â”‚
â”‚   'low': 1.0848, 'close': 1.0852,                          â”‚
â”‚   'volume': 1234,  (tick count)                            â”‚
â”‚   'vwap': 1.0851,                                           â”‚
â”‚   'range_pips': 0.70, 'body_pips': 0.20,                   â”‚
â”‚   'start_time': '2024-01-25T10:00:00+00:00',               â”‚
â”‚   'end_time': '2024-01-25T10:05:00+00:00',                 â”‚
â”‚   'source': 'live_aggregated',                              â”‚
â”‚   'event_type': 'ohlcv'                                     â”‚
â”‚ }                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 7: Message Queue (NATS/Kafka)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ NATS Subject: bars.EURUSD.5m                                â”‚
â”‚ Kafka Topic: aggregate_archive                              â”‚
â”‚                                                              â”‚
â”‚ Message Metadata:                                            â”‚
â”‚ - _source: 'live_aggregated' (from Aggregator)             â”‚
â”‚ - ingested_at: timestamp                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 8: Data Bridge (Aggregate Routing)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Service: data-bridge                                        â”‚
â”‚ Location: /02-data-processing/data-bridge/                 â”‚
â”‚                                                              â”‚
â”‚ Function:                                                    â”‚
â”‚ - Subscribe from NATS subject: bars.>                       â”‚
â”‚ - Filter: source = 'live_aggregated' â†’ ClickHouse          â”‚
â”‚ - Batch write via ClickHouse Writer (1000 candles/batch)   â”‚
â”‚ - Validate timeframes: M5, M15, M30, H1, H4, D1, W1       â”‚
â”‚                                                              â”‚
â”‚ Configuration:                                               â”‚
â”‚ - Batch size: 1000 candles                                  â”‚
â”‚ - Batch timeout: 10 seconds                                 â”‚
â”‚ - Tenant: 'system' (shared across all user tenants)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 9: ClickHouse Storage (FASTEST Historical Data!)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Database: suho_analytics                                    â”‚
â”‚ Table: aggregates                                           â”‚
â”‚ Schema Location: /central-hub/shared/schemas/clickhouse/   â”‚
â”‚                  02_aggregates.sql                          â”‚
â”‚                                                              â”‚
â”‚ Data Stored:                                                 â”‚
â”‚ - symbol: 'EUR/USD'                                         â”‚
â”‚ - timeframe: '5m' (one of 7: M5-W1)                        â”‚
â”‚ - timestamp: DateTime64                                     â”‚
â”‚ - OHLCV + volume, vwap, range_pips, body_pips             â”‚
â”‚ - source: 'live_aggregated'                                 â”‚
â”‚                                                              â”‚
â”‚ Retention: 10 years (3650 days via TTL)                    â”‚
â”‚ Compression: 10-100x (automatic)                            â”‚
â”‚                                                              â”‚
â”‚ âœ… AGGREGATED DATA IN CLICKHOUSE (FASTEST UPDATE!)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Benefit**: Live aggregated data arrives in ClickHouse within **minutes** (e.g., M5 arrives 5 minutes after bar close), much faster than REST API historical data which has 15-30 minute delay!

---

## ğŸ”µ HISTORICAL DATA FLOW (Gap Filling Only)

### **Source**: Polygon REST API (Only for Missing Data)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: Gap Detection & Download (Only Missing Data)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Gap Detector â†’ Check missing data in ClickHouse           â”‚
â”‚ Polygon REST API (/v2/aggs)                                â”‚
â”‚ - Download ONLY missing/gap data                           â”‚
â”‚ - Delay: ~15-30 minutes from real-time                     â”‚
â”‚ - Timeframes: M5, M15, M30, H1, H4, D1, W1 (7 total)      â”‚
â”‚ - Symbols: EUR/USD, GBP/USD, XAU/USD, etc.                â”‚
â”‚                                                              â”‚
â”‚ Note: Most data comes from live_aggregated (faster!)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: Historical Downloader (Gap Filling Only)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Service: polygon-historical-downloader                      â”‚
â”‚ Location: /00-data-ingestion/polygon-historical-downloader/â”‚
â”‚                                                              â”‚
â”‚ Function:                                                    â”‚
â”‚ - Download ONLY gap/missing bars via REST API              â”‚
â”‚ - Transform to standard format                              â”‚
â”‚ - Publish to NATS/Kafka (NO direct DB write)              â”‚
â”‚                                                              â”‚
â”‚ Use Case:                                                    â”‚
â”‚ - Backfill old data (before live collection started)       â”‚
â”‚ - Fill gaps during downtime/network issues                  â”‚
â”‚ - Initial historical download (e.g., last 2 years)         â”‚
â”‚                                                              â”‚
â”‚ Output Format:                                               â”‚
â”‚ {                                                            â”‚
â”‚   'symbol': 'EUR/USD',                                      â”‚
â”‚   'timeframe': '15m',  (configurable: 5m, 15m, 30m, etc.) â”‚
â”‚   'timestamp_ms': 1706198400000,                            â”‚
â”‚   'open': 1.0850, 'high': 1.0855,                          â”‚
â”‚   'low': 1.0848, 'close': 1.0852,                          â”‚
â”‚   'volume': 15234, 'vwap': 1.0851,                         â”‚
â”‚   'range_pips': 0.70,                                       â”‚
â”‚   'start_time': '2024-01-25T10:00:00+00:00', (ISO string)  â”‚
â”‚   'end_time': '2024-01-25T10:15:00+00:00',                 â”‚
â”‚   'source': 'polygon_historical',                           â”‚
â”‚   'event_type': 'ohlcv'                                     â”‚
â”‚ }                                                            â”‚
â”‚                                                              â”‚
â”‚ Timeframes Downloaded:                                       â”‚
â”‚ - M5  (5 minutes)  - Entry timing & micro patterns         â”‚
â”‚ - M15 (15 minutes) - Intraday signals                       â”‚
â”‚ - M30 (30 minutes) - Confirmation                           â”‚
â”‚ - H1  (1 hour)     - Short-term trend                       â”‚
â”‚ - H4  (4 hours)    - Medium-term trend                      â”‚
â”‚ - D1  (1 day)      - Long-term trend                        â”‚
â”‚ - W1  (1 week)     - Major market structure                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: Message Queue (NATS/Kafka)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ NATS Subject: bars.EURUSD.{5m|15m|30m|1h|4h|1d|1w}         â”‚
â”‚ Kafka Topic: aggregate_archive                              â”‚
â”‚                                                              â”‚
â”‚ Message Metadata:                                            â”‚
â”‚ - _source: 'polygon_historical' (gap filling)              â”‚
â”‚ - ingested_at: timestamp                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: Data Bridge (Routing Service)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Service: data-bridge                                        â”‚
â”‚ Location: /02-data-processing/data-bridge/                 â”‚
â”‚                                                              â”‚
â”‚ Function:                                                    â”‚
â”‚ - Subscribe from NATS subject: bars.>                       â”‚
â”‚ - Filter: _source == 'polygon_historical' â†’ ClickHouse     â”‚
â”‚ - Batch write via ClickHouse Writer (1000 candles/batch)   â”‚
â”‚ - Auto-convert: ISO strings â†’ DateTime64                    â”‚
â”‚ - Validate timeframes: 5m, 15m, 30m, 1h, 4h, 1d, 1w       â”‚
â”‚                                                              â”‚
â”‚ Configuration:                                               â”‚
â”‚ - Batch size: 1000 candles                                  â”‚
â”‚ - Batch timeout: 10 seconds                                 â”‚
â”‚ - Tenant: 'system' (shared across all user tenants)        â”‚
â”‚ - Supported timeframes: [5m, 15m, 30m, 1h, 4h, 1d, 1w]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 5: ClickHouse Storage (Gap Data)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Database: suho_analytics                                    â”‚
â”‚ Table: aggregates                                           â”‚
â”‚ Schema Location: /central-hub/shared/schemas/clickhouse/   â”‚
â”‚                  02_aggregates.sql                          â”‚
â”‚                                                              â”‚
â”‚ Data Stored (Gap Filling):                                   â”‚
â”‚ - symbol: 'EUR/USD'                                         â”‚
â”‚ - timeframe: '15m' (one of 7 supported)                    â”‚
â”‚ - timestamp: DateTime64 (bar start time)                    â”‚
â”‚ - source: 'polygon_historical'                              â”‚
â”‚ - timestamp_ms: UInt64                                      â”‚
â”‚ - OHLCV + volume, vwap, range_pips, body_pips             â”‚
â”‚ - start_time, end_time (DateTime64)                         â”‚
â”‚ - source: 'polygon_historical'                              â”‚
â”‚                                                              â”‚
â”‚ Retention: 10 years (3650 days via TTL)                    â”‚
â”‚ Partitioning: By symbol and month                           â”‚
â”‚ Compression: 10-100x (automatic ClickHouse compression)     â”‚
â”‚                                                              â”‚
â”‚ Materialized Views:                                          â”‚
â”‚ - daily_stats: Pre-aggregated daily statistics             â”‚
â”‚ - monthly_stats: Pre-aggregated monthly statistics         â”‚
â”‚                                                              â”‚
â”‚ âœ… DATA PERSISTED IN CLICKHOUSE                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Schema Locations (Central Hub)

All schemas are centralized in Central Hub for **single source of truth**:

### **TimescaleDB Schemas**:
```
/01-core-infrastructure/central-hub/shared/schemas/timescaledb/
â”œâ”€â”€ 01_market_ticks.sql       (Tick data - optional)
â”œâ”€â”€ 02_market_candles.sql     (âœ… LIVE 1s bars)
â””â”€â”€ 03_market_context.sql     (Economic calendar)
```

### **ClickHouse Schemas**:
```
/01-core-infrastructure/central-hub/shared/schemas/clickhouse/
â”œâ”€â”€ 01_ticks.sql              (Tick data - optional, research only)
â””â”€â”€ 02_aggregates.sql         (âœ… HISTORICAL 7 timeframes)
```

---

## âœ… Verification Checklist

### **Live Data Flow (Complete)**:
- [x] Polygon WebSocket â†’ Live Collector
- [x] Live Collector â†’ NATS/Kafka (publish only)
- [x] NATS/Kafka â†’ Data Bridge (source filter: NOT 'historical')
- [x] Data Bridge â†’ TimescaleDB.market_candles (via Database Manager)
- [x] Schema in Central Hub: `timescaledb/02_market_candles.sql`
- [x] tenant_id = 'system' (application-level)
- [x] Retention: 90 days

### **Historical Data Flow (Complete)**:
- [x] Polygon REST API â†’ Historical Downloader
- [x] Historical Downloader â†’ NATS/Kafka (publish only)
- [x] NATS/Kafka â†’ Data Bridge (source filter: 'historical')
- [x] Data Bridge â†’ ClickHouse.aggregates (via ClickHouse Writer)
- [x] Schema in Central Hub: `clickhouse/02_aggregates.sql`
- [x] tenant_id = 'system' (application-level)
- [x] 7 timeframes: M5, M15, M30, H1, H4, D1, W1
- [x] Retention: 10 years

---

## ğŸ”§ Environment Variables

### **Data Bridge Service**:
```bash
# Message Queues (from Central Hub)
CENTRAL_HUB_URL=http://suho-central-hub:8080
NATS_URL=nats://suho-nats-server:4222
KAFKA_BROKERS=suho-kafka:9092

# Databases (from Central Hub)
# TimescaleDB (for Live data)
TIMESCALEDB_HOST=suho-timescaledb
TIMESCALEDB_PORT=5432
TIMESCALEDB_DATABASE=suho_trading
TIMESCALEDB_USER=suho_service
TIMESCALEDB_PASSWORD=***

# ClickHouse (for Historical data)
CLICKHOUSE_HOST=suho-clickhouse
CLICKHOUSE_PORT=8123
CLICKHOUSE_DATABASE=suho_analytics
CLICKHOUSE_USER=default
CLICKHOUSE_PASSWORD=***

# DragonflyDB (cache layer)
DRAGONFLYDB_HOST=suho-dragonfly
DRAGONFLYDB_PORT=6379

# Batch Configuration
LIVE_BATCH_SIZE=100
LIVE_BATCH_TIMEOUT=5.0
HISTORICAL_BATCH_SIZE=1000
HISTORICAL_BATCH_TIMEOUT=10.0
```

---

## ğŸ“Š Data Storage Summary

| Data Type | Source | Timeframes | Database | Table | Retention | Tenant |
|-----------|--------|------------|----------|-------|-----------|--------|
| **Live** | WebSocket CAS | 1s | TimescaleDB | market_candles | 90 days | system |
| **Historical** | REST API | M5, M15, M30, H1, H4, D1, W1 | ClickHouse | aggregates | 10 years | system |

---

## ğŸ¯ Key Design Principles

1. **Separation of Concerns**: Collectors ONLY publish, Data Bridge handles routing & writes
2. **Intelligent Routing**: Single service (Data Bridge) routes based on data source
3. **Message Queue First**: All data goes through NATS/Kafka (no direct DB writes)
4. **Single Source of Truth**: All schemas centralized in Central Hub
5. **System-Level Tenant**: Data is application-level ('system'), not per-user
6. **Data Persistence**: Both flows guaranteed to persist data to databases
7. **Scalability**: Data Bridge can be scaled horizontally for high throughput

---

**Status**: âœ… **COMPLETE** - All data flows verified and documented.
