# Data Ingestion Service

## üéØ Purpose
**Server-side market data collection service** yang mengumpulkan real-time tick data dari approved brokers, convert ke Protocol Buffers, dan stream ke NATS/Kafka untuk processing di downstream services.

---

## üìä ChainFlow Diagram

```
Single Source Strategy ‚Üí UnifiedMarketData ‚Üí Hybrid Database ‚Üí Universal Signals
            ‚Üì                    ‚Üì                 ‚Üì                 ‚Üì
OANDA v20 API              Proto Convert      market_ticks        Signal Generation
External-Data (8 APIs)     Schema Validation  market_context      AI Analysis
Historical References      Binary Format      Optimized Indexes   Broadcast to Users
```

---

## üèóÔ∏è Service Architecture

### **Input Flow**: 4-category raw data collection
**Data Sources**:
- **broker-api**: OANDA v20 API (Single, reliable live tick source)
- **external-data**: 8 collectors (Yahoo Finance, FRED, CoinGecko, etc.)
- **Historical References**: Dukascopy Swiss-grade validation data
**Format**: Raw data converted to Protocol Buffers
**Frequency**: Real-time (50+ ticks/sec) streaming
**Performance Target**: <2ms data collection + proto conversion

### **Output Flow**: NATS/Kafka streaming to Data Bridge
**Destination**: NATS/Kafka message queues ‚Üí Data Bridge (direct consumer subscription)
**Format**: Protocol Buffers binary streams (60% smaller than JSON, 10x faster)
**Processing**: Raw data ‚Üí Proto serialization ‚Üí NATS/Kafka publish ‚Üí Data Bridge consumer
**Performance Target**: <1ms proto conversion + <2ms NATS/Kafka delivery = <3ms total

---

## üöÄ Transport Architecture & Efficiency Gains

### **Resource Optimization Strategy:**

#### **Traditional Architecture (Inefficient):**
```
1000 Users √ó 3 Brokers = 3000 MT5 Connections
Network: 3000x redundant streams
CPU: 3000x duplicate processing
Memory: 3000x data buffers
```

#### **Simplified Data Collection Architecture:**
```
4-Category Data Collection ‚Üí Protocol Buffers ‚Üí NATS/Kafka ‚Üí Data Bridge:
- Broker-API: OANDA v20 API (single, reliable live tick source)
- External-Data: 8 API connections (Yahoo Finance, FRED, CoinGecko, etc.)
- Historical-References: Dukascopy batch data import processes
- Secure Stream: Raw data ‚Üí Proto ‚Üí NATS/Kafka ‚Üí Data Bridge (direct consumer)
Total: ~10 data collection processes ‚Üí Secure message queue streaming
```

### **Data Collection Benefits:**
```
Resource & Efficiency Optimization:
- Connection Reduction: 1000+ clients ‚Üí ~10 server collectors (99% reduction)
- Secure Message Queues: NATS/Kafka for security, durability, and scalability
- Protocol Buffers: 60% smaller payloads, 10x faster serialization vs JSON
- High Throughput: Handle 50+ ticks/second with <1ms processing per tick
- Cost Efficiency: 99.3% reduction vs traditional client-broker model
- Centralized Collection: Single data source for all downstream services
```

### **Protocol Buffers Performance Benefits:**
```
Binary Serialization Advantages (external-data/schemas/market_data_pb2.py):
- Payload Size: 60% smaller than JSON (network bandwidth savings)
- Serialization Speed: 10x faster than JSON parsing
- Memory Usage: 40% less memory consumption
- CPU Efficiency: <1ms processing per tick vs 5-10ms with JSON
- Type Safety: Schema validation prevents data corruption
- Backward Compatibility: Schema evolution without breaking changes
```

---

## üîÑ Simplified Data Collection Pipeline

### **1. Raw Data Collection**:
```python
class BrokerDataCollector:
    async def collect_broker_data(self, broker_config: BrokerConfig):
        """Collect raw data from approved broker"""

        # Connect to approved broker only
        if not await self.validate_approved_broker(broker_config.name):
            raise Exception(f"Broker {broker_config.name} not approved")

        # Collect raw tick data
        raw_data = await self.get_raw_tick_data(broker_config)

        # Convert to Protocol Buffers
        proto_data = self.convert_to_protobuf(raw_data)

        # Stream to NATS/Kafka for secure delivery
        await self.stream_to_message_queue(proto_data)
```

### **2. Protocol Buffer Conversion**:
```python
async def convert_to_protobuf(self, raw_tick_data) -> bytes:
    """Convert raw tick data to Protocol Buffers binary"""

    # Create protobuf message
    market_data = MarketDataStream()
    market_data.source = self.broker_name
        market_tick.broker_timestamp = int(time.time() * 1000)

        # Send to aggregator
        await self.stream_to_aggregator(market_tick)
```

### **2. Market Data Aggregation**:
```python
class MarketAggregator:
    def __init__(self):
        self.broker_streams = {}  # Track data from each broker
        self.last_prices = {}     # Price deduplication cache
        self.quality_tracker = DataQualityTracker()

    async def aggregate_market_data(self, broker_tick: MarketTick):
        """Aggregate data dari multiple brokers"""

        symbol = broker_tick.symbol
        broker = broker_tick.source

        # Update broker stream tracking
        self.broker_streams[f"{broker}_{symbol}"] = broker_tick

        # Check for price differences antar brokers
        price_spread = self.calculate_broker_spread(symbol)

        # Select best price (usually tightest spread)
        best_tick = self.select_best_price(symbol)

        # Create aggregated market data stream
        market_stream = MarketDataStream()
        market_stream.ticks.append(best_tick)
        market_stream.source = "data-ingestion"
        market_stream.batch_time_msc = int(time.time() * 1000)

        # Distribute to subscribers
        await self.distribute_to_subscribers(market_stream)

        # Update quality metrics
        await self.quality_tracker.update_metrics(broker_tick, price_spread)
```

### **3. NATS/Kafka Distribution Engine**:
```python
class SecureDistributionEngine:
    def __init__(self):
        self.nats_client = NATSClient()
        self.kafka_client = KafkaProducer()
        self.central_hub_client = CentralHubClient()

    async def stream_to_message_queue(self, market_stream: MarketDataStream):
        """Stream data via NATS/Kafka for secure, scalable delivery"""

        # Serialize Protocol Buffers data
        binary_data = market_stream.SerializeToString()

        try:
            # Primary: NATS streaming (fast, low-latency)
            subject = f"market.{market_stream.source}.ticks"
            await self.nats_client.publish(subject, binary_data)

            # Backup: Kafka streaming (durability, replay capability)
            await self.kafka_client.produce(
                topic="market-data-stream",
                key=market_stream.source,
                value=binary_data
            )

            self.logger.debug(f"Streamed {len(market_stream.ticks)} ticks via NATS/Kafka")

        except Exception as e:
            self.logger.error(f"Failed to stream to message queue: {e}")
            # Register with Central Hub for health monitoring
            await self.register_failure_with_central_hub(e)

    async def register_with_central_hub(self):
        """Register data-ingestion service dengan Central Hub"""
        await self.central_hub_client.register_service({
            "name": "data-ingestion",
            "host": "data-ingestion",
            "port": 8002,
            "protocol": "nats+kafka",
            "health_endpoint": "/health",
            "topics": ["market-data-stream"],
            "subjects": ["market.*.ticks"]
        })
```

---

## üìã Broker Configuration

### **Multi-Broker Setup**:
```yaml
# Broker configurations
brokers:
  ic_markets:
    name: "IC Markets"
    server: "ICMarkets-Demo01"
    mt5_path: "/opt/mt5-ic/"
    symbols: ["EURUSD", "GBPUSD", "USDJPY", "USDCHF", "AUDUSD"]
    priority: 1  # Primary broker

  pepperstone:
    name: "Pepperstone"
    server: "Pepperstone-Demo"
    mt5_path: "/opt/mt5-pepper/"
    symbols: ["EURUSD", "GBPUSD", "USDJPY", "EURGBP", "EURJPY"]
    priority: 2  # Secondary broker

  fxcm:
    name: "FXCM"
    server: "FXCM-USDDemo01"
    mt5_path: "/opt/mt5-fxcm/"
    symbols: ["EURUSD", "GBPUSD", "USDJPY", "USDCAD", "GBPJPY"]
    priority: 3  # Tertiary broker
```

### **Broker Quality Comparison**:
```python
class BrokerQualityTracker:
    def compare_broker_feeds(self, symbol: str) -> BrokerComparison:
        """Compare data quality across brokers untuk symbol"""

        broker_metrics = {}

        for broker in self.active_brokers:
            metrics = self.get_broker_metrics(broker, symbol)
            broker_metrics[broker] = {
                "latency_ms": metrics.avg_latency,
                "spread_avg": metrics.avg_spread,
                "uptime_percent": metrics.uptime,
                "data_completeness": metrics.completeness,
                "tick_frequency": metrics.ticks_per_second
            }

        # Rank brokers by composite score
        best_broker = self.calculate_best_broker(broker_metrics)

        return BrokerComparison(
            best_broker=best_broker,
            metrics=broker_metrics,
            recommendation=f"Use {best_broker} as primary source for {symbol}"
        )
```

---

## ‚ö° Performance Optimizations

### **Connection Pooling & Management**:
```python
class ConnectionManager:
    def __init__(self):
        self.mt5_pools = {}      # Connection pools per broker
        self.health_checker = BrokerHealthChecker()
        self.failover_manager = FailoverManager()

    async def maintain_connections(self):
        """Maintain optimal broker connections"""

        while True:
            for broker_name, pool in self.mt5_pools.items():
                # Health check
                if not await self.health_checker.check_broker(broker_name):
                    await self.failover_manager.handle_broker_failure(broker_name)
                    continue

                # Connection optimization
                if pool.active_connections < pool.optimal_connections:
                    await pool.add_connection()
                elif pool.active_connections > pool.optimal_connections:
                    await pool.remove_connection()

            await asyncio.sleep(30)  # Check every 30 seconds
```

### **Data Deduplication & Quality Control**:
```python
class DataQualityManager:
    def __init__(self):
        self.tick_cache = TTLCache(maxsize=10000, ttl=60)  # 1 minute cache
        self.quality_thresholds = {
            "max_spread": 0.005,     # 5 pips maximum spread
            "min_tick_interval": 50,  # 50ms minimum between ticks
            "max_price_change": 0.02  # 2% maximum price change
        }

    async def validate_tick_quality(self, tick: MarketTick) -> ValidationResult:
        """Validate tick data quality"""

        errors = []

        # Spread validation
        spread = tick.ask - tick.bid
        if spread > self.quality_thresholds["max_spread"]:
            errors.append(f"Excessive spread: {spread}")

        # Price movement validation
        last_price = self.tick_cache.get(f"{tick.symbol}_price")
        if last_price:
            price_change = abs(tick.bid - last_price) / last_price
            if price_change > self.quality_thresholds["max_price_change"]:
                errors.append(f"Excessive price movement: {price_change:.2%}")

        # Update cache
        self.tick_cache[f"{tick.symbol}_price"] = tick.bid

        return ValidationResult(
            is_valid=len(errors) == 0,
            errors=errors,
            confidence=1.0 - (len(errors) * 0.2)
        )
```

---

## üîó Integration Points

### **Service Dependencies**:
- **OANDA v20 API**: Direct connection to primary data source
- **NATS/Kafka Cluster**: Secure message streaming untuk processed data
- **Central Hub**: Service discovery, registration, dan health reporting
- **External Data APIs**: 8 data collectors (Yahoo Finance, FRED, etc.)

### **External Dependencies**:
- **OANDA v20 Servers**: Primary market data provider
- **External API Providers**: News, calendar, sentiment data sources
- **NATS/Kafka Infrastructure**: High-throughput message distribution
- **Monitoring System**: Real-time data collection monitoring

---

## üéØ Business Value

### **Cost Efficiency**:
- **99.9% Infrastructure Savings**: From 3000 to 3 broker connections
- **99.7% Bandwidth Reduction**: Eliminate redundant data streams
- **99.8% Processing Savings**: Single aggregation vs multiple processing
- **Linear Scaling**: Add users without additional broker connections

### **Data Quality**:
- **Multi-Broker Redundancy**: Best price selection across brokers
- **Real-time Quality Control**: Automated spread dan price validation
- **Broker Performance Tracking**: Data-driven broker selection
- **Failover Capability**: Automatic broker switching untuk reliability

### **User Experience**:
- **<1ms Data Latency**: Optimized distribution pipeline
- **99.9% Uptime**: Multi-broker redundancy dan failover
- **Consistent Pricing**: Quality-controlled data feeds
- **Scalable Architecture**: Support 10,000+ concurrent users

---

## üìä Monitoring & Observability

### **Broker Connection Health**:
```python
@app.get("/health")
async def ingestion_health_check():
    """Comprehensive ingestion service health"""
    return {
        "service_name": "data-ingestion",
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "1.0.0",
        "uptime_seconds": await get_uptime_seconds(),
        "broker_connections": {
            broker: {
                "status": await check_broker_health(broker),
                "active_symbols": await get_active_symbols(broker),
                "ticks_per_second": await get_broker_throughput(broker),
                "latency_ms": await get_broker_latency(broker)
            } for broker in ["ic_markets", "pepperstone", "fxcm"]
        },
        "aggregation_metrics": {
            "total_symbols_tracked": await count_active_symbols(),
            "subscribers_count": await count_active_subscribers(),
            "distribution_latency_ms": await get_distribution_latency()
        },
        "quality_metrics": {
            "data_completeness_percent": await get_data_completeness(),
            "avg_spread_quality": await get_spread_quality(),
            "broker_sync_status": await get_broker_sync_status()
        }
    }
```

---

**Input Flow**: OANDA v20 API + External Data APIs ‚Üí Data Ingestion (aggregation & quality control)
**Output Flow**: Data Ingestion ‚Üí NATS/Kafka (secure streaming) ‚Üí Data Bridge (direct consumer subscription)
**Key Innovation**: Server-side data aggregation dengan secure message queue streaming untuk optimal performance, security, dan scalability