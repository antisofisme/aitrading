# External Data Collector

Economic Calendar and market data collection system with **Central Hub integration**.

## ğŸ¯ New Features (Central Hub Integration)

### âœ… What Changed

1. **Central Hub SDK Integration**
   - Service registration & discovery
   - Periodic heartbeat with metrics
   - Graceful deregistration

2. **YAML Configuration System**
   - `config/scrapers.yaml` - Centralized configuration
   - Environment variable support
   - Hot-reload ready

3. **Modern Architecture**
   - `src/main.py` - Service orchestrator with Central Hub
   - `src/config/` - Configuration module
   - Clean separation of concerns

4. **Production Ready**
   - Dockerfile with SDK installation
   - docker-compose.yml with Central Hub dependency
   - Health checks and monitoring

### ğŸ“¦ File Structure

```
external-data-collector/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                          # Central Hub integrated service
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ config.py                    # YAML config loader
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â””â”€â”€ mql5_historical_scraper.py   # MQL5 Economic Calendar
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ date_tracker.py              # Date tracking utility
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ scrapers.yaml                    # Service configuration
â”‚
â”œâ”€â”€ Dockerfile                            # With Central Hub SDK
â”œâ”€â”€ docker-compose.yml                    # Full stack deployment
â”œâ”€â”€ requirements.txt                      # Dependencies (no Playwright!)
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ HISTORICAL_SCRAPING_STRATEGY.md  # Technical details
â”‚
â”œâ”€â”€ README.md                             # This file
â”œâ”€â”€ CENTRAL_HUB_INTEGRATION.md           # Integration guide
â”œâ”€â”€ DEPLOYMENT.md                         # Deployment instructions
â””â”€â”€ PROJECT_STRUCTURE.md                  # Project structure docs
```

## ğŸš€ Quick Start

### 1. Build & Deploy with Central Hub

```bash
# Build from backend root
cd /mnt/g/khoirul/aitrading/project3/backend

docker build \
  -f 00-data-ingestion/external-data-collector/Dockerfile \
  -t external-data-collector:latest \
  .

# Deploy with docker-compose
cd 00-data-ingestion/external-data-collector
docker-compose up -d

# Check logs
docker logs -f external-data-collector
```

### 2. Expected Output

```
================================================================================
EXTERNAL DATA COLLECTOR + CENTRAL HUB
================================================================================
Instance ID: external-data-collector-1
Log Level: INFO
Central Hub: Enabled
ğŸš€ Starting External Data Collector...
âœ… Registered with Central Hub: external-data-collector
ğŸ”§ Initializing scrapers...
âœ… Initialized MQL5 Economic Calendar scraper
   Z.ai Parser: Enabled
   Database: JSON fallback
ğŸ”„ Starting scraping loop for mql5_economic_calendar (interval: 3600s)
âœ… External Data Collector started with 1 scrapers
```

## âš™ï¸ Configuration

### Environment Variables

```bash
# Service identity
INSTANCE_ID=external-data-collector-1
LOG_LEVEL=INFO

# Central Hub
CENTRAL_HUB_URL=http://suho-central-hub:7000
HEARTBEAT_INTERVAL=30

# API Keys
ZAI_API_KEY=your-key-here

# Database (optional)
DB_HOST=suho-postgresql
DB_PORT=5432
DB_NAME=aitrading
```

### YAML Configuration (`config/scrapers.yaml`)

```yaml
scrapers:
  mql5_economic_calendar:
    enabled: true
    source: mql5.com
    scrape_interval: 3600  # 1 hour

storage:
  type: json  # or postgresql

backfill:
  enabled: false  # Set true for 1-year backfill
  months_back: 12
```

## ğŸ“Š Features

### MQL5 Economic Calendar Scraper

- âœ… **aiohttp + BeautifulSoup** - No Playwright (no bot detection)
- âœ… **Z.ai Integration** - AI-powered adaptive HTML parsing
- âœ… **Smart Date Tracking** - Automatic missing date detection
- âœ… **Incremental Scraping** - Only update what's needed
- âœ… **Historical Backfill** - 1-year+ data collection

### Central Hub Features

- âœ… **Service Registration** - Auto-register on startup
- âœ… **Periodic Heartbeat** - Health & metrics reporting
- âœ… **Graceful Shutdown** - Proper deregistration
- âœ… **Metrics Tracking** - Events, dates, errors

### Data Quality

From production tests (405 events, 8 days):
- 87.9% have forecast values
- 95.1% have previous values
- 69.4% have actual values
- 64.4% complete (F+P+A)

## ğŸ”„ Scraping Strategy

### Three Modes

1. **Initial Backfill** (One-time)
   ```bash
   # Set in config/scrapers.yaml
   backfill:
     enabled: true
     months_back: 12
   ```

2. **Daily Updates** (Automatic)
   - Runs every 1 hour (configurable)
   - Updates recent 7 days for actual values

3. **Custom Scraping** (Manual)
   ```bash
   docker exec -it external-data-collector \
     python3 scripts/run_backfill.py backfill --months 6
   ```

## ğŸ“ˆ Monitoring

### Central Hub Dashboard

```bash
curl http://suho-central-hub:7000/api/discovery/services

# Response includes:
{
  "name": "external-data-collector",
  "status": "healthy",
  "metrics": {
    "events_scraped": 18000,
    "dates_tracked": 250,
    "errors": 0
  }
}
```

### Service Metrics

- `events_scraped` - Total events collected
- `dates_tracked` - Total dates in coverage
- `errors` - Error count
- `last_scrape` - Last successful scrape

## ğŸ›¡ï¸ Error Handling

### Fallback Behavior

- **No Central Hub**: Runs in standalone mode
- **No Database**: Falls back to JSON file tracking
- **No Z.ai Key**: Uses regex parser (still works!)

## ğŸ“š Documentation

- **[CENTRAL_HUB_INTEGRATION.md](CENTRAL_HUB_INTEGRATION.md)** - Integration guide
- **[DEPLOYMENT.md](DEPLOYMENT.md)** - Deployment instructions
- **[PROJECT_STRUCTURE.md](PROJECT_STRUCTURE.md)** - File structure
- **[docs/HISTORICAL_SCRAPING_STRATEGY.md](docs/HISTORICAL_SCRAPING_STRATEGY.md)** - Technical strategy

## ğŸ”§ Development

### Run Tests

```bash
# Build test image
docker build -f Dockerfile.historical -t mql5-test .

# Run tests
docker run --rm mql5-test
```

### Manual Scraping

```bash
# Enter container
docker exec -it external-data-collector bash

# Run backfill
python3 scripts/run_backfill.py backfill --months 12

# Update recent data
python3 scripts/run_backfill.py update --days 7

# Check coverage
python3 scripts/run_backfill.py report
```

## ğŸ¯ Integration Pattern

Follows same pattern as other collectors:

- **polygon-live-collector** âœ…
- **polygon-historical-downloader** âœ…
- **external-data-collector** âœ… (THIS)

All use:
- Central Hub SDK
- YAML configuration
- Service registration
- Heartbeat monitoring

## ğŸ“ Changes Summary

| Aspect | Before | After |
|--------|--------|-------|
| **Integration** | Standalone | Central Hub SDK |
| **Configuration** | Hardcoded | YAML + environment |
| **Service Discovery** | None | Auto-registration |
| **Monitoring** | Manual | Automated heartbeat |
| **Architecture** | Script-based | Service-based |

## ğŸš¨ Migration from Old Setup

Old files moved to `_archived/`:
- `economic_calendar_ai.py` - Old ForexFactory scraper
- `test_playwright_stealth.py` - Bot detection tests
- All Playwright/Selenium dependencies removed

## ğŸ‰ Production Status

**âœ… Ready for Production**

- All tests passed (4/4)
- Central Hub integrated
- Configuration system ready
- Documentation complete
- Docker deployment ready

---

**Version**: 2.0.0 (With Central Hub)
**Last Updated**: 2025-10-05
**Status**: Production Ready âœ…
