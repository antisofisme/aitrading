services:
  # ================================
  # DATABASE INFRASTRUCTURE SERVICES
  # ================================

  # PostgreSQL with TimescaleDB - Primary OLTP Database
  postgresql:
    image: timescale/timescaledb-ha:pg16
    container_name: suho-postgresql
    restart: unless-stopped
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=suho_trading
      - POSTGRES_USER=suho_admin
      - POSTGRES_PASSWORD=suho_secure_password_2024
      - POSTGRES_MULTIPLE_DATABASES=suho_trading,suho_analytics
      - TS_TUNE_MEMORY=4GB
      - TS_TUNE_NUM_CPUS=4
    volumes:
      - postgresql_data:/home/postgres/pgdata/data
      - ./01-core-infrastructure/central-hub/base/config/database/init-scripts:/docker-entrypoint-initdb.d
    command:
      - postgres
      - -c
      - shared_preload_libraries=timescaledb
      - -c
      - max_connections=200
      - -c
      - shared_buffers=512MB
      - -c
      - effective_cache_size=1GB
    networks:
      - suho-trading-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U suho_admin -d suho_trading"]
      interval: 30s
      timeout: 10s
      retries: 5
    labels:
      - "com.suho.service=postgresql"
      - "com.suho.type=primary-database"

  # ClickHouse - OLAP Analytics Database
  clickhouse:
    image: clickhouse/clickhouse-server:24.8
    container_name: suho-clickhouse
    restart: unless-stopped
    ports:
      - "8123:8123"   # HTTP interface
      - "9000:9000"   # Native client
    environment:
      - CLICKHOUSE_DB=suho_analytics
      - CLICKHOUSE_USER=suho_analytics
      - CLICKHOUSE_PASSWORD=clickhouse_secure_2024
      - CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT=1
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - clickhouse_logs:/var/log/clickhouse-server
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    networks:
      - suho-trading-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8123/ping"]
      interval: 30s
      timeout: 10s
      retries: 5
    labels:
      - "com.suho.service=clickhouse"
      - "com.suho.type=analytics-database"

  # DragonflyDB - High-performance Redis Cache
  dragonflydb:
    image: docker.dragonflydb.io/dragonflydb/dragonfly:v1.21.2
    container_name: suho-dragonflydb
    restart: unless-stopped
    ports:
      - "6379:6379"
    command: ["dragonfly", "--logtostderr", "--requirepass=dragonfly_secure_2024"]
    volumes:
      - dragonflydb_data:/data
    networks:
      - suho-trading-network
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "dragonfly_secure_2024", "ping"]
      interval: 15s
      timeout: 5s
      retries: 3
    labels:
      - "com.suho.service=dragonflydb"
      - "com.suho.type=cache-database"

  # Weaviate - Vector Database for AI/ML
  weaviate:
    image: cr.weaviate.io/semitechnologies/weaviate:1.26.1
    container_name: suho-weaviate
    restart: unless-stopped
    ports:
      - "8080:8080"
      - "50051:50051"
    environment:
      - QUERY_DEFAULTS_LIMIT=25
      - AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true
      - PERSISTENCE_DATA_PATH=/var/lib/weaviate
      - DEFAULT_VECTORIZER_MODULE=none
      - ENABLE_MODULES=
      - CLUSTER_HOSTNAME=weaviate-node1
      - LOG_LEVEL=info
    volumes:
      - weaviate_data:/var/lib/weaviate
    networks:
      - suho-trading-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/v1/.well-known/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
    labels:
      - "com.suho.service=weaviate"
      - "com.suho.type=vector-database"

  # ArangoDB - Multi-model Graph Database
  arangodb:
    image: arangodb:3.12.1
    container_name: suho-arangodb
    restart: unless-stopped
    ports:
      - "8529:8529"
    environment:
      - ARANGO_NO_AUTH=0
      - ARANGO_ROOT_PASSWORD=arango_secure_password_2024
    command:
      - arangod
      - --server.endpoint=tcp://0.0.0.0:8529
      - --database.directory=/var/lib/arangodb3
      - --log.level=info
      - --server.authentication=true
      - --server.jwt-secret=your-jwt-secret-change-in-production
    volumes:
      - arangodb_data:/var/lib/arangodb3
      - arangodb_apps:/var/lib/arangodb3-apps
    networks:
      - suho-trading-network
    labels:
      - "com.suho.service=arangodb"
      - "com.suho.type=graph-database"

  # NATS Server with JetStream for message streaming
  nats:
    image: nats:2.10-alpine
    container_name: suho-nats-server
    restart: unless-stopped
    ports:
      - "4222:4222"   # Client connections
      - "8222:8222"   # HTTP monitoring
    command: ["nats-server", "--jetstream", "--store_dir=/data", "--http_port=8222"]
    volumes:
      - nats_data:/data
    networks:
      - suho-trading-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8222/healthz"]
      interval: 15s
      timeout: 5s
      retries: 3
    labels:
      - "com.suho.service=nats"
      - "com.suho.type=message-broker"

  # Zookeeper for Kafka coordination
  zookeeper:
    image: confluentinc/cp-zookeeper:7.7.0
    container_name: suho-zookeeper
    restart: unless-stopped
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_logs:/var/lib/zookeeper/log
    networks:
      - suho-trading-network
    healthcheck:
      test: ["CMD", "bash", "-c", "echo ruok | nc localhost 2181"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      - "com.suho.service=zookeeper"

  # Kafka for durable message streaming
  kafka:
    image: confluentinc/cp-kafka:7.7.0
    container_name: suho-kafka
    restart: unless-stopped
    ports:
      - "9092:9092"
      - "9101:9101"
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: suho-zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://suho-kafka:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 6
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_LOG_RETENTION_MS: 604800000  # 7 days (CRITICAL: Extended for ClickHouse recovery)
      KAFKA_LOG_SEGMENT_BYTES: 104857600  # 100MB
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - suho-trading-network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "suho-kafka:9092"]
      interval: 30s
      timeout: 10s
      retries: 5
    labels:
      - "com.suho.service=kafka"
      - "com.suho.type=message-broker"

  # ================================
  # APPLICATION SERVICES
  # ================================

  # Central Hub - Configuration and Component Management
      # Economic Calendar Historical Data
      - ECONOMIC_CALENDAR_HISTORY_MONTHS=${ECONOMIC_CALENDAR_HISTORY_MONTHS:-1}

  central-hub:
    build:
      context: ./01-core-infrastructure/central-hub
      dockerfile: Dockerfile  # Changed from Dockerfile.wheelhouse for new dependencies
    container_name: suho-central-hub
    restart: unless-stopped
    ports:
      - "7000:7000"
    environment:
      # === AUTO-SYNC WITH shared/static/ configs ===
      # Database passwords (synced from shared/static/database/*.json)
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - CLICKHOUSE_PASSWORD=${CLICKHOUSE_PASSWORD}
      - DRAGONFLY_PASSWORD=${DRAGONFLY_PASSWORD}
      - ARANGO_ROOT_PASSWORD=${ARANGO_ROOT_PASSWORD}
      - ARANGO_JWT_SECRET=${ARANGO_JWT_SECRET}

      # === LEGACY SUPPORT (backward compatibility) ===
      - NODE_ENV=${NODE_ENV:-development}
      - PYTHONPATH=${PYTHONPATH:-/app}
      - DATABASE_URL=${DATABASE_URL}
      - CACHE_URL=${CACHE_URL}
      - NATS_URL=${NATS_URL}
      - KAFKA_BROKERS=${KAFKA_BROKERS}

      # === POSTGRES CONNECTION DETAILS ===
      - POSTGRES_HOST=${POSTGRES_HOST}
      - POSTGRES_PORT=${POSTGRES_PORT}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}

      # === DRAGONFLY CONNECTION DETAILS ===
      - DRAGONFLY_HOST=${DRAGONFLY_HOST}
      - DRAGONFLY_PORT=${DRAGONFLY_PORT}

      # === CLICKHOUSE CONNECTION DETAILS ===
      - CLICKHOUSE_HOST=${CLICKHOUSE_HOST}
      - CLICKHOUSE_PORT=${CLICKHOUSE_PORT}
      - CLICKHOUSE_DB=${CLICKHOUSE_DB}
      - CLICKHOUSE_USER=${CLICKHOUSE_USER}
      - CLICKHOUSE_PASSWORD=${CLICKHOUSE_PASSWORD}

      # === NATS CONNECTION DETAILS ===
      - NATS_HOST=suho-nats-server
      - NATS_PORT=4222

      # === SERVICE SETTINGS ===
      - HOT_RELOAD_ENABLED=${HOT_RELOAD_ENABLED:-false}
      - COMPONENT_WATCH_ENABLED=${COMPONENT_WATCH_ENABLED:-false}
    volumes:
      # Mount shared components for hot reload
      - ./01-core-infrastructure/central-hub/shared:/app/shared:rw
      - ./01-core-infrastructure/central-hub/base:/app/base:rw
      - ./01-core-infrastructure/central-hub/contracts:/app/contracts:rw
      # Volume for component cache
      - central-hub-components:/app/shared/.component_cache
    networks:
      - suho-trading-network
    depends_on:
      - postgresql
      - dragonflydb
      - nats
      - kafka
    labels:
      - "com.suho.service=central-hub"
      - "com.suho.type=core-service"


  # ================================
  # DATA INGESTION SERVICES
  # ================================

  # Polygon.io Live Collector - Real-time + OHLCV Data
  live-collector:
    build:
      context: .
      dockerfile: 00-data-ingestion/polygon-live-collector/Dockerfile
    container_name: suho-live-collector
    hostname: suho-live-collector
    restart: unless-stopped
    environment:
      - INSTANCE_ID=polygon-live-collector-1
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - POLYGON_API_KEY=${POLYGON_API_KEY}
      - NATS_URL=nats://suho-nats-server:4222
      - KAFKA_BROKERS=suho-kafka:9092
      - CENTRAL_HUB_URL=http://suho-central-hub:7000
      - HEARTBEAT_INTERVAL=30
    volumes:
      - ./00-data-ingestion/polygon-live-collector/config:/app/config:ro
      - polygon_live_logs:/var/log/polygon-live-collector
    networks:
      - suho-trading-network
    depends_on:
      nats:
        condition: service_healthy
      kafka:
        condition: service_healthy
      central-hub:
        condition: service_started
    labels:
      - "com.suho.service=polygon-live-collector"
      - "com.suho.type=data-collector"

  # Polygon Historical Downloader - Downloads historical data → NATS/Kafka
  historical-downloader:
    build:
      context: .
      dockerfile: 00-data-ingestion/polygon-historical-downloader/Dockerfile
    container_name: suho-historical-downloader
    hostname: suho-historical-downloader
    restart: on-failure:3  # Restart up to 3 times on failure (OOM, crashes)
    environment:
      - INSTANCE_ID=polygon-historical-downloader-1
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - POLYGON_API_KEY=${POLYGON_API_KEY}
      - HISTORICAL_START_DATE=${HISTORICAL_START_DATE:-2023-01-01}
      - HISTORICAL_END_DATE=${HISTORICAL_END_DATE:-now}
      - NATS_URL=nats://suho-nats-server:4222
      - KAFKA_BROKERS=suho-kafka:9092
      - CENTRAL_HUB_URL=http://suho-central-hub:7000
    volumes:
      # Use named volumes instead of bind mounts (WSL Docker Desktop fix)
      - polygon_historical_logs:/var/log/polygon-historical-downloader
    deploy:
      resources:
        limits:
          memory: 4G  # Increased for 10-year download (was 2G)
        reservations:
          memory: 1G
    networks:
      - suho-trading-network
    depends_on:
      nats:
        condition: service_healthy
      kafka:
        condition: service_healthy
      central-hub:
        condition: service_started
    labels:
      - "com.suho.service=polygon-historical-downloader"
      - "com.suho.type=data-downloader"

  # External Data Collector - Economic Calendar & Market Data (MQL5, etc)
  external-data-collector:
    build:
      context: .
      dockerfile: 00-data-ingestion/external-data-collector/Dockerfile
    container_name: suho-external-collector
    hostname: suho-external-collector
    restart: unless-stopped
    environment:
      # Service identity
      - INSTANCE_ID=external-data-collector-1
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

      # Central Hub
      # Economic Calendar Historical Data
      - ECONOMIC_CALENDAR_HISTORY_MONTHS=${ECONOMIC_CALENDAR_HISTORY_MONTHS:-1}

      - CENTRAL_HUB_URL=http://suho-central-hub:7000
      - HEARTBEAT_INTERVAL=30

      # API Keys for external services
      - ZAI_API_KEY=${ZAI_API_KEY:-}
      - FRED_API_KEY=${FRED_API_KEY:-}
      - COINGECKO_API_KEY=${COINGECKO_API_KEY:-}

      # Database (optional - falls back to JSON if not set)
      - DB_HOST=suho-postgresql
      - DB_PORT=5432
      - DB_NAME=suho_trading
      - DB_USER=suho_admin
      - DB_PASSWORD=${POSTGRES_PASSWORD:-suho_secure_password_2024}

      # Messaging (optional - for future data streaming)
      - NATS_URL=nats://suho-nats-server:4222
      - KAFKA_BROKERS=suho-kafka:9092

      # Python
      - PYTHONUNBUFFERED=1
    volumes:
      - ./00-data-ingestion/external-data-collector/config:/app/config:ro
      - external_collector_data:/app/data
      - external_collector_logs:/app/logs
    networks:
      - suho-trading-network
    depends_on:
      nats:
        condition: service_healthy
      postgresql:
        condition: service_healthy
      central-hub:
        condition: service_started
    labels:
      - "com.suho.service=external-data-collector"
      - "com.suho.type=data-collector"

  # ===================================
  # DATA PROCESSING SERVICES
  # ===================================

  # Data Bridge - NATS+Kafka → Database Manager → TimescaleDB + DragonflyDB
  data-bridge:
    build:
      context: .  # Root context for SDK access
      dockerfile: 02-data-processing/data-bridge/Dockerfile
    container_name: suho-data-bridge
    hostname: suho-data-bridge
    restart: unless-stopped
    environment:
      - INSTANCE_ID=data-bridge-1
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      # Central Hub URL (for config distribution)
      # Economic Calendar Historical Data
      - ECONOMIC_CALENDAR_HISTORY_MONTHS=${ECONOMIC_CALENDAR_HISTORY_MONTHS:-1}

      - CENTRAL_HUB_URL=http://suho-central-hub:7000
      # Database credentials (used by Database Manager)
      - POSTGRES_HOST=suho-postgresql
      - POSTGRES_PORT=5432
      - POSTGRES_DB=suho_trading
      - POSTGRES_USER=suho_admin
      - POSTGRES_PASSWORD=suho_secure_password_2024
      - DRAGONFLY_HOST=suho-dragonflydb
      - DRAGONFLY_PORT=6379
      - DRAGONFLY_PASSWORD=dragonfly_secure_2024
      # Messaging (fallback if Central Hub unavailable)
      - NATS_URL=nats://suho-nats-server:4222
      - KAFKA_BROKERS=suho-kafka:9092
    volumes:
      - ./02-data-processing/data-bridge/config:/app/config:ro
      - ./01-core-infrastructure/central-hub/shared:/app/central-hub/shared:ro  # Access to Database Manager
      - data_bridge_logs:/app/logs
    networks:
      - suho-trading-network
    depends_on:
      nats:
        condition: service_healthy
      kafka:
        condition: service_healthy
      postgresql:
        condition: service_healthy
      dragonflydb:
        condition: service_healthy
      central-hub:
        condition: service_started
    labels:
      - "com.suho.service=data-bridge"
      - "com.suho.type=data-processor"

  # Tick Aggregator - TimescaleDB Ticks → Aggregated Candles with Indicators → NATS
  tick-aggregator:
    build:
      context: .  # Root context for SDK access
      dockerfile: 02-data-processing/tick-aggregator/Dockerfile
    container_name: suho-tick-aggregator
    hostname: suho-tick-aggregator
    restart: unless-stopped
    environment:
      - INSTANCE_ID=tick-aggregator-1
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      # TimescaleDB connection
      - TIMESCALEDB_HOST=${TIMESCALEDB_HOST}
      - TIMESCALEDB_PORT=${TIMESCALEDB_PORT}
      - TIMESCALEDB_DB=${TIMESCALEDB_DB}
      - TIMESCALEDB_USER=${TIMESCALEDB_USER}
      - TIMESCALEDB_PASSWORD=${TIMESCALEDB_PASSWORD}
      # ClickHouse for gap detection (native client needs port 9000, not HTTP 8123)
      - CLICKHOUSE_HOST=${CLICKHOUSE_HOST:-suho-clickhouse}
      - CLICKHOUSE_PORT=9000  # Native protocol port (NOT HTTP 8123)
      - CLICKHOUSE_DATABASE=${CLICKHOUSE_DATABASE:-suho_analytics}
      - CLICKHOUSE_USER=${CLICKHOUSE_USER:-suho_analytics}
      - CLICKHOUSE_PASSWORD=${CLICKHOUSE_PASSWORD}
      # NATS for publishing aggregates
      - NATS_URL=nats://suho-nats-server:4222
    volumes:
      - ./02-data-processing/tick-aggregator/config:/app/config:ro
      - ./01-core-infrastructure/central-hub/shared:/app/shared:ro
      - tick_aggregator_logs:/app/logs
    networks:
      - suho-trading-network
    depends_on:
      postgresql:
        condition: service_healthy
      nats:
        condition: service_healthy
    labels:
      - "com.suho.service=tick-aggregator"
      - "com.suho.type=data-processor"

  # ClickHouse Consumer - NATS+Kafka → ClickHouse (DEPRECATED - Use data-bridge instead)
  # clickhouse-consumer:
  #   build:
  #     context: ./clickhouse-consumer
  #     dockerfile: Dockerfile
  #   container_name: suho-clickhouse-consumer
  #   hostname: suho-clickhouse-consumer
  #   restart: unless-stopped
  #   environment:
  #     - INSTANCE_ID=clickhouse-consumer-1
  #     - LOG_LEVEL=${LOG_LEVEL:-INFO}
  #     - CLICKHOUSE_PASSWORD=${CLICKHOUSE_PASSWORD}
  #     - NATS_URL=nats://suho-nats-server:4222
  #     - KAFKA_BROKERS=suho-kafka:9092
  #   volumes:
  #     - ./00-data-ingestion/clickhouse-consumer/config:/app/config:ro
  #     - clickhouse_consumer_logs:/var/log/clickhouse-consumer
  #   networks:
  #     - suho-trading-network
  #   depends_on:
  #     nats:
  #       condition: service_healthy
  #     kafka:
  #       condition: service_healthy
  #     clickhouse:
  #       condition: service_healthy
  #   healthcheck:
  #     test: ["CMD", "python3", "-c", "import sys; sys.exit(0)"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 40s
  #   labels:
  #     - "com.suho.service=clickhouse-consumer"
  #     - "com.suho.type=data-consumer"

  # ===================================
  # MACHINE LEARNING SERVICES
  # ===================================

  # Feature Engineering Service - Generates 63 ML features from OHLCV + External Data
  feature-engineering-service:
    build:
      context: .
      dockerfile: 03-machine-learning/feature-engineering-service/Dockerfile
    container_name: suho-feature-engineering
    hostname: suho-feature-engineering
    restart: unless-stopped
    environment:
      - INSTANCE_ID=feature-engineering-1
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      # Central Hub
      - CENTRAL_HUB_URL=http://suho-central-hub:7000
      - HEARTBEAT_INTERVAL=30
      # ClickHouse connection
      - CLICKHOUSE_HOST=${CLICKHOUSE_HOST:-suho-clickhouse}
      - CLICKHOUSE_PORT=8123  # HTTP interface
      - CLICKHOUSE_DATABASE=${CLICKHOUSE_DATABASE:-suho_analytics}
      - CLICKHOUSE_USER=${CLICKHOUSE_USER:-suho_analytics}
      - CLICKHOUSE_PASSWORD=${CLICKHOUSE_PASSWORD}
      # Service settings
      - SERVICE_NAME=feature-engineering-service
      - SERVICE_TYPE=ml-feature-engineering
      - SERVICE_VERSION=1.0.0
      - PYTHONUNBUFFERED=1
    volumes:
      - ./03-machine-learning/feature-engineering-service/config:/app/config:ro
      - feature_engineering_logs:/app/logs
    networks:
      - suho-trading-network
    depends_on:
      clickhouse:
        condition: service_healthy
      central-hub:
        condition: service_started
    labels:
      - "com.suho.service=feature-engineering"
      - "com.suho.type=ml-service"

  # API Gateway - Main Entry Point (OFFLINE NODE_MODULES)
  api-gateway:
    build:
      context: .
      dockerfile: 01-core-infrastructure/api-gateway/Dockerfile.offline
    container_name: suho-api-gateway
    restart: unless-stopped
    ports:
      - "8000:8000"   # HTTP API server
      - "8001:8001"   # Trading WebSocket channel
      - "8002:8002"   # Price Stream WebSocket channel
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=${API_GATEWAY_PORT}
      - DATABASE_URL=${DATABASE_URL}
      - POSTGRES_HOST=${POSTGRES_HOST}
      - POSTGRES_PORT=${POSTGRES_PORT}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - CACHE_URL=${CACHE_URL}
      - NATS_URL=${NATS_URL}
      - KAFKA_BROKERS=${KAFKA_BROKERS}
      - CENTRAL_HUB_URL=${CENTRAL_HUB_URL}
      - JWT_SECRET=${JWT_SECRET}
      - JWT_EXPIRES_IN=${JWT_EXPIRES_IN}
      - JWT_ISSUER=${JWT_ISSUER}
      - JWT_AUDIENCE=${JWT_AUDIENCE}
      - LOG_LEVEL=${LOG_LEVEL}
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN}
      - HOT_RELOAD_ENABLED=${HOT_RELOAD_ENABLED}
      - CORS_ENABLED=${CORS_ENABLED}
      - WEBSOCKET_ENABLED=${WEBSOCKET_ENABLED}
      - KAFKAJS_NO_PARTITIONER_WARNING=${KAFKAJS_NO_PARTITIONER_WARNING}
    volumes:
      # API Gateway source code
      - ./01-core-infrastructure/api-gateway/src:/app/src:rw
      # Central hub shared components for direct access
      - ./01-core-infrastructure/central-hub/shared:/app/shared/central-hub:ro
      # Logs and contracts from individual compose
      - ./01-core-infrastructure/api-gateway/logs:/app/logs
      - ./01-core-infrastructure/api-gateway/contracts:/app/contracts:ro
      # Create hot cache directory
      - api-gateway-cache:/app/.hot_cache
    networks:
      - suho-trading-network
    depends_on:
      - postgresql
      - dragonflydb
      - nats
      - kafka
      - central-hub
    labels:
      - "com.suho.service=api-gateway"
      - "com.suho.type=gateway-service"

# ================================
# VOLUMES
# ================================
volumes:
  # Database storage volumes
  postgresql_data:
    driver: local
    labels:
      - "com.suho.data=postgresql"

  clickhouse_data:
    driver: local
    labels:
      - "com.suho.data=clickhouse"

  clickhouse_logs:
    driver: local
    labels:
      - "com.suho.data=clickhouse-logs"

  dragonflydb_data:
    driver: local
    labels:
      - "com.suho.data=dragonflydb"

  weaviate_data:
    driver: local
    labels:
      - "com.suho.data=weaviate"

  arangodb_data:
    driver: local
    labels:
      - "com.suho.data=arangodb"

  arangodb_apps:
    driver: local
    labels:
      - "com.suho.data=arangodb-apps"

  # Message broker storage
  nats_data:
    driver: local
    labels:
      - "com.suho.data=nats"

  kafka_data:
    driver: local
    labels:
      - "com.suho.data=kafka"

  zookeeper_data:
    driver: local
    labels:
      - "com.suho.data=zookeeper"

  zookeeper_logs:
    driver: local
    labels:
      - "com.suho.data=zookeeper-logs"

  # Application volumes
  central-hub-components:
    driver: local
    labels:
      - "com.suho.data=central-hub-components"

  api-gateway-cache:
    driver: local
    labels:
      - "com.suho.data=api-gateway-cache"

  # Polygon.io services
  polygon_live_logs:
    driver: local
    labels:
      - "com.suho.data=polygon-live-logs"

  polygon_historical_logs:
    driver: local
    labels:
      - "com.suho.data=polygon-historical-logs"

  external_collector_data:
    driver: local
    labels:
      - "com.suho.data=external-collector-data"

  external_collector_logs:
    driver: local
    labels:
      - "com.suho.data=external-collector-logs"

  # Data processing services
  data_bridge_logs:
    driver: local
    labels:
      - "com.suho.data=data-bridge-logs"

  tick_aggregator_logs:
    driver: local
    labels:
      - "com.suho.data=tick-aggregator-logs"

  # Machine learning services
  feature_engineering_logs:
    driver: local
    labels:
      - "com.suho.data=feature-engineering-logs"

  # clickhouse_consumer_logs:  # DEPRECATED - Use data-bridge instead
  #   driver: local
  #   labels:
  #     - "com.suho.data=clickhouse-consumer-logs"

# ================================
# NETWORKS
# ================================
networks:
  suho-trading-network:
    driver: bridge
    name: suho-trading-network
    labels:
      - "com.suho.network=main"