"""
hybrid_bridge.py - Hybrid Bridge CLI Application

üéØ PURPOSE:
Business: Main CLI interface for hybrid MT5-WebSocket trading bridge
Technical: Coordinated MT5 and WebSocket integration with command-line interface
Domain: CLI/Bridge Application/User Interface

ü§ñ AI GENERATION INFO:
Generated: 2025-08-25T02:45:27.892Z
Session: client-side-ai-brain-full-compliance
Confidence: 94%
Complexity: high

üß© PATTERNS USED:
- AI_BRAIN_CLI_APPLICATION: CLI application with centralized infrastructure
- HYBRID_BRIDGE_PATTERN: Integrated MT5-WebSocket bridge architecture

üì¶ DEPENDENCIES:
Internal: central_hub, bridge_app, service_manager, websocket_monitor
External: argparse, asyncio, signal, sys, os

üí° AI DECISION REASONING:
Hybrid bridge provides seamless integration between MT5 terminal and WebSocket services, enabling real-time trading data flow.

üöÄ USAGE:
python hybrid_bridge.py --mode=production

See also:
- ai-brain/memory/pattern-registry.json
- implementations/ for domain-specific examples
"""

import asyncio
import signal
import sys
import threading
import time
import io
import contextlib
from datetime import datetime, timedelta
from typing import Dict, Any, List
from dataclasses import asdict

# CENTRALIZED INFRASTRUCTURE IMPORTS
from src.infrastructure import (
    # Central Hub & Status
    client_central_hub,
    get_system_status,
    get_health_summary,
    
    # Centralized Logging
    get_logger,
    setup_mt5_logger,
    
    # Centralized Configuration  
    get_config,
    get_client_settings,
    
    # Centralized Error Handling
    handle_error,
    handle_mt5_error,
    handle_websocket_error,
    ErrorCategory,
    ErrorSeverity,
    
    # Performance Tracking
    performance_tracked,
    track_performance,
    get_performance_report,
    
    # Validation
    validate_field,
    validate_dict,
    validate_mt5_connection,
    validate_trading_symbol,
    
    # Import Management
    get_class,
    get_module
)

# Context manager to suppress unwanted print statements during imports
@contextlib.contextmanager
def suppress_stdout():
    old_stdout = sys.stdout
    sys.stdout = io.StringIO()
    try:
        yield
    finally:
        sys.stdout = old_stdout

# CENTRALIZED IMPORTS - Using centralized import manager
try:
    # MT5 Handler via centralized system
    MT5Handler = get_class('mt5_handler', 'MT5Handler')
    mt5_module = get_module('mt5_handler')
    MT5_ORDER_TYPES = getattr(mt5_module, 'MT5_ORDER_TYPES', {})
    MT5_TIMEFRAMES = getattr(mt5_module, 'MT5_TIMEFRAMES', {})
except Exception as e:
    # Fallback for development/testing
    from src.infrastructure.mt5.mt5_handler import MT5Handler, MT5_ORDER_TYPES, MT5_TIMEFRAMES

try:
    # WebSocket Client via centralized system
    WebSocketClient = get_class('websocket_client', 'WebSocketClient')
    websocket_module = get_module('websocket_client')
    MessageTypes = getattr(websocket_module, 'MessageTypes', None)
except Exception as e:
    # Fallback for development/testing
    from src.infrastructure.websocket.websocket_client import WebSocketClient, MessageTypes

# Import streaming modules with output suppression to avoid unwanted print statements
with suppress_stdout():
    try:
        # Redpanda Manager via centralized system
        redpanda_module = get_module('redpanda_manager')
        MT5RedpandaConfig = getattr(redpanda_module, 'MT5RedpandaConfig', None)
        AIOKAFKA_AVAILABLE = getattr(redpanda_module, 'AIOKAFKA_AVAILABLE', False)
        KAFKA_PYTHON_AVAILABLE = getattr(redpanda_module, 'KAFKA_PYTHON_AVAILABLE', False)
        MT5RedpandaManager = getattr(redpanda_module, 'MT5RedpandaManager', None)
    except Exception:
        # Fallback for development/testing
        from src.infrastructure.streaming.mt5_redpanda import MT5RedpandaConfig, AIOKAFKA_AVAILABLE, KAFKA_PYTHON_AVAILABLE, MT5RedpandaManager

# Define EventTypes for Redpanda events
class EventTypes:
    ORDER_PLACED = "order_placed"
    POSITION_CLOSED = "position_closed"


class HeartbeatLogger:
    """Organized monitoring dashboard with data source tracking"""
    
    def __init__(self):
        # Data tracking
        self.tick_count = 0
        self.account_updates = 0
        self.positions_updates = 0
        self.last_update = datetime.now()
        self.start_time = datetime.now()
        self.rate_samples = []
        
        # Status tracking
        self.websocket_status = "‚ùå"
        self.mt5_status = "‚ùå"
        self.server_status = "‚ùå"
        
        # WebSocket Monitor integration
        self.websocket_monitor = None
        self.websocket_endpoints_status = {}
        
        # MT5 Account Information
        self.account_info = None
        self.trading_enabled = False
        
        # Data sources monitoring
        self.mql5_status = "‚ùì"
        self.tradingview_status = "‚ùì"
        self.dukascopy_status = "‚ùì"
        self.database_status = "‚ùì"
        self.bridge_service_status = "‚ùì"
        self.db_service_status = "‚ùì"
        
        # Enhanced monitoring data
        self.redpanda_server = "127.0.0.1:19092"
        self.websocket_url = "ws://localhost:8001/api/v1/ws"
        self.reconnect_attempts = 0
        self.message_queue_size = 0
        
        # Initialize WebSocket endpoints status display
        self.websocket_endpoints_display = []
        self.max_reconnect_attempts = 5
        self.handler_count = 20
        self.last_error = "Connection timeout"
        self.deals_count = 21
        self.orders_count = 19
        self.error_count_today = 0
        self.last_error_time = None
        self.critical_alerts = 0
        self.warning_count = 0
        
        # Docker container status
        self.docker_services = {
            'api-gateway': {'port': 8000, 'status': '‚ùì', 'uptime': 'N/A'},
            'data-bridge': {'port': 8001, 'status': '‚ùì', 'uptime': 'N/A'},
            'ai-orchestration': {'port': 8003, 'status': '‚ùì', 'uptime': 'N/A'},
            'deep-learning': {'port': 8004, 'status': '‚ùì', 'uptime': 'N/A'},
            'ai-provider': {'port': 8005, 'status': '‚ùì', 'uptime': 'N/A'},
            'ml-processing': {'port': 8006, 'status': '‚ùì', 'uptime': 'N/A'},
            'trading-engine': {'port': 8007, 'status': '‚ùì', 'uptime': 'N/A'},
            'database-service': {'port': 8008, 'status': '‚ùì', 'uptime': 'N/A'},
            'user-service': {'port': 8009, 'status': '‚ùì', 'uptime': 'N/A'}
        }
        
        # Database Stack Status (Docker containers)
        self.database_stack = {
            'postgresql': {'port': 5432, 'status': '‚ùì', 'container': 'database-postgresql', 'type': 'OLTP'},
            'clickhouse': {'port': 8123, 'status': '‚ùì', 'container': 'database-clickhouse', 'type': 'OLAP'},
            'dragonflydb': {'port': 6379, 'status': '‚ùì', 'container': 'database-dragonflydb', 'type': 'Cache'},
            'weaviate': {'port': 8080, 'status': '‚ùì', 'container': 'database-weaviate', 'type': 'Vector'},
            'arangodb': {'port': 8529, 'status': '‚ùì', 'container': 'database-arangodb', 'type': 'Multi-Model'},
            'nats': {'port': 4222, 'status': '‚ùì', 'container': 'message-broker-nats', 'type': 'Message Broker'}
        }
        
        # Data Processing Pipeline Status
        self.data_pipeline = {
            'mql5_scraper': {
                'status': '‚ùì', 'last_fetch': 'N/A', 'records_fetched': 0, 'processing_time': 'N/A',
                'validation_status': '‚ùì', 'storage_status': '‚ùì', 'last_error': None
            },
            'tradingview_scraper': {
                'status': '‚ùì', 'last_fetch': 'N/A', 'records_fetched': 0, 'processing_time': 'N/A',
                'validation_status': '‚ùì', 'storage_status': '‚ùì', 'last_error': None
            },
            'dukascopy_client': {
                'status': '‚ùì', 'last_fetch': 'N/A', 'records_fetched': 0, 'processing_time': 'N/A',
                'validation_status': '‚ùì', 'storage_status': '‚ùì', 'last_error': None
            },
            'historical_downloader': {
                'status': '‚ùì', 'last_fetch': 'N/A', 'records_fetched': 0, 'processing_time': 'N/A',
                'validation_status': '‚ùì', 'storage_status': '‚ùì', 'last_error': None
            }
        }
        
        # Performance metrics
        self.last_error_count = 0
        self.memory_usage = "N/A"
        self.cpu_usage = "N/A"
        self.network_latency = "N/A"
        self.message_queue_size = 0
        
        self.running = False
        self.status_thread = None
        self.lock = threading.Lock()
        self.display_lines = 30  # Number of lines to reserve for display
        
    def start(self):
        """Start the organized dashboard display"""
        self.running = True
        self.status_thread = threading.Thread(target=self._dashboard_loop, daemon=True)
        self.status_thread.start()
        
    def stop(self):
        """Stop the dashboard display"""
        self.running = False
        if self.status_thread:
            self.status_thread.join(timeout=1)
        # Clear the dashboard
        self._clear_dashboard()
        
    def _clear_dashboard(self):
        """Clear the dashboard display"""
        # Move cursor up and clear lines
        for _ in range(self.display_lines):
            sys.stdout.write('\033[1A\033[2K')  # Move up and clear line
        sys.stdout.flush()
        
    def update_tick_count(self):
        """Increment tick count"""
        with self.lock:
            self.tick_count += 1
            self._calculate_rate()
            
    def update_account(self, account_info=None):
        """Update account information and increment count"""
        with self.lock:
            self.account_updates += 1
            if account_info:
                self.account_info = account_info
                
    def set_trading_status(self, enabled: bool):
        """Set trading enabled status"""
        with self.lock:
            self.trading_enabled = enabled
            
    def update_reconnect_attempts(self, attempts: int):
        """Update WebSocket reconnection attempts"""
        with self.lock:
            self.reconnect_attempts = attempts
            
    def log_error(self, error_msg: str):
        """Log error and update error tracking"""
        with self.lock:
            self.error_count_today += 1
            self.last_error = error_msg
            self.last_error_time = datetime.now()
            
    def update_historical_data(self, deals: int, orders: int):
        """Update historical data counts"""
        with self.lock:
            self.deals_count = deals
            self.orders_count = orders
    
    def update_positions(self):
        """Update positions activity timestamp"""
        with self.lock:
            # Simply track that positions were updated (we don't store position data in logger)
            pass
            
    def update_docker_service_status(self, service_name: str, status: str, uptime: str = 'N/A'):
        """Update Docker service status"""
        with self.lock:
            if service_name in self.docker_services:
                self.docker_services[service_name]['status'] = status
                self.docker_services[service_name]['uptime'] = uptime
    
    def update_database_stack_status(self, db_name: str, status: str, connection_info: str = 'N/A'):
        """Update Database Stack status"""
        with self.lock:
            if db_name in self.database_stack:
                self.database_stack[db_name]['status'] = status
                self.database_stack[db_name]['connection_info'] = connection_info
                
    def update_data_pipeline_status(self, source_name: str, status: str, last_fetch: str = None, 
                                   records_fetched: int = 0, processing_time: str = None,
                                   validation_status: str = None, storage_status: str = None, 
                                   error: str = None):
        """Update data processing pipeline status"""
        with self.lock:
            if source_name in self.data_pipeline:
                pipeline = self.data_pipeline[source_name]
                pipeline['status'] = status
                if last_fetch:
                    pipeline['last_fetch'] = last_fetch
                if records_fetched is not None:
                    pipeline['records_fetched'] = records_fetched
                if processing_time:
                    pipeline['processing_time'] = processing_time
                if validation_status:
                    pipeline['validation_status'] = validation_status
                if storage_status:
                    pipeline['storage_status'] = storage_status
                if error:
                    pipeline['last_error'] = error
            
    def set_websocket_status(self, connected: bool):
        """Update WebSocket status"""
        with self.lock:
            self.websocket_status = "‚úÖ" if connected else "‚ùå"
            
    def set_mt5_status(self, connected: bool):
        """Update MT5 status"""
        with self.lock:
            self.mt5_status = "‚úÖ" if connected else "‚ùå"
            
    def set_server_status(self, status: str):
        """Update server processing status"""
        with self.lock:
            self.server_status = "‚úÖ" if status == "received" else "‚ùå"
            
    def set_websocket_monitor(self, websocket_monitor):
        """Set WebSocket Monitor instance for integration"""
        with self.lock:
            self.websocket_monitor = websocket_monitor
            
    async def update_websocket_endpoints_status(self):
        """Update WebSocket endpoints status from WebSocket Monitor"""
        if not self.websocket_monitor:
            return
            
        try:
            # Get status from WebSocket Monitor
            status_summary = self.websocket_monitor.get_status_summary()
            
            with self.lock:
                # Update summary metrics
                total_endpoints = status_summary['metrics']['total_endpoints']
                connected_endpoints = status_summary['metrics']['connected_endpoints']
                critical_services_up = status_summary['metrics']['critical_services_up']
                total_messages = status_summary['metrics']['total_messages']
                
                # Create status display for dashboard
                self.websocket_endpoints_display = []
                
                # Overall status first
                overall_status = "‚úÖ" if connected_endpoints > 0 else "‚ùå"
                self.websocket_endpoints_display.append(
                    f"WebSocket Monitor System    {overall_status} {connected_endpoints}/{total_endpoints} connected"
                )
                
                # Critical services status
                critical_services = status_summary['summary']['critical_services_status']
                for service, health in critical_services.items():
                    status_icon = "‚úÖ" if health >= 80 else "‚ö†Ô∏è" if health >= 50 else "‚ùå"
                    service_name = service.title().replace('-', ' ')
                    self.websocket_endpoints_display.append(
                        f"{service_name} (CRITICAL)       {status_icon} {health:.0f}% health"
                    )
                
                # Other services (top 3 by health score)
                other_services = [(name, data['health_score']) for name, data in status_summary['services'].items() 
                                if name not in critical_services]
                other_services.sort(key=lambda x: x[1], reverse=True)
                
                for service_name, health_score in other_services[:3]:
                    status_icon = "‚úÖ" if health_score >= 80 else "‚ö†Ô∏è" if health_score >= 50 else "‚ùå"
                    display_name = service_name.title().replace('-', ' ')
                    self.websocket_endpoints_display.append(
                        f"{display_name:<25} {status_icon} {health_score:.0f}% health"
                    )
                
                # Messages summary
                if total_messages > 0:
                    self.websocket_endpoints_display.append(
                        f"Total Messages              üí¨ {total_messages} messages processed"
                    )
                    
        except Exception as e:
            with self.lock:
                self.websocket_endpoints_display = [
                    f"WebSocket Monitor Error     ‚ùå {str(e)[:30]}..."
                ]
            
    def set_data_source_status(self, source: str, status: bool, details: str = ""):
        """Update data source status"""
        with self.lock:
            status_icon = "‚úÖ" if status else "‚ùå"
            if source == "mql5":
                self.mql5_status = status_icon
            elif source == "tradingview":
                self.tradingview_status = status_icon
            elif source == "dukascopy":
                self.dukascopy_status = status_icon
            elif source == "database":
                self.database_status = status_icon
            elif source == "bridge_service":
                self.bridge_service_status = status_icon
            elif source == "db_service":
                self.db_service_status = status_icon
                
    def _calculate_rate(self):
        """Calculate processing rate"""
        now = datetime.now()
        self.rate_samples.append((now, self.tick_count))
        
        # Keep only last 10 seconds of samples
        cutoff = now - timedelta(seconds=10)
        self.rate_samples = [(t, c) for t, c in self.rate_samples if t > cutoff]
        
        if len(self.rate_samples) >= 2:
            oldest_time, oldest_count = self.rate_samples[0]
            time_diff = (now - oldest_time).total_seconds()
            count_diff = self.tick_count - oldest_count
            return count_diff / time_diff if time_diff > 0 else 0
        return 0
        
    def _dashboard_loop(self):
        """Main dashboard display loop"""
        # Wait a moment for startup logs to finish
        time.sleep(3)
        
        # Print header once without clearing screen
        print("\n" + "="*80)
        print("üîç REAL-TIME DATA SOURCE MONITORING DASHBOARD".center(80))
        print("="*80)
        print("üéõÔ∏è  TIP: Web-like dashboard - scroll freely, data updates every 2 seconds")
        print("="*80)
        
        # Reserve space for dashboard  
        print("\n" * (self.display_lines - 4))  # Account for header
        
        while self.running:
            try:
                with self.lock:
                    self._update_dashboard_display()
                    
            except Exception as e:
                # If something goes wrong, just continue
                pass
                
            time.sleep(2)  # Update every 2 seconds for readability
            
    def _update_dashboard_display(self):
        """Update the complete dashboard display - Professional format"""
        # Get current metrics
        now = datetime.now()
        rate = self._calculate_rate()
        uptime = (now - self.start_time).total_seconds()
        
        # Format uptime
        if uptime < 60:
            uptime_str = f"{uptime:.0f}s"
        elif uptime < 3600:
            uptime_str = f"{uptime/60:.0f}m"
        else:
            uptime_str = f"{uptime/3600:.1f}h"
        
        # Two-column dashboard layout
        import os
        os.system('cls' if os.name == 'nt' else 'clear')
        
        # Header
        print("=" * 120)
        print(f"üîç REAL-TIME MONITORING DASHBOARD - TWO COLUMN LAYOUT".center(120))
        print(f"{now.strftime('%Y-%m-%d %H:%M:%S')} | Uptime: {uptime_str} | Last Update: {now.strftime('%H:%M:%S')}".center(120))
        print("=" * 120)
        
        # Prepare data for two columns
        left_data = []
        right_data = []
        
        self._prepare_left_column(left_data, now, rate)
        self._prepare_right_column(right_data)
        
        # Print columns side by side
        self._print_two_columns(left_data, right_data)
        
        print("=" * 120)
        print("üéõÔ∏è  Two-Column Layout Dashboard | Left: Status & Metrics | Right: Services & APIs | Ctrl+C to quit".center(120))
        print("=" * 120)
    
    def _prepare_left_column(self, left_data, now, rate):
        """Prepare left column data - Sections A through F (balanced height)"""
        
        # A. MT5 WINDOWS STATUS
        left_data.append("A. MT5 WINDOWS STATUS")
        try:
            import psutil
            mt5_running = False
            for proc in psutil.process_iter(['pid', 'name']):
                if proc.info['name'] and 'terminal64.exe' in proc.info['name'].lower():
                    mt5_running = True
                    break
            
            if mt5_running:
                left_data.append("1. MetaTrader 5 Terminal           ‚úÖ RUNNING")
            else:
                left_data.append("1. MetaTrader 5 Terminal           ‚ùå NOT RUNNING")
        except:
            left_data.append("1. MetaTrader 5 Terminal           ‚ùì UNKNOWN")
        left_data.append("")
        
        # B. MT5 CONNECTION
        left_data.append("B. MT5 CONNECTION")
        if self.account_info:
            server = getattr(self.account_info, 'server', 'N/A')
            left_data.append(f"1. Server                          {server}")
            left_data.append(f"2. Account                         {getattr(self.account_info, 'login', 'N/A')}")
            left_data.append(f"3. Balance                         ${getattr(self.account_info, 'balance', 0):.2f}")
            left_data.append(f"4. Equity                          ${getattr(self.account_info, 'equity', 0):.2f}")
            left_data.append(f"5. Margin Level                    {getattr(self.account_info, 'margin_level', 0):.2f}%")
            left_data.append(f"6. Free Margin                     ${getattr(self.account_info, 'free_margin', 0):.2f}")
            left_data.append(f"7. Company                         {getattr(self.account_info, 'company', 'N/A')}")
        else:
            left_data.append("1. Connection                      ‚ùå NOT CONNECTED")
            left_data.append("2. Server                          N/A")
            left_data.append("3. Account                         N/A")
            left_data.append("4. Balance                         $0.00")
            left_data.append("5. Status                          Disconnected")
        left_data.append("")
        
        # C. DATA STREAMING
        left_data.append("C. DATA STREAMING")
        left_data.append(f"1. Redpanda Server                 ‚úÖ {self.redpanda_server}")
        left_data.append(f"2. Tick Data Stream                ‚úÖ Active (Rate: {rate:.0f}/sec)")
        left_data.append(f"3. WebSocket URL                   {self.websocket_url}")
        left_data.append(f"4. Reconnect Attempts              {self.reconnect_attempts}/{self.max_reconnect_attempts}")
        left_data.append("")
        
        # D. WEBSOCKET STATUS
        left_data.append("D. WEBSOCKET STATUS")
        left_data.append(f"1. Server Connection               {self.server_status}")
        left_data.append(f"2. MT5 Bridge                      {self.mt5_status}")
        left_data.append(f"3. WebSocket                       {self.websocket_status}")
        if self.websocket_endpoints_display:
            for i, endpoint in enumerate(self.websocket_endpoints_display[:3], 4):  # Show first 3
                left_data.append(f"{i}. {endpoint}")
        left_data.append("")
        
        # E. DATA FLOW METRICS
        left_data.append("E. DATA FLOW METRICS")
        left_data.append(f"1. Total Ticks Processed           {self.tick_count:,}")
        left_data.append(f"2. Tick Rate (per second)          {rate:.1f}")
        left_data.append(f"3. Account Updates                 {self.account_updates}")
        left_data.append(f"4. Position Updates                {self.positions_updates}")
        left_data.append(f"5. Error Count Today               0")
        left_data.append(f"6. Last Data Update                {now.strftime('%H:%M:%S')}")
        left_data.append("")
        
        # F. TRADING STATUS
        left_data.append("F. TRADING STATUS")
        left_data.append(f"1. Trading Enabled                 {'‚úÖ YES' if self.trading_enabled else '‚ùå NO'}")
        left_data.append("2. Paper Trading                  ‚úÖ Active")
        left_data.append("3. Emergency Stop                 ‚ùå Disabled")
        left_data.append("4. Today's Trades                 0/10")
        left_data.append("5. Open Positions                 0")
        left_data.append("6. Pending Orders                 0")
        left_data.append("")
        
        # G. DOCKER SERVICES (moved from right column)
        left_data.append("G. DOCKER SERVICES")
        for i, (service_name, service_info) in enumerate(self.docker_services.items(), 1):
            service_display = service_name.replace('-', ' ').title()
            port = service_info['port']
            status = service_info['status']
            uptime = service_info['uptime']
            left_data.append(f"{i}. {service_display:<20} ({port})    {status} {uptime}")
        left_data.append("")
        
        # H. DATABASE STACK (moved from right column)
        left_data.append("H. DATABASE STACK")
        if hasattr(self, 'database_stack') and self.database_stack:
            for i, (db_name, db_info) in enumerate(self.database_stack.items(), 1):
                db_display = db_name.replace('_', ' ').title()
                port = db_info.get('port', 'N/A')
                status = db_info.get('status', '‚ùì')
                uptime = db_info.get('uptime', 'N/A')
                left_data.append(f"{i}. {db_display:<20} ({port})    {status} {uptime}")
        else:
            left_data.append("1. PostgreSQL (5432)              ‚ùì Unknown")
            left_data.append("2. ClickHouse (8123)              ‚ùì Unknown")
            left_data.append("3. DragonflyDB (6379)             ‚ùì Unknown")
            left_data.append("4. Weaviate (8080)                ‚ùì Unknown")
            left_data.append("5. ArangoDB (8529)                ‚ùì Unknown")
            left_data.append("6. NATS (4222)                    ‚ùì Unknown")
        left_data.append("")
        
        # I. DATA SOURCES (moved from right column)
        left_data.append("I. DATA SOURCES")
        left_data.append(f"1. MQL5 Scraper                   {self.mql5_status}")
        left_data.append(f"2. TradingView                    {self.tradingview_status}")
        left_data.append(f"3. Dukascopy                      {self.dukascopy_status}")
        left_data.append(f"4. Bridge Service                 {self.bridge_service_status}")
        left_data.append(f"5. Database Service               {self.db_service_status}")
    
    def _prepare_right_column(self, right_data):
        """Prepare right column data - Comprehensive API endpoints status"""
        
        # Calculate API status percentages
        total_services = len(self.docker_services)
        healthy_services = sum(1 for service_info in self.docker_services.values() if service_info['status'] == '‚úÖ')
        failed_services = total_services - healthy_services
        healthy_pct = (healthy_services / total_services * 100) if total_services > 0 else 0
        failed_pct = (failed_services / total_services * 100) if total_services > 0 else 0
        
        # J. COMPREHENSIVE API ENDPOINTS STATUS
        right_data.append(f"J. API ENDPOINTS STATUS ({healthy_pct:.0f}% OK, {failed_pct:.0f}% FAILED)")
        right_data.append("")
        
        # Complete API endpoints per service - more comprehensive
        api_services = {
            "API Gateway (8000)": {
                "status": self.docker_services.get('api-gateway', {}).get('status', '‚ùì'),
                "endpoints": [
                    "GET  /health",
                    "GET  /status", 
                    "WS   /api/stream/market-data",
                    "ANY  /api/v1/{service_path}",
                    "GET  /docs",
                    "POST /api/v1/auth/login"
                ]
            },
            "Data Bridge (8001)": {
                "status": self.docker_services.get('data-bridge', {}).get('status', '‚ùì'),
                "endpoints": [
                    "GET  /health",
                    "GET  /status",
                    "WS   /api/v1/ws",
                    "POST /api/v1/economic_calendar/events",
                    "GET  /api/v1/deduplication/status",
                    "POST /api/v1/ticks/batch"
                ]
            },
            "AI Orchestration (8003)": {
                "status": self.docker_services.get('ai-orchestration', {}).get('status', '‚ùì'),
                "endpoints": [
                    "GET  /health",
                    "POST /api/v1/workflows/execute",
                    "POST /api/v1/agents/coordinate",
                    "GET  /api/v1/orchestration/status",
                    "POST /api/v1/tasks/schedule"
                ]
            },
            "Deep Learning (8004)": {
                "status": self.docker_services.get('deep-learning', {}).get('status', '‚ùì'),
                "endpoints": [
                    "GET  /health",
                    "POST /api/v1/deep-learning/train",
                    "POST /api/v1/deep-learning/inference",
                    "GET  /api/v1/models/status",
                    "POST /api/v1/predictions/lstm"
                ]
            },
            "AI Provider (8005)": {
                "status": self.docker_services.get('ai-provider', {}).get('status', '‚ùì'),
                "endpoints": [
                    "GET  /health",
                    "POST /api/v1/completion",
                    "GET  /api/v1/models",
                    "POST /api/v1/embeddings",
                    "GET  /api/v1/providers/status"
                ]
            },
            "ML Processing (8006)": {
                "status": self.docker_services.get('ml-processing', {}).get('status', '‚ùì'),
                "endpoints": [
                    "GET  /health",
                    "POST /api/v1/ml/train",
                    "POST /api/v1/ml/predict",
                    "GET  /api/v1/features/extract",
                    "POST /api/v1/models/evaluate"
                ]
            },
            "Trading Engine (8007)": {
                "status": self.docker_services.get('trading-engine', {}).get('status', '‚ùì'),
                "endpoints": [
                    "GET  /health",
                    "POST /api/v1/trading/execute",
                    "GET  /api/v1/positions",
                    "GET  /api/v1/risk/metrics",
                    "POST /api/v1/trading/config"
                ]
            },
            "Database Service (8008)": {
                "status": self.docker_services.get('database-service', {}).get('status', '‚ùì'),
                "endpoints": [
                    "GET  /health",
                    "POST /api/v1/clickhouse/ticks",
                    "GET  /api/v1/schemas/clickhouse",
                    "POST /api/v1/postgresql/users",
                    "GET  /api/v1/databases/status"
                ]
            },
            "User Service (8009)": {
                "status": self.docker_services.get('user-service', {}).get('status', '‚ùì'),
                "endpoints": [
                    "GET  /health",
                    "POST /api/v1/users",
                    "POST /api/v1/workflows/execute",
                    "GET  /api/v1/projects",
                    "POST /api/v1/auth/validate"
                ]
            },
            "Performance Analytics (8002)": {
                "status": self.docker_services.get('performance-analytics', {}).get('status', '‚ùì'),
                "endpoints": [
                    "GET  /health",
                    "GET  /api/v1/metrics/performance",
                    "GET  /api/v1/analytics/insights",
                    "POST /api/v1/alerts/configure",
                    "GET  /api/v1/reports/generate"
                ]
            },
            "Strategy Optimization (8010)": {
                "status": self.docker_services.get('strategy-optimization', {}).get('status', '‚ùì'),
                "endpoints": [
                    "GET  /health",
                    "POST /api/v1/optimize/strategy",
                    "GET  /api/v1/backtest/results",
                    "POST /api/v1/genetic/evolve",
                    "GET  /api/v1/strategy/performance"
                ]
            }
        }
        
        for service_name, service_data in api_services.items():
            status = service_data['status']
            endpoints = service_data['endpoints']
            
            right_data.append(f"‚Ä¢ {service_name} {status}")
            
            for endpoint in endpoints:
                right_data.append(f"  {endpoint}")
            
            right_data.append("")
    
    def _print_two_columns(self, left_data, right_data):
        """Print data in two columns with equal width (50:50)"""
        max_lines = max(len(left_data), len(right_data))
        
        # Equal column widths: 59 chars each + separator = 120 total
        left_width = 59
        right_width = 59
        
        for i in range(max_lines):
            left_line = left_data[i] if i < len(left_data) else ""
            right_line = right_data[i] if i < len(right_data) else ""
            
            # Truncate if too long, pad if too short
            left_formatted = f"{left_line[:left_width]:<{left_width}}"
            right_formatted = f"{right_line[:right_width]:<{right_width}}"
            
            print(f"{left_formatted}‚îÇ{right_formatted}")
    
    def _legacy_display_continue(self):
        """Legacy display - will be removed"""
        pass  # All display now handled by two-column layout
    
    # Legacy methods below - can be removed later
    def _old_display_method(self):
        """Old display method - kept for reference"""
        return  # Skip old display
        if self.account_info:
            server = getattr(self.account_info, 'server', 'N/A')
            account = getattr(self.account_info, 'login', 'N/A')
            balance = getattr(self.account_info, 'balance', 0)
            currency = getattr(self.account_info, 'currency', 'USD')
            broker = getattr(self.account_info, 'company', 'N/A')
            
            print(f"1. Server Status                   ‚úÖ {server} Connected")
            print(f"2. Account                         ‚úÖ {account}")
            print(f"3. Balance                         üí∞ {balance:.2f} {currency}")
            print(f"4. Broker                          üè¢ {broker}")
        else:
            print("1. Server Status                   ‚ùå Not Connected")
            print("2. Account                         ‚ùå N/A")
            print("3. Balance                         üí∞ N/A")
            print("4. Broker                          üè¢ N/A")
        
        print(f"5. Trading Status                  {'‚úÖ ENABLED' if self.trading_enabled else '‚ùå DISABLED'}")
        print(f"6. Terminal                        {self.mt5_status} Running")
        print()
        
        # C. DATA STREAMING
        print("C. DATA STREAMING")
        print(f"1. Redpanda Server                 ‚úÖ {self.redpanda_server}")
        print(f"2. Tick Data Stream               ‚úÖ Active (Rate: {rate:.0f}/sec)")
        print("3. EURUSD                         ‚úÖ Streaming")
        print("4. GBPUSD                         ‚úÖ Streaming")
        print(f"5. Total Ticks                    üìä {self.tick_count} ticks")
        print()
        
        # D. WEBSOCKET CONNECTION
        print("D. WEBSOCKET CONNECTION")
        ws_status = "‚ùå FAILED" if self.websocket_status == "‚ùå" else "‚úÖ CONNECTED"
        print(f"1. Backend Connection             {ws_status}")
        print(f"2. URL Target                     üîå {self.websocket_url}")
        print(f"3. Reconnect Attempts             üîÑ {self.reconnect_attempts}/{self.max_reconnect_attempts}")
        print(f"4. Handler Registration           ‚úÖ {self.handler_count} handlers ready")
        print(f"5. Last Error                     ‚ö†Ô∏è {self.last_error}")
        print()
        
        # E. HISTORICAL DATA
        print("E. HISTORICAL DATA")
        print(f"1. Deals History                  üìú {self.deals_count} records loaded")
        print(f"2. Orders History                 üìú {self.orders_count} records loaded")
        print("3. Data Range                     üìÖ ALL TIME")
        print()
        
        # F. COMMUNICATION CHANNELS
        print("F. COMMUNICATION CHANNELS")
        print("1. High-Frequency (Redpanda)      ‚úÖ Tick data, Market events")
        ws_comm_status = "‚ùå Account updates, Control commands" if self.websocket_status == "‚ùå" else "‚úÖ Account updates, Control commands"
        print(f"2. Low-Frequency (WebSocket)      {ws_comm_status}")
        reconnect_status = "üîÑ Attempting reconnect" if self.websocket_status == "‚ùå" else "‚úÖ Connected"
        print(f"3. Account Monitoring             {reconnect_status}")
        print()
        
        # G. PERFORMANCE METRICS
        print("G. PERFORMANCE METRICS")
        try:
            import psutil
            memory = psutil.virtual_memory()
            cpu_percent = psutil.cpu_percent()
            memory_mb = memory.used / 1024 / 1024
            print(f"1. Memory Usage                   üìä {memory_mb:.0f} MB")
            print(f"2. CPU Usage                      üìä {cpu_percent:.1f}%")
        except:
            print("1. Memory Usage                   üìä N/A")
            print("2. CPU Usage                      üìä N/A")
        
        latency = "<50ms" if self.websocket_status == "‚úÖ" else "N/A"
        print(f"3. Network Latency                üìä {latency}")
        print(f"4. Message Queue Size             üìä {self.message_queue_size} pending")
        print()
        
        # H. KEY API ENDPOINTS
        print("H. KEY API ENDPOINTS")
        api_urls = [
            "WS   /api/v1/ws                    (MT5 WebSocket)",
            "POST /api/v1/clickhouse/ticks      (Insert tick data)", 
            "POST /api/v1/workflows/execute     (AI workflows)",
            "POST /api/v1/deep-learning/train   (Train models)",
            "POST /api/v1/completion            (AI completion)",
            "POST /api/v1/ml/predict            (ML predictions)",
            "POST /api/v1/trading/execute       (Execute trades)",
            "GET  /api/v1/schemas/clickhouse    (DB schemas)",
            "POST /api/v1/users                 (User management)",
            "GET  /health                       (All services)"
        ]
        
        for i, endpoint in enumerate(api_urls, 1):
            print(f"{i}. {endpoint}")
        print()
        
        # I. DOCKER SERVICES
        print("I. DOCKER SERVICES")
        for i, (service_name, service_info) in enumerate(self.docker_services.items(), 1):
            service_display = service_name.replace('-', ' ').title()
            port = service_info['port']
            status = service_info['status']
            uptime = service_info['uptime']
            print(f"{i}. {service_display:<20} ({port})    {status} {uptime}")
        print()
        
        # Calculate API status percentages
        total_services = len(self.docker_services)
        healthy_services = sum(1 for service_info in self.docker_services.values() if service_info['status'] == '‚úÖ')
        failed_services = total_services - healthy_services
        healthy_pct = (healthy_services / total_services * 100) if total_services > 0 else 0
        failed_pct = (failed_services / total_services * 100) if total_services > 0 else 0
        
        # J. API ENDPOINTS BY SERVICE
        print(f"J. API ENDPOINTS BY SERVICE ({healthy_pct:.0f}% OK, {failed_pct:.0f}% FAILED)")
        
        api_endpoints_by_service = {
            "API Gateway (8000)": [
                "GET  /",
                "GET  /health", 
                "GET  /status",
                "WS   /ws/test",
                "WS   /api/v1/ws/test",
                "WS   /api/stream/market-data",
                "WS   /api/stream/trading",
                "ANY  /api/v1/{service_path:path}"
            ],
            "Data Bridge (8001)": [
                "GET  /",
                "GET  /health",
                "GET  /status", 
                "WS   /api/v1/ws",
                "WS   /api/v1/ws/status",
                "GET  /api/v1/ws/performance",
                "GET  /api/v1/deduplication/health",
                "POST /api/v1/deduplication/configure",
                "GET  /api/v1/economic_calendar/health",
                "POST /api/v1/economic_calendar/events"
            ],
            "AI Orchestration (8003)": [
                "GET  /",
                "GET  /health",
                "GET  /status",
                "POST /api/v1/workflows/execute",
                "GET  /api/v1/workflows/{workflow_id}/status",
                "POST /api/v1/agents/coordinate",
                "GET  /api/v1/agents/performance",
                "POST /api/v1/tasks/schedule"
            ],
            "Deep Learning (8004)": [
                "GET  /",
                "GET  /health",
                "GET  /status",
                "POST /api/v1/deep-learning/train",
                "POST /api/v1/deep-learning/inference",
                "GET  /api/v1/deep-learning/models",
                "POST /api/v1/deep-learning/fine-tune",
                "GET  /api/v1/deep-learning/frameworks"
            ],
            "AI Provider (8005)": [
                "GET  /",
                "GET  /health",
                "GET  /status",
                "POST /api/v1/completion",
                "POST /api/v1/providers/{provider_name}/completion",
                "GET  /api/v1/models",
                "GET  /api/v1/providers",
                "GET  /api/v1/metrics",
                "GET  /api/v1/analytics/costs"
            ],
            "ML Processing (8006)": [
                "GET  /",
                "GET  /health", 
                "GET  /status",
                "POST /api/v1/ml/train",
                "POST /api/v1/ml/predict",
                "GET  /api/v1/ml/models",
                "POST /api/v1/ml/feature-engineering",
                "GET  /api/v1/ml/performance"
            ],
            "Trading Engine (8007)": [
                "GET  /",
                "GET  /health",
                "GET  /status",
                "POST /api/v1/trading/execute",
                "POST /api/v1/strategies/create",
                "GET  /api/v1/strategies",
                "GET  /api/v1/positions",
                "POST /api/v1/risk/assess",
                "GET  /api/v1/risk/metrics"
            ],
            "Database Service (8008)": [
                "GET  /",
                "GET  /health",
                "GET  /status",
                "POST /api/v1/clickhouse/ticks",
                "POST /api/v1/clickhouse/ticks/batch",
                "POST /api/v1/clickhouse/account_info",
                "GET  /api/v1/clickhouse/economic_calendar",
                "GET  /api/v1/schemas/clickhouse/economic_calendar",
                "GET  /api/v1/cache/get/{key}",
                "POST /api/v1/cache/set/{key}"
            ],
            "User Service (8009)": [
                "GET  /",
                "GET  /health",
                "GET  /status", 
                "POST /api/v1/users",
                "GET  /api/v1/users/{user_id}",
                "GET  /api/v1/users/{user_id}/ai-preferences",
                "POST /api/v1/projects",
                "GET  /api/v1/projects/{project_id}",
                "POST /api/v1/workflows/execute"
            ],
            "Performance Analytics (8002)": [
                "GET  /",
                "GET  /health",
                "GET  /status",
                "GET  /api/v1/metrics/performance",
                "GET  /api/v1/metrics/latency",
                "GET  /api/v1/analytics/insights",
                "POST /api/v1/alerts/configure",
                "GET  /api/v1/reports/generate",
                "GET  /api/v1/dashboards/performance"
            ],
            "Strategy Optimization (8010)": [
                "GET  /",
                "GET  /health",
                "GET  /status",
                "POST /api/v1/optimize/strategy",
                "GET  /api/v1/backtest/results",
                "POST /api/v1/genetic/evolve",
                "GET  /api/v1/strategy/performance",
                "POST /api/v1/parameters/tune",
                "GET  /api/v1/optimization/status"
            ]
        }
        
        for service_name, endpoints in api_endpoints_by_service.items():
            # Get service status
            service_key = service_name.split(' ')[0].lower() + '-' + service_name.split(' ')[1].lower()
            if service_key in self.docker_services:
                status = self.docker_services[service_key]['status']
            else:
                status = "‚ùì"
            
            print(f"\n{service_name} {status}")
            for endpoint in endpoints:
                print(f"  {endpoint}")
        
        # Footer with controls
        print("\n" + "="*80)
        print("üéõÔ∏è  INFO: Web-like updates every 2 seconds - scroll to view all data | Ctrl+C to quit")
        print("="*80)
        
        # I.1 DATABASE STACK
        print("I.1 DATABASE STACK")
        for i, (db_name, db_info) in enumerate(self.database_stack.items(), 1):
            db_display = f"{db_name.upper()} ({db_info['type']})"
            port = db_info['port'] 
            status = db_info['status']
            container = db_info['container']
            connection_info = db_info.get('connection_info', 'N/A')
            print(f"{i}. {db_display:<22} ({port})    {status}")
            print(f"   {'':>23} üì¶ {container}")
            if connection_info != 'N/A':
                print(f"   {'':>23} üîó {connection_info}")
            print()
        
        # J. DATA PROCESSING PIPELINE
        print("J. DATA PROCESSING PIPELINE")
        for i, (source_name, pipeline) in enumerate(self.data_pipeline.items(), 1):
            source_display = source_name.replace('_', ' ').title()
            status = pipeline['status']
            last_fetch = pipeline['last_fetch']
            records = pipeline['records_fetched']
            processing_time = pipeline['processing_time']
            validation = pipeline['validation_status']
            storage = pipeline['storage_status']
            
            print(f"{i}. {source_display:<20}    {status} | Last: {last_fetch}")
            print(f"   {'':>23} üìä Records: {records} | Time: {processing_time}")
            print(f"   {'':>23} üîç Validation: {validation} | üíæ Storage: {storage}")
            if pipeline['last_error']:
                error_short = pipeline['last_error'][:30] + "..." if len(pipeline['last_error']) > 30 else pipeline['last_error']
                print(f"   {'':>23} ‚ùå Error: {error_short}")
            print()
        
        # K. WEBSOCKET ENDPOINTS
        print("K. WEBSOCKET ENDPOINTS")
        if self.websocket_endpoints_display:
            for i, endpoint_status in enumerate(self.websocket_endpoints_display, 1):
                print(f"{i}. {endpoint_status}")
        else:
            print("1. WebSocket Monitor              üîó Initializing...")
        print()
        
        # L. OPERATIONAL CONTROLS
        print("L. OPERATIONAL CONTROLS")
        print("1. Emergency Stop                 üõë READY")
        print("2. Auto-Reconnect                 üîÑ ENABLED")
        print("3. Trading Mode                   üí∞ DEMO")
        print("4. Debug Level                    üìù INFO")
        
    def log_important(self, message: str):
        """Log important message (scroll-friendly)"""
        if self.running:
            # Simply print the message - no cursor positioning
            print(f"\nüì¢ IMPORTANT: {message}")
            # Dashboard will be updated on next cycle
            
    async def check_docker_services_status(self):
        """Check Docker container status for all microservices"""
        try:
            # First try to check via health endpoints
            await self._check_services_via_health_endpoints()
        except Exception as e:
            # If health endpoints fail, try Docker command as fallback
            await self._check_services_via_docker_command()
    
    async def check_database_stack_status(self):
        """Check Database Stack container status"""
        try:
            # Check database containers via health endpoints and Docker commands
            await self._check_database_via_health_endpoints()
        except Exception as e:
            # Fallback to Docker command status
            await self._check_database_via_docker_command()
    
    async def _check_services_via_health_endpoints(self):
        """Check services via HTTP health endpoints"""
        import aiohttp
        
        # Check each microservice health endpoint
        for service_name, service_info in self.docker_services.items():
            port = service_info['port']
            try:
                async with aiohttp.ClientSession() as session:
                    health_url = f"http://localhost:{port}/health"
                    async with session.get(health_url, timeout=aiohttp.ClientTimeout(total=3)) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            uptime_seconds = data.get('uptime_seconds', 0)
                            
                            # Format uptime
                            if uptime_seconds < 60:
                                uptime_str = f"{uptime_seconds:.0f}s"
                            elif uptime_seconds < 3600:
                                uptime_str = f"{uptime_seconds/60:.0f}m"
                            else:
                                uptime_str = f"{uptime_seconds/3600:.1f}h"
                                
                            self.update_docker_service_status(service_name, "‚úÖ", f"Up {uptime_str}")
                        else:
                            self.update_docker_service_status(service_name, "‚ùå", "HTTP Error")
            except asyncio.TimeoutError:
                self.update_docker_service_status(service_name, "‚è≥", "Timeout")
            except Exception as e:
                self.update_docker_service_status(service_name, "‚ùå", "Down")
    
    async def _check_services_via_docker_command(self):
        """Check services via Docker command as fallback"""
        import subprocess
        
        try:
            # Get Docker container status
            result = subprocess.run(
                ['docker', 'ps', '--format', 'table {{.Names}}\t{{.Status}}'],
                capture_output=True,
                text=True,
                timeout=5
            )
            
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')[1:]  # Skip header
                container_status = {}
                
                for line in lines:
                    if '\t' in line:
                        name, status = line.split('\t', 1)
                        container_status[name] = status
                
                # Update service status based on container names
                for service_name in self.docker_services.keys():
                    container_name = f"server_microservice-{service_name}-1"  # Docker Compose naming
                    
                    if container_name in container_status:
                        status = container_status[container_name]
                        if 'Up' in status:
                            # Extract uptime from status
                            if 'second' in status:
                                uptime = status.split('Up')[1].strip()
                                self.update_docker_service_status(service_name, "‚úÖ", uptime)
                            else:
                                self.update_docker_service_status(service_name, "‚úÖ", "Running")
                        else:
                            self.update_docker_service_status(service_name, "‚ùå", status)
                    else:
                        self.update_docker_service_status(service_name, "‚≠ï", "Not Found")
            else:
                # Docker command failed, mark all as unknown
                for service_name in self.docker_services.keys():
                    self.update_docker_service_status(service_name, "‚ùì", "Docker N/A")
                    
        except subprocess.TimeoutExpired:
            for service_name in self.docker_services.keys():
                self.update_docker_service_status(service_name, "‚è≥", "Docker Timeout")
        except FileNotFoundError:
            # Docker not installed/available
            for service_name in self.docker_services.keys():
                self.update_docker_service_status(service_name, "‚ùå", "Docker N/A")
        except Exception as e:
            # Other errors
            for service_name in self.docker_services.keys():
                self.update_docker_service_status(service_name, "‚ùì", f"Error: {str(e)[:10]}")

    async def _check_database_via_health_endpoints(self):
        """Check database containers via health endpoints"""
        import aiohttp
        import asyncio
        
        # PostgreSQL check - Real-time via Docker + API
        try:
            # Method 1: Check Docker container status (fast)
            docker_healthy = False
            try:
                import subprocess
                result = subprocess.run(['docker', 'ps', '--filter', 'name=database-postgresql', '--format', '{{.Status}}'], 
                                      capture_output=True, text=True, timeout=3)
                docker_healthy = 'Up' in result.stdout and ('healthy' in result.stdout or 'Up' in result.stdout)
            except:
                pass
            
            # Method 2: Quick API health check via database service
            api_healthy = False
            try:
                import aiohttp
                async with aiohttp.ClientSession() as session:
                    async with session.get('http://localhost:8008/health', timeout=aiohttp.ClientTimeout(total=2)) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            # Check if database service reports healthy status
                            api_healthy = data.get('status') == 'healthy'
            except:
                pass
            
            # Real-time status determination
            if docker_healthy and api_healthy:
                self.update_database_stack_status('postgresql', "‚úÖ", "Docker + API healthy")
            elif docker_healthy:
                self.update_database_stack_status('postgresql', "‚ö†Ô∏è", "Docker OK, API issue")  
            elif api_healthy:
                self.update_database_stack_status('postgresql', "‚ö†Ô∏è", "API OK, Docker issue")
            else:
                self.update_database_stack_status('postgresql', "‚ùå", "SERVICE DOWN")
        except Exception as e:
            self.update_database_stack_status('postgresql', "‚ùì", f"Error: {str(e)[:20]}")
        
        # ClickHouse check
        try:
            async with aiohttp.ClientSession() as session:
                url = "http://localhost:8123/ping"  # ClickHouse ping endpoint
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=3)) as resp:
                    if resp.status == 200:
                        self.update_database_stack_status('clickhouse', "‚úÖ", "Ping OK")
                    else:
                        self.update_database_stack_status('clickhouse', "‚ùå", f"HTTP {resp.status}")
        except Exception as e:
            self.update_database_stack_status('clickhouse', "‚ùì", f"Error: {str(e)[:20]}")
        
        # DragonflyDB check - Real-time via Docker + API
        try:
            # Method 1: Check Docker container status (fast)
            docker_healthy = False
            try:
                import subprocess
                result = subprocess.run(['docker', 'ps', '--filter', 'name=database-dragonflydb', '--format', '{{.Status}}'], 
                                      capture_output=True, text=True, timeout=3)
                docker_healthy = 'Up' in result.stdout and ('healthy' in result.stdout or 'Up' in result.stdout)
            except:
                pass
            
            # Method 2: Quick API health check via database service  
            api_healthy = False
            try:
                import aiohttp
                async with aiohttp.ClientSession() as session:
                    async with session.get('http://localhost:8008/health', timeout=aiohttp.ClientTimeout(total=2)) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            # Check if database service reports healthy status
                            api_healthy = data.get('status') == 'healthy'
            except:
                pass
            
            # Method 3: Direct ping test (as backup verification)
            direct_ping = False
            try:
                import aiohttp
                async with aiohttp.ClientSession() as session:
                    # Try a quick Redis ping equivalent via HTTP (if DragonflyDB has HTTP interface)
                    # Or use a simple socket connection test
                    import socket
                    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                    sock.settimeout(1)
                    result = sock.connect_ex(('localhost', 6379))
                    sock.close()
                    direct_ping = result == 0
            except:
                pass
            
            # Real-time status determination
            if docker_healthy and api_healthy:
                self.update_database_stack_status('dragonflydb', "‚úÖ", "Docker + API healthy")
            elif docker_healthy and direct_ping:
                self.update_database_stack_status('dragonflydb', "‚úÖ", "Docker + Direct ping OK")
            elif docker_healthy:
                self.update_database_stack_status('dragonflydb', "‚ö†Ô∏è", "Docker OK, API issue")
            elif api_healthy:
                self.update_database_stack_status('dragonflydb', "‚ö†Ô∏è", "API OK, Docker issue")
            elif direct_ping:
                self.update_database_stack_status('dragonflydb', "‚ö†Ô∏è", "Direct ping OK only")
            else:
                self.update_database_stack_status('dragonflydb', "‚ùå", "SERVICE DOWN")
        except Exception as e:
            self.update_database_stack_status('dragonflydb', "‚ùì", f"Error: {str(e)[:20]}")
        
        # Weaviate check
        try:
            async with aiohttp.ClientSession() as session:
                url = "http://localhost:8080/v1/.well-known/ready"  # Weaviate ready endpoint
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=5)) as resp:
                    if resp.status == 200:
                        self.update_database_stack_status('weaviate', "‚úÖ", "Ready endpoint OK")
                    else:
                        self.update_database_stack_status('weaviate', "‚ùå", f"HTTP {resp.status}")
        except Exception as e:
            self.update_database_stack_status('weaviate', "‚ùì", f"Error: {str(e)[:20]}")
        
        # ArangoDB check
        try:
            async with aiohttp.ClientSession() as session:
                url = "http://localhost:8529/_api/version"  # ArangoDB version endpoint
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=5)) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        version = data.get('version', 'Unknown')
                        self.update_database_stack_status('arangodb', "‚úÖ", f"Version: {version}")
                    else:
                        self.update_database_stack_status('arangodb', "‚ùå", f"HTTP {resp.status}")
        except Exception as e:
            self.update_database_stack_status('arangodb', "‚ùì", f"Error: {str(e)[:20]}")
        
        # NATS Message Broker check
        try:
            # NATS doesn't have HTTP API by default, so check via TCP connection
            import socket
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(3)
            result = sock.connect_ex(('localhost', 4222))
            sock.close()
            
            if result == 0:
                self.update_database_stack_status('nats', "‚úÖ", "TCP Connection OK")
            else:
                self.update_database_stack_status('nats', "‚ùå", "Connection Failed")
        except Exception as e:
            self.update_database_stack_status('nats', "‚ùì", f"Error: {str(e)[:20]}")

    async def _check_database_via_docker_command(self):
        """Check database containers via Docker command as fallback"""
        import subprocess
        
        try:
            # Get Docker container status
            result = subprocess.run(
                ['docker', 'ps', '--format', 'table {{.Names}}\t{{.Status}}'],
                capture_output=True,
                text=True,
                timeout=5
            )
            
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')[1:]  # Skip header
                container_status = {}
                
                for line in lines:
                    if '\t' in line:
                        name, status = line.split('\t', 1)
                        container_status[name] = status
                
                # Update database status based on container names
                for db_name, db_info in self.database_stack.items():
                    container_name = db_info['container']
                    
                    if container_name in container_status:
                        status = container_status[container_name]
                        if 'Up' in status:
                            # Extract uptime from status
                            if 'second' in status:
                                uptime = status.split('Up')[1].strip()
                                self.update_database_stack_status(db_name, "‚úÖ", f"Up {uptime}")
                            else:
                                self.update_database_stack_status(db_name, "‚úÖ", "Running")
                        else:
                            self.update_database_stack_status(db_name, "‚ùå", status)
                    else:
                        self.update_database_stack_status(db_name, "‚≠ï", "Container Not Found")
            else:
                # Docker command failed, mark all as unknown
                for db_name in self.database_stack.keys():
                    self.update_database_stack_status(db_name, "‚ùì", "Docker N/A")
                    
        except subprocess.TimeoutExpired:
            for db_name in self.database_stack.keys():
                self.update_database_stack_status(db_name, "‚è≥", "Docker Timeout")
        except FileNotFoundError:
            # Docker not installed/available
            for db_name in self.database_stack.keys():
                self.update_database_stack_status(db_name, "‚ùå", "Docker N/A")
        except Exception as e:
            # Other errors
            for db_name in self.database_stack.keys():
                self.update_database_stack_status(db_name, "‚ùì", f"Error: {str(e)[:10]}")

    async def check_data_pipeline_status(self):
        """Check data processing pipeline status"""
        import aiohttp
        
        # Check MQL5 Scraper status
        await self._check_mql5_scraper_status()
        
        # Check TradingView Scraper status  
        await self._check_tradingview_scraper_status()
        
        # Check Dukascopy Client status
        await self._check_dukascopy_client_status()
        
        # Check Historical Downloader status
        await self._check_historical_downloader_status()
    
    async def _check_mql5_scraper_status(self):
        """Check MQL5 scraper processing status"""
        import aiohttp
        
        try:
            async with aiohttp.ClientSession() as session:
                # Check MQL5 scraper endpoint
                url = "http://localhost:8001/api/v1/economic_calendar/sources/mql5/events"
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=5)) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        
                        # Extract status information from response
                        events = data.get('events', [])
                        metadata = data.get('metadata', {})
                        
                        status = "‚úÖ" if len(events) > 0 else "‚ö†Ô∏è"
                        last_fetch = metadata.get('fetch_time', datetime.now().strftime('%H:%M:%S'))
                        records_fetched = len(events)
                        processing_time = metadata.get('processing_duration', 'N/A')
                        validation_status = "‚úÖ" if metadata.get('validation_passed', True) else "‚ùå"
                        storage_status = "‚úÖ" if metadata.get('stored_to_db', True) else "‚ùå"
                        
                        self.update_data_pipeline_status(
                            'mql5_scraper', status, last_fetch, records_fetched, 
                            processing_time, validation_status, storage_status
                        )
                    else:
                        self.update_data_pipeline_status('mql5_scraper', "‚ùå", error="HTTP Error")
        except Exception as e:
            self.update_data_pipeline_status('mql5_scraper', "‚ùå", error=str(e))
    
    async def _check_tradingview_scraper_status(self):
        """Check TradingView scraper processing status"""
        import aiohttp
        
        try:
            async with aiohttp.ClientSession() as session:
                # Check TradingView scraper endpoint
                url = "http://localhost:8001/api/v1/economic_calendar/sources/tradingview/events"
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=5)) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        
                        # Extract status information from response
                        events = data.get('events', [])
                        metadata = data.get('metadata', {})
                        
                        status = "‚úÖ" if len(events) > 0 else "‚ö†Ô∏è"
                        last_fetch = metadata.get('fetch_time', datetime.now().strftime('%H:%M:%S'))
                        records_fetched = len(events)
                        processing_time = metadata.get('processing_duration', 'N/A')
                        validation_status = "‚úÖ" if metadata.get('validation_passed', True) else "‚ùå"
                        storage_status = "‚úÖ" if metadata.get('stored_to_db', True) else "‚ùå"
                        
                        self.update_data_pipeline_status(
                            'tradingview_scraper', status, last_fetch, records_fetched, 
                            processing_time, validation_status, storage_status
                        )
                    else:
                        self.update_data_pipeline_status('tradingview_scraper', "‚ùå", error="HTTP Error")
        except Exception as e:
            self.update_data_pipeline_status('tradingview_scraper', "‚ùå", error=str(e))
    
    async def _check_dukascopy_client_status(self):
        """Check Dukascopy client processing status"""
        import aiohttp
        
        try:
            async with aiohttp.ClientSession() as session:
                # Check data-bridge health for Dukascopy status
                url = "http://localhost:8001/health"
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=5)) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        
                        # Extract status from health data
                        source_status = data.get('source_status', {})
                        dukascopy_active = source_status.get('dukascopy_client', False)
                        
                        status = "‚úÖ" if dukascopy_active else "‚ö†Ô∏è"
                        last_fetch = datetime.now().strftime('%H:%M:%S')
                        records_fetched = source_status.get('dukascopy_records', 0)
                        processing_time = "N/A"
                        validation_status = "‚úÖ" if dukascopy_active else "‚ö†Ô∏è"
                        storage_status = "‚úÖ" if dukascopy_active else "‚ö†Ô∏è"
                        
                        self.update_data_pipeline_status(
                            'dukascopy_client', status, last_fetch, records_fetched, 
                            processing_time, validation_status, storage_status
                        )
                    else:
                        self.update_data_pipeline_status('dukascopy_client', "‚ùå", error="HTTP Error")
        except Exception as e:
            self.update_data_pipeline_status('dukascopy_client', "‚ùå", error=str(e))
    
    async def _check_historical_downloader_status(self):
        """Check Historical downloader processing status"""
        import aiohttp
        
        try:
            async with aiohttp.ClientSession() as session:
                # Check Historical downloader via main health endpoint
                url = "http://localhost:8001/health"
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=5)) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        
                        # Extract status information from health endpoint
                        source_status = data.get('source_status', {})
                        historical_active = source_status.get('historical_downloader', False)
                        
                        status = "‚úÖ" if historical_active else "‚ùå"
                        
                        # Get uptime and processing info
                        uptime = data.get('uptime_seconds', 0)
                        service_healthy = data.get('service_healthy', False)
                        status_text = data.get('status', 'unknown')
                        
                        # Format times and details
                        last_fetch = f"{uptime:.0f}s ago" if uptime > 0 else "N/A"
                        records_fetched = "Available" if historical_active else 0
                        processing_time = f"{uptime:.1f}s" if uptime > 0 else "N/A"
                        validation_status = "‚úÖ" if service_healthy and historical_active else "‚ùå"
                        storage_status = "‚úÖ" if service_healthy and historical_active else "‚ùå"
                        
                        self.update_data_pipeline_status(
                            'historical_downloader', status, last_fetch, records_fetched, 
                            processing_time, validation_status, storage_status
                        )
                    else:
                        self.update_data_pipeline_status('historical_downloader', "‚ùå", error="HTTP Error")
        except Exception as e:
            self.update_data_pipeline_status('historical_downloader', "‚ùå", error=str(e))

    async def update_data_sources_status(self):
        """Update data sources status by checking server endpoints"""
        import aiohttp
        
        try:
            # Check MQL5 scraper
            async with aiohttp.ClientSession() as session:
                try:
                    async with session.get("http://localhost:8001/api/v1/economic_calendar/sources/mql5/events", 
                                         timeout=aiohttp.ClientTimeout(total=3)) as resp:
                        self.set_data_source_status("mql5", resp.status == 200)
                except:
                    self.set_data_source_status("mql5", False)
                
                # Check TradingView scraper
                try:
                    async with session.get("http://localhost:8001/api/v1/economic_calendar/sources/tradingview/events", 
                                         timeout=aiohttp.ClientTimeout(total=5)) as resp:
                        self.set_data_source_status("tradingview", resp.status == 200)
                except:
                    self.set_data_source_status("tradingview", False)
                    
                # Check services
                try:
                    async with session.get("http://localhost:8001/health", 
                                         timeout=aiohttp.ClientTimeout(total=3)) as resp:
                        self.set_data_source_status("bridge_service", resp.status == 200)
                except:
                    self.set_data_source_status("bridge_service", False)
                    
                try:
                    async with session.get("http://localhost:8008/health", 
                                         timeout=aiohttp.ClientTimeout(total=3)) as resp:
                        self.set_data_source_status("db_service", resp.status == 200)
                        if resp.status == 200:
                            data = await resp.json()
                            # Check database health
                            db_health = data.get('database_health', {})
                            clickhouse_ok = db_health.get('clickhouse', {}).get('status') == 'healthy'
                            self.set_data_source_status("database", clickhouse_ok)
                except:
                    self.set_data_source_status("db_service", False)
                    self.set_data_source_status("database", False)
                    
        except Exception as e:
            # If general error, mark as unknown
            pass


class HybridMT5Bridge:
    """
    Hybrid MT5 Bridge Application
    Combines WebSocket and Redpanda for optimal performance:
    - High-frequency data ‚Üí Redpanda
    - Control commands ‚Üí WebSocket
    """
    
    def __init__(self):
        # CENTRALIZED INFRASTRUCTURE INITIALIZATION
        # Initialize centralized logger first
        self.logger = get_logger('hybrid_bridge', context={
            'component': 'hybrid_bridge',
            'version': '2.0.0-centralized'
        })
        
        # Load centralized configuration
        try:
            # Try centralized configuration first
            centralized_settings = get_client_settings()
            
            # Check if centralized config has the expected structure
            if isinstance(centralized_settings, dict) and 'mt5' not in centralized_settings:
                # Fallback to original ClientSettings for backward compatibility
                from src.shared.config.client_settings import get_client_settings as get_original_settings
                self.settings = get_original_settings()
                self.logger.info("Using original ClientSettings (centralized config incomplete)")
            else:
                # Use centralized configuration
                self.settings = centralized_settings
                self.logger.info("Using centralized configuration")
            
            self.config = {
                'client': get_config('client'),
                'mt5': get_config('mt5'), 
                'websocket': get_config('websocket'),
                'redpanda': get_config('redpanda'),
                'trading': get_config('trading')
            }
        except Exception as e:
            error_context = handle_error(
                error=e,
                category=ErrorCategory.CONFIG,
                severity=ErrorSeverity.CRITICAL,
                component='hybrid_bridge',
                operation='initialization'
            )
            self.logger.error(f"Failed to load configuration: {error_context.message}")
            raise
        
        # Initialize components
        self.mt5_handler = None
        self.ws_client = None
        self.redpanda_manager = None
        
        # Initialize heartbeat logger
        self.heartbeat_logger = HeartbeatLogger()
        
        # Application state
        self.running = False
        self.last_heartbeat = None
        self.daily_trades_count = 0
        self.last_trade_date = None
        
        # Communication mode selection
        self.websocket_only_mode = True  # Default to WebSocket-only (reliable)
        self.redpanda_available = False
        self.enable_redpanda_background = False  # Disable client-side Redpanda completely
        
        # Data streaming configuration - get from centralized config
        trading_config = self.config.get('trading', {})
        self.tick_symbols = trading_config.get('symbols', ["EURUSD", "GBPUSD", "USDJPY", "AUDUSD", "USDCAD", "USDCHF", "EURJPY", "GBPJPY"])
        self.last_tick_times = {}  # Track last tick time per symbol
        
        # Tasks
        self.tick_streaming_task = None
        self.account_monitoring_task = None
        self.ws_listening_task = None
        
        # Centralized system status logging
        self.logger.info("üöÄ Hybrid Bridge initialized with centralized infrastructure")
        
        # Setup logging (keep for compatibility but use centralized)
        self._setup_logging()
    
    def _get_config_value(self, config_path: str):
        """
        Safely get configuration value supporting both dict and object access patterns
        
        Args:
            config_path: Dot-separated path like 'mt5.login' or 'app.trading_enabled'
        
        Returns:
            Configuration value or None if not found
        """
        try:
            # Split the path
            parts = config_path.split('.')
            
            # Start with settings object/dict
            current = self.settings
            
            # Navigate through the path
            for part in parts:
                if hasattr(current, part):
                    # Object access (e.g., settings.mt5)
                    current = getattr(current, part)
                elif isinstance(current, dict) and part in current:
                    # Dictionary access (e.g., settings['mt5'])
                    current = current[part]
                else:
                    # Path not found, try from self.config as fallback
                    if len(parts) >= 2:
                        section = parts[0]
                        key = '.'.join(parts[1:])
                        if section in self.config:
                            config_section = self.config[section]
                            # Navigate nested keys
                            for key_part in parts[1:]:
                                if isinstance(config_section, dict) and key_part in config_section:
                                    config_section = config_section[key_part]
                                else:
                                    return None
                            return config_section
                    return None
            
            return current
            
        except Exception as e:
            self.logger.debug(f"Error accessing config path '{config_path}': {e}")
            return None
    
    def _safe_get_config(self, obj, key: str, default=None):
        """
        Safely get configuration value supporting both dict and object access
        
        Args:
            obj: Configuration object or dictionary
            key: Configuration key
            default: Default value if not found
        
        Returns:
            Configuration value or default
        """
        try:
            if hasattr(obj, key):
                # Object access (e.g., config.websocket_url)
                return getattr(obj, key)
            elif isinstance(obj, dict) and key in obj:
                # Dictionary access (e.g., config['websocket_url'])
                return obj[key]
            elif hasattr(obj, 'get') and callable(getattr(obj, 'get')):
                # Dictionary-like object with get method
                return obj.get(key, default)
            else:
                return default
        except Exception as e:
            self.logger.debug(f"Error accessing config key '{key}': {e}")
            return default
    
    def _setup_logging(self):
        """Setup logging configuration - Using centralized logging system"""
        # CENTRALIZED LOGGING CONFIGURATION
        # The centralized logger manager already handles everything:
        # - File rotation, formatting, partial coloring, single instance
        # - No need to configure anything else - just use the centralized logger
        
        try:
            # The logger is already configured by centralized logger manager
            # Just log a confirmation that we're using the centralized system
            self.logger.info("üìù Centralized logging system active")
            
        except Exception as e:
            # Minimal fallback - still use centralized logger but log error
            self.logger.error(f"Failed to confirm centralized logging: {e}")
        
        self.logger.info("üìù Hybrid Bridge logging configured (File + Database + Console)")
    
    @performance_tracked("application_startup", "hybrid_bridge")
    async def start(self):
        """Start Hybrid MT5 Bridge application with centralized tracking"""
        try:
            with track_performance("startup_initialization", "hybrid_bridge"):
                self.logger.info("üöÄ Starting Hybrid MT5 Bridge Application with Centralized Infrastructure")
                
                # Display centralized system status first
                system_health = get_health_summary()
                self.logger.info(f"üè• Centralized System Health: {system_health}")
                
                # Initialize components with performance tracking
                with track_performance("mt5_initialization", "hybrid_bridge"):
                    await self._initialize_mt5()
                
                with track_performance("redpanda_initialization", "hybrid_bridge"):
                    await self._initialize_redpanda()
                
                with track_performance("websocket_initialization", "hybrid_bridge"):
                    await self._initialize_websocket()
                
                # Register signal handlers
                self._setup_signal_handlers()
                
                # Start background tasks
                await self._start_background_tasks()
                
                self.running = True
                self.logger.success("‚úÖ Hybrid MT5 Bridge started successfully with centralized infrastructure")
                
                # Display initial status
                await self._display_status()
                
                # Give user a moment to see the status display before starting heartbeat
                await asyncio.sleep(2)
                
                # Add a blank line to separate startup logs from heartbeat display
                print()
                
                # Start heartbeat logging
                self.heartbeat_logger.start()
                
                # Brief pause for heartbeat to initialize
                await asyncio.sleep(1)
                
                # Main application loop with performance tracking
                with track_performance("main_application_loop", "hybrid_bridge"):
                    await self._main_loop()
            
        except Exception as e:
            # Use centralized error handling
            error_context = handle_error(
                error=e,
                category=ErrorCategory.SYSTEM,
                severity=ErrorSeverity.CRITICAL,
                component='hybrid_bridge',
                operation='application_startup'
            )
            self.logger.error(f"Application startup failed: {error_context.message}")
            await self.stop()
    
    async def stop(self):
        """Stop Hybrid MT5 Bridge application"""
        try:
            # Stop heartbeat logger first to clean up display
            self.heartbeat_logger.stop()
            
            self.logger.info("üõë Stopping Hybrid MT5 Bridge Application")
            
            self.running = False
            
            # Cancel background tasks
            tasks = [
                self.tick_streaming_task,
                self.account_monitoring_task,
                self.data_sources_monitoring_task,
                self.ws_listening_task
            ]
            
            for task in tasks:
                if task:
                    task.cancel()
                    try:
                        await task
                    except asyncio.CancelledError:
                        pass
            
            # Disconnect from services
            # Note: No client-side Redpanda to stop (handled server-side)
            
            if self.ws_client:
                await self.ws_client.disconnect()
            
            if self.mt5_handler:
                await self.mt5_handler.shutdown()
            
            self.logger.info("‚úÖ Hybrid MT5 Bridge stopped")
            
        except Exception as e:
            self.logger.error(f"Stop application error: {e}")
    
    async def _initialize_mt5(self):
        """Initialize MT5 connection with centralized validation"""
        try:
            # Get MT5 config from centralized system
            mt5_config = self.config.get('mt5', {})
            
            # CENTRALIZED VALIDATION - Validate MT5 connection parameters
            mt5_login = self._get_config_value('mt5.login')
            mt5_server = self._get_config_value('mt5.server')
            mt5_password = self._get_config_value('mt5.password')
            
            validation_result = validate_mt5_connection(
                login=mt5_login,
                server=mt5_server,
                password=mt5_password
            )
            
            if not validation_result.is_valid:
                error_messages = [error.message for error in validation_result.errors]
                raise ValueError(f"MT5 configuration validation failed: {'; '.join(error_messages)}")
            
            self.logger.info("‚úÖ MT5 configuration validation passed")
            
            # Initialize MT5 Handler with validated parameters
            mt5_installation_path = self._get_config_value('mt5.installation_path')
            
            self.mt5_handler = MT5Handler(
                login=mt5_login,
                password=mt5_password,
                server=mt5_server,
                path=mt5_installation_path
            )
            
            # Connect with performance tracking
            with track_performance("mt5_connection", "mt5_handler"):
                success = await self.mt5_handler.connect()
                
            if not success:
                raise Exception("Failed to connect to MT5 terminal")
            
            # Update heartbeat status
            self.heartbeat_logger.set_mt5_status(True)
            self.logger.success(f"‚úÖ MT5 connected: Account {mt5_login} on {mt5_server}")
                
        except Exception as e:
            self.heartbeat_logger.set_mt5_status(False)
            
            # Use centralized error handling
            error_context = handle_mt5_error(e, "mt5_initialization")
            self.logger.error(f"MT5 initialization failed: {error_context.message}")
            
            # Log recovery suggestions
            for suggestion in error_context.recovery_suggestions:
                self.logger.info(f"üí° Recovery: {suggestion}")
            
            raise
    
    async def _initialize_redpanda(self):
        """Initialize Redpanda connection"""
        try:
            # Log Kafka libraries status with consistent format
            self.logger.info(f"üì¶ Kafka Libraries: aiokafka={'‚úÖ' if AIOKAFKA_AVAILABLE else '‚ùå'}, kafka-python={'‚úÖ' if KAFKA_PYTHON_AVAILABLE else '‚ùå'}")
            
            # Get streaming config using safe access
            redpanda_config = self._get_config_value('streaming')
            if not redpanda_config:
                # Fallback to config dict
                redpanda_config = self.config.get('redpanda', {})
            # Get bootstrap servers and client ID with safe access
            bootstrap_servers = self._safe_get_config(redpanda_config, 'bootstrap_servers', 'localhost:9092')
            client_id = self._safe_get_config(redpanda_config, 'client_id', 'mt5_bridge_client')
            
            config = MT5RedpandaConfig(
                bootstrap_servers=bootstrap_servers,
                client_id=client_id
            )
            
            # Test Redpanda server availability using safe access
            primary_servers = self._safe_get_config(redpanda_config, 'primary_servers', [bootstrap_servers])
            available_server = config._find_available_server(primary_servers)
            if available_server:
                self.logger.info(f"üî• Redpanda server found: {available_server}")
            
            # Skip Redpanda initialization - use WebSocket-only mode for reliability
            self.redpanda_manager = None
            self.logger.info("üîÑ Using WebSocket-only mode (Redpanda processing handled server-side)")
            
            # Note: Redpanda processing now handled server-side via WebSocket
            # This eliminates client-side connection timeout issues
                
        except Exception as e:
            self.logger.error(f"Redpanda initialization error: {e}")
            raise
    
    async def _initialize_websocket(self):
        """Initialize WebSocket connection with centralized validation"""
        try:
            # Get WebSocket config from centralized system
            websocket_config = self.config.get('websocket', {})
            # Get network config using safe access
            backend_config = self._get_config_value('network')
            if not backend_config:
                # Fallback to config dict
                backend_config = self.config.get('websocket', {})
            
            # CENTRALIZED VALIDATION - Validate WebSocket configuration using safe access
            ws_url = self._safe_get_config(backend_config, 'websocket_url', 'ws://localhost:8000/api/v1/ws/mt5')
            validation_result = validate_field('websocket_url', ws_url, 'websocket_url')
            
            if not validation_result.is_valid:
                error_messages = [error.message for error in validation_result.errors]
                raise ValueError(f"WebSocket URL validation failed: {'; '.join(error_messages)}")
            
            self.logger.info(f"‚úÖ WebSocket URL validation passed: {ws_url}")
            
            # Initialize WebSocket Client with validated URL using safe access
            auth_token = self._safe_get_config(backend_config, 'auth_token', None)
            
            self.ws_client = WebSocketClient(
                ws_url,
                auth_token
            )
            
            # Register message handlers for control commands
            self._register_ws_handlers()
            
            # Connect with performance tracking
            with track_performance("websocket_connection", "websocket_client"):
                success = await self.ws_client.connect()
                
            if not success:
                self.logger.warning("WebSocket connection failed, will retry later")
                self.heartbeat_logger.set_websocket_status(False)
            else:
                self.heartbeat_logger.set_websocket_status(True)
                self.logger.success(f"‚úÖ WebSocket connected: {ws_url}")
                
        except Exception as e:
            self.heartbeat_logger.set_websocket_status(False)
            
            # Use centralized error handling
            error_context = handle_websocket_error(e, "websocket_initialization")
            self.logger.error(f"WebSocket initialization failed: {error_context.message}")
            
            # Log recovery suggestions
            for suggestion in error_context.recovery_suggestions:
                self.logger.info(f"üí° Recovery: {suggestion}")
            
            # Don't raise - Redpanda can work independently
    
    def _register_ws_handlers(self):
        """Register WebSocket message handlers for control commands"""
        # Original control command handlers
        self.ws_client.register_handler(MessageTypes.CLOSE_POSITION, self._handle_ws_close_position)
        self.ws_client.register_handler(MessageTypes.MODIFY_ORDER, self._handle_ws_modify_order)
        self.ws_client.register_handler(MessageTypes.GET_DATA, self._handle_ws_get_data)
        self.ws_client.register_handler(MessageTypes.EMERGENCY_STOP, self._handle_ws_emergency_stop)
        self.ws_client.register_handler(MessageTypes.CONFIG_UPDATE, self._handle_ws_config_update)
        self.ws_client.register_handler(MessageTypes.DATA_PROCESSED, self._handle_ws_data_processed)
        
        # NEW: Enhanced server optimization handlers
        self.ws_client.register_handler(MessageTypes.LIVE_DATA_PROCESSED, self._handle_ws_live_data_processed)
        self.ws_client.register_handler(MessageTypes.POSITIONS_PROCESSED, self._handle_ws_positions_processed)
        self.ws_client.register_handler(MessageTypes.ORDER_RESULT_PROCESSED, self._handle_ws_order_result_processed)
        self.ws_client.register_handler(MessageTypes.HEARTBEAT_RESPONSE, self._handle_ws_heartbeat_response)
        self.ws_client.register_handler(MessageTypes.ERROR_ACKNOWLEDGED, self._handle_ws_error_acknowledged)
        self.ws_client.register_handler(MessageTypes.PERFORMANCE_ALERT, self._handle_ws_performance_alert)
        
        # NEW: Additional server response handlers
        self.ws_client.register_handler("connection_established", self._handle_ws_connection_established)
        self.ws_client.register_handler("message_received", self._handle_ws_message_received)
        self.ws_client.register_handler("account_info_processed", self._handle_ws_account_info_processed)
        
        # Server error handler
        self.ws_client.register_handler("error", self._handle_ws_error)
        
        # Server tick data processing confirmation
        self.ws_client.register_handler("tick_data_processed", self._handle_ws_tick_data_processed)
        
        # Additional server response handlers for missing message types
        self.ws_client.register_handler("connection_acknowledged", self._handle_ws_connection_acknowledged)
        self.ws_client.register_handler("orders_processed", self._handle_ws_orders_processed)
        self.ws_client.register_handler("deals_history_processed", self._handle_ws_deals_history_processed)
        self.ws_client.register_handler("orders_history_processed", self._handle_ws_orders_history_processed)
        self.ws_client.register_handler("unknown", self._handle_ws_unknown)
    
    async def _start_background_tasks(self):
        """Start background tasks"""
        # High-frequency tick streaming via Redpanda
        self.tick_streaming_task = asyncio.create_task(self._tick_streaming_loop())
        
        # Low-frequency account monitoring via WebSocket
        self.account_monitoring_task = asyncio.create_task(self._account_monitoring_loop())
        
        # Data sources monitoring
        self.data_sources_monitoring_task = asyncio.create_task(self._data_sources_monitoring_loop())
        
        # WebSocket message listening
        if self.ws_client and self.ws_client.is_connected:
            self.ws_listening_task = asyncio.create_task(self.ws_client.listen_for_messages())
    
    def _setup_signal_handlers(self):
        """Setup signal handlers for graceful shutdown"""
        def signal_handler(signum, frame):
            self.logger.info(f"Received signal {signum}, shutting down...")
            asyncio.create_task(self.stop())
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
    
    async def _display_status(self):
        """Display current status with comprehensive formatting"""
        print("="*70)
        print("üåâ HYBRID MT5 BRIDGE - DUAL CHANNEL COMMUNICATION")
        print("="*70)
        
        # Basic status info
        print(f"    üìÖ Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # MT5 Account information (if available)
        if self.mt5_handler:
            try:
                account_info = await self.mt5_handler.get_account_info()
                if account_info:
                    print(f"    üìä Server    : {account_info.server}")
                    print(f"    üë§ Account    : {account_info.login}")
                    print(f"    üí∞ Balance    : {account_info.balance:.2f} {account_info.currency}")
                    print(f"    üè¢ Broker    : {account_info.company}")
                    
                    # Update heartbeat logger with account info
                    self.heartbeat_logger.update_account(account_info)
                else:
                    print(f"    üìà MT5          : ‚úÖ Connected")
            except:
                print(f"    üìà MT5          : ‚úÖ Connected")
        else:
            print(f"    üìà MT5          : ‚ùå Disconnected")
            
        trading_enabled = self._get_config_value('app.trading_enabled')
        if trading_enabled is None:
            trading_enabled = self.config.get('trading', {}).get('enabled', False)
        print(f"    üí∞ Trading    : {'‚úÖ ENABLED' if trading_enabled else '‚ùå DISABLED'}")
        
        # Update heartbeat logger with trading status
        self.heartbeat_logger.set_trading_status(trading_enabled)
        print("="*70)
        
        # Streaming symbols
        print(f"    {', '.join(self.tick_symbols)}")
        print("="*70)
        
        # Data streams
        print(f"       üî• High-Freq (Redpanda)    : Tick data, Market events")
        print(f"       üåê Low-Freq (WebSocket)    : Account updates, Control commands")
        print("="*70)
    
    async def _main_loop(self):
        """Main application loop with connection monitoring"""
        try:
            while self.running:
                # Check and maintain connections
                await self._check_connections()
                
                # Sleep
                await asyncio.sleep(5)
                
        except Exception as e:
            self.logger.error(f"Main loop error: {e}")
    
    async def _check_connections(self):
        """Check and maintain all connections"""
        try:
            # Check MT5 connection
            if not await self.mt5_handler.is_connected():
                self.logger.warning("MT5 connection lost, attempting reconnect...")
                await self.mt5_handler.connect()
            
            # Check Redpanda connection
            # Note: Redpanda processing handled server-side, no client-side reconnection needed
            
            # Check WebSocket connection
            if self.ws_client:
                is_connected = self.ws_client.is_connected
                self.heartbeat_logger.set_websocket_status(is_connected)
                
                if not is_connected:
                    self.heartbeat_logger.log_important("‚ö†Ô∏è WebSocket connection lost, attempting reconnect...")
                    
                    # Update reconnect attempts count if available
                    if hasattr(self.ws_client, 'reconnect_attempts'):
                        self.heartbeat_logger.update_reconnect_attempts(self.ws_client.reconnect_attempts)
                    
                    await self.ws_client.auto_reconnect()
                    
                    # Update status after reconnect attempt
                    self.heartbeat_logger.set_websocket_status(self.ws_client.is_connected)
                    if hasattr(self.ws_client, 'reconnect_attempts'):
                        self.heartbeat_logger.update_reconnect_attempts(self.ws_client.reconnect_attempts)
                
        except Exception as e:
            self.logger.error(f"Check connections error: {e}")
    
    async def _tick_streaming_loop(self):
        """Stream tick data via Redpanda (High-frequency)"""
        try:
            self.logger.info("üì° Started tick data streaming via Redpanda\n")
            
            # First, ensure all symbols are selected in MT5
            for symbol in self.tick_symbols:
                try:
                    success = await self.mt5_handler.select_symbol(symbol)
                    if success:
                        self.logger.info(f"‚úÖ Symbol {symbol} selected for streaming")
                    else:
                        self.logger.warning(f"‚ùå Failed to select symbol {symbol}")
                except Exception as e:
                    self.logger.error(f"Error selecting symbol {symbol}: {e}")
            
            tick_count = 0
            
            while self.running:
                # Note: Redpanda processing handled server-side
                if False:  # Skip Redpanda health check
                    await asyncio.sleep(1)
                    continue
                
                # Stream tick data for each symbol
                for symbol in self.tick_symbols:
                    try:
                        tick = await self.mt5_handler.get_tick(symbol)
                        if tick:
                            # Convert tick to dict
                            tick_data = {
                                "symbol": symbol,  # Use symbol from loop, not tick.symbol
                                "bid": tick.bid,
                                "ask": tick.ask,
                                "last": tick.last,
                                "volume": tick.volume,
                                "spread": tick.ask - tick.bid,  # Calculate spread
                                "time": tick.time.isoformat()  # Already datetime object
                            }
                            
                            # Primary: Send to WebSocket for immediate processing & database storage
                            if self.ws_client and self.ws_client.is_connected:
                                await self.ws_client.send_tick_data(tick_data)
                            
                            # Note: Redpanda processing now handled server-side via WebSocket
                            # This eliminates client-side Redpanda connection issues
                            
                            # Track last tick time
                            self.last_tick_times[symbol] = datetime.now()
                            
                            # Update heartbeat counter (no logging spam)
                            self.heartbeat_logger.update_tick_count()
                            tick_count += 1
                        else:
                            # Log when no tick data is available
                            self.logger.warning(f"‚ö†Ô∏è No tick data available for {symbol}")
                            
                    except Exception as e:
                        self.logger.error(f"Tick streaming error for {symbol}: {e}")
                        continue
                
                # Short delay untuk high-frequency streaming
                await asyncio.sleep(0.1)  # 10 Hz tick streaming
                
        except asyncio.CancelledError:
            self.logger.debug("Tick streaming loop cancelled")
        except Exception as e:
            self.logger.error(f"Tick streaming error: {e}")
    
    async def _account_monitoring_loop(self):
        """Monitor account status via WebSocket (Low-frequency)"""
        try:
            self.logger.info("üìä Started account monitoring via WebSocket")
            
            while self.running:
                try:
                    # Get account info
                    account_info = await self.mt5_handler.get_account_info()
                    if account_info:
                        # Send via WebSocket
                        if self.ws_client and self.ws_client.is_connected:
                            await self.ws_client.send_account_info(account_info)
                            self.heartbeat_logger.update_account(account_info)
                        
                        # Also send via Redpanda for analytics
                        if False:  # Skip client-side Redpanda (handled server-side)
                            account_data = asdict(account_info)
                            await self.redpanda_manager.send_account_update(account_data)
                    
                    # Get positions
                    positions = await self.mt5_handler.get_positions()
                    if self.ws_client and self.ws_client.is_connected:
                        await self.ws_client.send_positions(positions)
                        self.heartbeat_logger.update_positions()
                    
                    # Get orders
                    orders = await self.mt5_handler.get_orders()
                    if self.ws_client and self.ws_client.is_connected:
                        await self.ws_client.send_orders(orders)
                    
                    # Get historical deals (every 60 seconds to avoid spam)
                    if hasattr(self, 'last_history_sync'):
                        time_since_last = (datetime.now() - self.last_history_sync).total_seconds()
                        if time_since_last >= 60:  # 1 minute interval
                            deals_history = await self.mt5_handler.get_deals_history()  # ALL TIME - same as initial
                            if self.ws_client and self.ws_client.is_connected:
                                await self.ws_client.send_deals_history(deals_history)
                            
                            orders_history = await self.mt5_handler.get_orders_history()  # ALL TIME - same as initial
                            if self.ws_client and self.ws_client.is_connected:
                                await self.ws_client.send_orders_history(orders_history)
                            
                            self.last_history_sync = datetime.now()
                            # Update historical data counts
                            self.heartbeat_logger.update_historical_data(len(deals_history), len(orders_history))
                            # Use heartbeat logger instead of regular logger to avoid breaking display
                            self.heartbeat_logger.log_important(f"üìú History synced: {len(deals_history)} deals, {len(orders_history)} orders")
                    else:
                        # First run - sync ALL TIME history immediately
                        deals_history = await self.mt5_handler.get_deals_history()  # ALL TIME
                        if self.ws_client and self.ws_client.is_connected:
                            await self.ws_client.send_deals_history(deals_history)
                        
                        orders_history = await self.mt5_handler.get_orders_history()  # ALL TIME
                        if self.ws_client and self.ws_client.is_connected:
                            await self.ws_client.send_orders_history(orders_history)
                            
                        # Update historical data counts
                        self.heartbeat_logger.update_historical_data(len(deals_history), len(orders_history))
                        
                        self.last_history_sync = datetime.now()
                        # Use heartbeat logger instead of regular logger to avoid breaking display  
                        if len(deals_history) == 0 and len(orders_history) == 0:
                            # Get account info for debugging
                            account_info = await self.mt5_handler.get_account_info()
                            if account_info:
                                self.heartbeat_logger.log_important(f"üìú No trading history found (new {account_info.server} demo account?)")
                                self.logger.info(f"üí° Account: {account_info.login}@{account_info.server} - Try placing test trades first")
                            else:
                                self.heartbeat_logger.log_important(f"üìú No trading history found - Try placing test trades first")
                        else:
                            self.heartbeat_logger.log_important(f"üìú Initial history loaded: {len(deals_history)} deals, {len(orders_history)} orders")
                    
                except Exception as e:
                    # Only show critical errors in heartbeat, debug goes to file only
                    if "TradePosition" in str(e) or "TradeOrder" in str(e):
                        self.logger.debug(f"Account monitoring error: {e}")  # File only
                    else:
                        self.heartbeat_logger.log_important(f"‚ö†Ô∏è Account monitoring error: {e}")
                
                # Wait before next update (low frequency)
                await asyncio.sleep(10)  # Update every 10 seconds
                
        except asyncio.CancelledError:
            self.logger.debug("Account monitoring loop cancelled")
        except Exception as e:
            self.logger.error(f"Account monitoring error: {e}")
    
    async def _data_sources_monitoring_loop(self):
        """Monitor data sources status (Low-frequency)"""
        try:
            self.logger.info("üìä Started data sources monitoring")
            
            while self.running:
                try:
                    # Update data sources status in heartbeat logger
                    await self.heartbeat_logger.update_data_sources_status()
                    
                    # Update Docker services status
                    await self.heartbeat_logger.check_docker_services_status()
                    
                    # Update database stack status
                    await self.heartbeat_logger.check_database_stack_status()
                    
                    # Update data processing pipeline status
                    await self.heartbeat_logger.check_data_pipeline_status()
                    
                    # Update WebSocket endpoints status
                    await self.heartbeat_logger.update_websocket_endpoints_status()
                    
                except Exception as e:
                    self.logger.debug(f"Data sources monitoring error: {e}")
                
                # Wait before next update (low frequency - every 10 seconds)
                await asyncio.sleep(10)
                
        except asyncio.CancelledError:
            self.logger.debug("Data sources monitoring loop cancelled")
        except Exception as e:
            self.logger.error(f"Data sources monitoring error: {e}")
    
    # Redpanda Signal Handlers
    
    async def _handle_redpanda_signal(self, topic: str, key: str, value: Dict[str, Any]):
        """Handle trading signal from Redpanda"""
        try:
            signal_data = value.get("data", {})
            
            self.logger.info(f"üìà Received Redpanda signal: {signal_data}")
            
            # Check if trading is enabled
            trading_enabled = self._get_config_value('app.trading_enabled')
            emergency_stop = self._get_config_value('app.emergency_stop')
            
            if trading_enabled is None:
                trading_enabled = self.config.get('trading', {}).get('enabled', False)
            if emergency_stop is None:
                emergency_stop = self.config.get('trading', {}).get('emergency_stop', False)
                
            if not trading_enabled or emergency_stop:
                self.logger.warning("Trading disabled, ignoring Redpanda signal")
                return
            
            # Check daily trade limit
            if not self._check_daily_trade_limit():
                self.logger.warning("Daily trade limit exceeded, ignoring signal")
                return
            
            # Process signal sama seperti WebSocket handler
            await self._process_trading_signal(signal_data, source="redpanda")
            
        except Exception as e:
            self.logger.error(f"Handle Redpanda signal error: {e}")
    
    async def _process_trading_signal(self, signal_data: Dict[str, Any], source: str = "unknown"):
        """Process trading signal dari any source"""
        try:
            # Extract signal parameters
            symbol = signal_data.get("symbol")
            action = signal_data.get("action")  # "BUY" or "SELL"
            volume = signal_data.get("volume", 0.01)
            sl = signal_data.get("stop_loss")
            tp = signal_data.get("take_profit")
            comment = f"AI Signal ({source})"
            
            if not symbol or not action:
                self.logger.error("Invalid signal: missing symbol or action")
                return
            
            # Place order
            order_type = MT5_ORDER_TYPES.get(action)
            if order_type is None:
                self.logger.error(f"Invalid order type: {action}")
                return
            
            ticket = await self.mt5_handler.place_order(
                symbol=symbol,
                order_type=order_type,
                volume=volume,
                sl=sl,
                tp=tp,
                comment=comment
            )
            
            # Send trade result event via Redpanda
            if False:  # Skip client-side Redpanda (handled server-side)
                event_data = {
                    "signal_id": signal_data.get("id"),
                    "symbol": symbol,
                    "action": action,
                    "volume": volume,
                    "ticket": ticket,
                    "success": ticket is not None,
                    "message": "Order placed successfully" if ticket else "Order failed",
                    "source": source
                }
                await self.redpanda_manager.send_trading_event(EventTypes.ORDER_PLACED, event_data)
            
            # Also send via WebSocket if available
            if self.ws_client and self.ws_client.is_connected:
                result = {
                    "signal_id": signal_data.get("id"),
                    "success": ticket is not None,
                    "ticket": ticket,
                    "message": "Order placed successfully" if ticket else "Order failed"
                }
                await self.ws_client.send_trade_result(result)
            
            if ticket:
                self.daily_trades_count += 1
                self.logger.success(f"‚úÖ Signal executed from {source}: {symbol} {action} - Ticket: {ticket}")
            
        except Exception as e:
            self.logger.error(f"Process trading signal error: {e}")
    
    # WebSocket Command Handlers (Low-frequency control)
    
    async def _handle_ws_close_position(self, data: Dict[str, Any]):
        """Handle close position command from WebSocket"""
        try:
            position_data = data.get("data", {})
            ticket = position_data.get("ticket")
            
            if not ticket:
                self.logger.error("Close position: missing ticket")
                return
            
            success = await self.mt5_handler.close_position(ticket)
            
            # Send event via Redpanda
            if False:  # Skip client-side Redpanda (handled server-side)
                event_data = {
                    "ticket": ticket,
                    "success": success,
                    "message": "Position closed" if success else "Close failed",
                    "source": "websocket"
                }
                await self.redpanda_manager.send_trading_event(EventTypes.POSITION_CLOSED, event_data)
            
            # Reply via WebSocket
            result = {
                "ticket": ticket,
                "success": success,
                "message": "Position closed" if success else "Close failed"
            }
            await self.ws_client.send_trade_result(result)
            
        except Exception as e:
            self.logger.error(f"Handle close position error: {e}")
    
    async def _handle_ws_modify_order(self, data: Dict[str, Any]):
        """Handle modify order command"""
        try:
            # Implementation for order modification
            self.logger.info("Order modification not implemented yet")
            
        except Exception as e:
            self.logger.error(f"Handle modify order error: {e}")
    
    async def _handle_ws_get_data(self, data: Dict[str, Any]):
        """Handle data request from WebSocket"""
        try:
            request_type = data.get("request_type")
            
            if request_type == "account_info":
                account_info = await self.mt5_handler.get_account_info()
                await self.ws_client.send_account_info(account_info)
            
            elif request_type == "positions":
                positions = await self.mt5_handler.get_positions()
                await self.ws_client.send_positions(positions)
            
            elif request_type == "orders":
                orders = await self.mt5_handler.get_orders()
                await self.ws_client.send_orders(orders)
            
        except Exception as e:
            self.logger.error(f"Handle get data error: {e}")
    
    async def _handle_ws_emergency_stop(self, data: Dict[str, Any]):
        """Handle emergency stop command"""
        try:
            self.logger.critical("üö® EMERGENCY STOP TRIGGERED VIA WEBSOCKET")
            
            # Close all positions
            positions = await self.mt5_handler.get_positions()
            for position in positions:
                await self.mt5_handler.close_position(position.ticket)
            
            # Set emergency stop flag
            # Note: This is a runtime setting change, may need to be persisted
            # For now, we'll update both potential configuration sources
            if hasattr(self.settings, 'app') and hasattr(self.settings.app, 'emergency_stop'):
                self.settings.app.emergency_stop = True
            if 'trading' in self.config:
                self.config['trading']['emergency_stop'] = True
            
            # Send emergency event via Redpanda
            if False:  # Skip client-side Redpanda (handled server-side)
                event_data = {
                    "trigger": "websocket_command",
                    "positions_closed": len(positions),
                    "timestamp": datetime.now().isoformat()
                }
                await self.redpanda_manager.send_trading_event("emergency_stop", event_data)
            
            self.logger.warning("All positions closed, trading stopped")
            
        except Exception as e:
            self.logger.error(f"Handle emergency stop error: {e}")
    
    async def _handle_ws_config_update(self, data: Dict[str, Any]):
        """Handle configuration update"""
        try:
            config_data = data.get("data", {})
            self.logger.info(f"Configuration update: {config_data}")
            
            # Update settings (implementation depends on requirements)
            
        except Exception as e:
            self.logger.error(f"Handle config update error: {e}")
    
    async def _handle_ws_data_processed(self, data: Dict[str, Any]):
        """Handle data processed acknowledgment from server"""
        try:
            # This is just an acknowledgment that the server received and processed our data
            # Server sends message_type and status at the root level of the response
            message_type = data.get("message_type", "unknown")
            status = data.get("status", "unknown")
            
            # Update heartbeat logger with server status
            self.heartbeat_logger.set_server_status(status)
            
            # Optional: Update metrics or track successful data transmissions
            # self.metrics.increment_processed_count(message_type)
            
        except Exception as e:
            self.logger.error(f"Handle data processed error: {e}")
    
    def _check_daily_trade_limit(self) -> bool:
        """Check if daily trade limit is reached"""
        today = datetime.now().date()
        
        # Reset counter if new day
        if self.last_trade_date != today:
            self.daily_trades_count = 0
            self.last_trade_date = today
        
        max_daily_trades = self._get_config_value('app.max_daily_trades')
        if max_daily_trades is None:
            max_daily_trades = self.config.get('trading', {}).get('max_daily_trades', 10)
            
        return self.daily_trades_count < max_daily_trades
    
    # NEW: Enhanced WebSocket server optimization handlers
    
    async def _handle_ws_live_data_processed(self, data: Dict[str, Any]):
        """Handle enhanced live data processed response from server"""
        try:
            response_data = data.get("data", data)
            self.logger.debug(f"‚úÖ Server processed live data: {response_data.get('status', 'unknown')}")
            
            # Enhanced response tracking for hybrid bridge
            if "account_balance" in response_data:
                self.logger.info(f"üí∞ Account Balance confirmed: {response_data['account_balance']}")
                
            # Track processing for Redpanda analytics
            # Skip client-side Redpanda (handled server-side)
            if False:
                processing_event = {
                    "event_type": "live_data_confirmed",
                    "response_data": response_data,
                    "timestamp": datetime.now().isoformat(),
                    "source": "hybrid_bridge"
                }
                # Fire-and-forget event (don't await to maintain performance)
                asyncio.create_task(
                    self.redpanda_manager.send_trading_event("processing_confirmation", processing_event)
                )
            
        except Exception as e:
            self.logger.error(f"Handle live data processed error: {e}")
    
    async def _handle_ws_positions_processed(self, data: Dict[str, Any]):
        """Handle enhanced positions processed response from server"""
        try:
            response_data = data.get("data", data)
            self.logger.debug(f"üìã Server processed positions: {response_data.get('status', 'unknown')}")
            
            # Enhanced position tracking for risk management
            if "position_count" in response_data and "total_profit" in response_data:
                position_count = response_data['position_count']
                total_profit = response_data['total_profit']
                
                self.logger.info(f"üìä Positions confirmed: {position_count}, Total P&L: {total_profit}")
                
                # Stream position summary to Redpanda for analytics
                # Skip client-side Redpanda (handled server-side)
                if False:
                    position_summary = {
                        "event_type": "position_summary_confirmed",
                        "position_count": position_count,
                        "total_profit": total_profit,
                        "timestamp": datetime.now().isoformat(),
                        "source": "hybrid_bridge"
                    }
                    asyncio.create_task(
                        self.redpanda_manager.send_trading_event("position_confirmation", position_summary)
                    )
            
        except Exception as e:
            self.logger.error(f"Handle positions processed error: {e}")
    
    async def _handle_ws_order_result_processed(self, data: Dict[str, Any]):
        """Handle enhanced order result processed response from server"""
        try:
            response_data = data.get("data", data)
            self.logger.debug(f"üìù Server processed order result: {response_data.get('status', 'unknown')}")
            
            # Enhanced order tracking for trade journal
            if "order_data" in response_data:
                order_info = response_data['order_data']
                self.logger.info(f"‚úÖ Order result confirmed by server: {order_info}")
                
                # Forward order confirmation to Redpanda for trade analytics
                # Skip client-side Redpanda (handled server-side)
                if False:
                    order_confirmation = {
                        "event_type": "order_result_confirmed",
                        "order_data": order_info,
                        "timestamp": datetime.now().isoformat(),
                        "source": "hybrid_bridge"
                    }
                    asyncio.create_task(
                        self.redpanda_manager.send_trading_event("order_confirmation", order_confirmation)
                    )
            
        except Exception as e:
            self.logger.error(f"Handle order result processed error: {e}")
    
    async def _handle_ws_heartbeat_response(self, data: Dict[str, Any]):
        """Handle enhanced heartbeat response with server and Redpanda stats"""
        try:
            response_data = data.get("data", data)
            server_status = response_data.get('server_status', 'unknown')
            
            self.logger.debug(f"üíì Enhanced heartbeat response: {server_status}")
            
            # Enhanced heartbeat with dual-channel stats
            if "data_flow_stats" in response_data:
                server_stats = response_data['data_flow_stats']
                self.logger.debug(f"üìä Server Data Flow Stats:")
                self.logger.debug(f"   Live Updates: {server_stats.get('live_updates', 0)}")
                self.logger.debug(f"   Analytical Batches: {server_stats.get('analytical_batches', 0)}")
                self.logger.debug(f"   Buffer Size: {server_stats.get('buffer_size', 0)}")
                
                # Combine with Redpanda stats if available
                redpanda_stats = {}
                if self.redpanda_manager and hasattr(self.redpanda_manager, 'get_stats'):
                    try:
                        redpanda_stats = self.redpanda_manager.get_stats()
                    except:
                        pass
                
                # Store combined stats for monitoring
                self.dual_channel_stats = {
                    "server_stats": server_stats,
                    "redpanda_stats": redpanda_stats,
                    "last_updated": datetime.now().isoformat()
                }
                
                # Log combined performance if both channels active
                if redpanda_stats and server_stats:
                    self.logger.debug(f"üåâ Dual Channel Performance:")
                    self.logger.debug(f"   WebSocket Live Updates: {server_stats.get('live_updates', 0)}")
                    self.logger.debug(f"   Redpanda Events Sent: {redpanda_stats.get('events_sent', 0)}")
            
        except Exception as e:
            self.logger.error(f"Handle heartbeat response error: {e}")
    
    async def _handle_ws_error_acknowledged(self, data: Dict[str, Any]):
        """Handle enhanced error acknowledgment from server"""
        try:
            response_data = data.get("data", data)
            handler_info = response_data.get('handled_by', 'unknown')
            
            self.logger.debug(f"üö® Server acknowledged error via: {handler_info}")
            
            # Enhanced error tracking for hybrid bridge
            if "error_data" in response_data:
                error_info = response_data['error_data']
                self.logger.info(f"‚úÖ Error handled by server: {error_info}")
                
                # Forward error acknowledgment to Redpanda for error analytics
                # Skip client-side Redpanda (handled server-side)
                if False:
                    error_ack = {
                        "event_type": "error_acknowledged",
                        "error_data": error_info,
                        "handled_by": handler_info,
                        "timestamp": datetime.now().isoformat(),
                        "source": "hybrid_bridge"
                    }
                    asyncio.create_task(
                        self.redpanda_manager.send_trading_event("error_acknowledgment", error_ack)
                    )
            
        except Exception as e:
            self.logger.error(f"Handle error acknowledged error: {e}")
    
    async def _handle_ws_error(self, data: Dict[str, Any]):
        """Handle error messages from server"""
        try:
            error_message = data.get("message", "Unknown error")
            error_time = data.get("timestamp", "Unknown time")
            
            # Log the error with context
            self.logger.warning(f"üö® Server error: {error_message}")
            self.logger.debug(f"üìÖ Error time: {error_time}")
            
            # Update connection status only for serious connection errors (not validation errors)
            if "connection" in error_message.lower() and "invalid" not in error_message.lower():
                self.server_status = "error"
                self.logger.warning("üî¥ Server status updated to error due to connection error")
            
        except Exception as e:
            self.logger.error(f"Handle server error message error: {e}")
    
    async def _handle_ws_tick_data_processed(self, data: Dict[str, Any]):
        """Handle tick data processing confirmation from server"""
        try:
            # This is a high-frequency message, so log minimally
            processed_data = data.get("data", {})
            symbol = processed_data.get("symbol", "unknown")
            timestamp = processed_data.get("timestamp", "unknown")
            
            # Update server status to show it's processing data
            self.heartbeat_logger.set_server_status("received")
            
            # Optional: Log occasionally for monitoring (every 100th message)
            if not hasattr(self, '_tick_processed_count'):
                self._tick_processed_count = 0
            self._tick_processed_count += 1
            
            if self._tick_processed_count % 100 == 0:
                self.logger.debug(f"üìà Server processed {self._tick_processed_count} ticks (latest: {symbol} at {timestamp})")
            
        except Exception as e:
            self.logger.error(f"Handle tick data processed error: {e}")
    
    async def _handle_ws_connection_acknowledged(self, data: Dict[str, Any]):
        """Handle connection acknowledgment from server"""
        try:
            client_id = data.get("client_id", "unknown")
            status = data.get("status", "unknown")
            self.logger.debug(f"üîó Connection acknowledged: client_id={client_id}, status={status}")
            self.heartbeat_logger.set_server_status("connected")
        except Exception as e:
            self.logger.error(f"Handle connection acknowledged error: {e}")
    
    async def _handle_ws_orders_processed(self, data: Dict[str, Any]):
        """Handle orders processing confirmation from server"""
        try:
            count = data.get("count", 0)
            self.logger.debug(f"üìã Server processed {count} orders")
            self.heartbeat_logger.update_positions()
        except Exception as e:
            self.logger.error(f"Handle orders processed error: {e}")
    
    async def _handle_ws_deals_history_processed(self, data: Dict[str, Any]):
        """Handle deals history processing confirmation from server"""
        try:
            count = data.get("count", 0)
            self.logger.debug(f"üìú Server processed {count} deals from history")
        except Exception as e:
            self.logger.error(f"Handle deals history processed error: {e}")
    
    async def _handle_ws_orders_history_processed(self, data: Dict[str, Any]):
        """Handle orders history processing confirmation from server"""
        try:
            count = data.get("count", 0)
            self.logger.debug(f"üìù Server processed {count} orders from history")
        except Exception as e:
            self.logger.error(f"Handle orders history processed error: {e}")
    
    async def _handle_ws_unknown(self, data: Dict[str, Any]):
        """Handle unknown message types from server"""
        try:
            message_type = data.get("type", "truly_unknown")
            self.logger.debug(f"‚ùì Unknown message type received: {message_type}")
        except Exception as e:
            self.logger.error(f"Handle unknown message error: {e}")
    
    async def _handle_ws_performance_alert(self, data: Dict[str, Any]):
        """Handle performance monitoring alerts from server with hybrid bridge actions"""
        try:
            alert_data = data.get("data", data)
            alert_type = alert_data.get("alert_type", "unknown")
            message = alert_data.get("message", "Performance alert received")
            severity = alert_data.get("severity", "medium")
            
            # Log based on severity with hybrid bridge context
            if severity == "critical":
                self.logger.critical(f"üö® CRITICAL PERFORMANCE ALERT (Hybrid): {message}")
            elif severity == "high":
                self.logger.error(f"‚ö†Ô∏è HIGH PERFORMANCE ALERT (Hybrid): {message}")
            elif severity == "medium":
                self.logger.warning(f"‚ö†Ô∏è PERFORMANCE ALERT (Hybrid): {message}")
            else:
                self.logger.info(f"üìä Performance Info (Hybrid): {message}")
            
            # Take hybrid-specific actions based on alert type
            if alert_type == "high_latency":
                self.logger.info("üîÑ Hybrid Action: Consider switching more data to Redpanda channel")
            elif alert_type == "memory_usage":
                self.logger.info("üíæ Hybrid Action: Flushing analytical buffers via Redpanda")
                if self.redpanda_manager and hasattr(self.redpanda_manager, 'flush_buffers'):
                    await self.redpanda_manager.flush_buffers()
            elif alert_type == "connection_issues":
                self.logger.info("üîå Hybrid Action: Enhanced connection monitoring for both channels")
                # Monitor both WebSocket and Redpanda connections
                asyncio.create_task(self._enhanced_connection_monitoring())
            elif alert_type == "data_flow_imbalance":
                self.logger.info("üåâ Hybrid Action: Rebalancing WebSocket vs Redpanda data flows")
            
            # Store performance alerts for trend analysis
            if not hasattr(self, 'performance_alerts'):
                self.performance_alerts = []
            self.performance_alerts.append({
                "timestamp": datetime.now(),
                "alert_type": alert_type,
                "severity": severity,
                "message": message,
                "bridge_type": "hybrid"
            })
            
            # Keep only last 50 alerts
            if len(self.performance_alerts) > 50:
                self.performance_alerts = self.performance_alerts[-50:]
            
            # Forward performance alert to Redpanda for monitoring analytics
            # Skip client-side Redpanda (handled server-side)
            if False:
                performance_alert = {
                    "event_type": "performance_alert_received",
                    "alert_type": alert_type,
                    "severity": severity,
                    "message": message,
                    "timestamp": datetime.now().isoformat(),
                    "bridge_type": "hybrid",
                    "source": "server_monitoring"
                }
                asyncio.create_task(
                    self.redpanda_manager.send_trading_event("performance_monitoring", performance_alert)
                )
            
        except Exception as e:
            self.logger.error(f"Handle performance alert error: {e}")
    
    async def _handle_ws_connection_established(self, data: Dict[str, Any]):
        """Handle connection established message from server"""
        try:
            message = data.get("message", "Connected to server")
            server_info = data.get("server_info", {})
            client_id = data.get("client_id", "unknown")
            
            self.logger.success(f"üîå {message}")
            self.logger.info(f"üë§ Client ID: {client_id}")
            
            if server_info:
                version = server_info.get("version", "unknown")
                architecture = server_info.get("architecture", "unknown")
                features = server_info.get("features", [])
                
                self.logger.info(f"üèóÔ∏è Architecture: {architecture}")
                self.logger.info(f"üì¶ Version: {version}")
                if features:
                    self.logger.info(f"‚ú® Features: {', '.join(features)}")
            
            # Mark connection as established
            self.heartbeat_logger.set_websocket_status(True)
            self.heartbeat_logger.set_server_status("received")  # Set server status to online
            
        except Exception as e:
            self.logger.error(f"Handle connection established error: {e}")
    
    async def _handle_ws_message_received(self, data: Dict[str, Any]):
        """Handle generic message received acknowledgment from server"""
        try:
            message = data.get("message", "Message acknowledged")
            original_type = data.get("original_type", "unknown")
            
            # Only log if debug mode is enabled to avoid spam
            if hasattr(self, 'debug_mode') and self.debug_mode:
                self.logger.debug(f"üì® Server acknowledged: {original_type} - {message}")
            
        except Exception as e:
            self.logger.error(f"Handle message received error: {e}")
    
    async def _handle_ws_account_info_processed(self, data: Dict[str, Any]):
        """Handle account info processed acknowledgment from server"""
        try:
            message = data.get("message", "Account info processed")
            account = data.get("account", "unknown")
            
            # Silent update - just update internal status, no logging
            # Account info is routine data, only log on first connection or errors
            
            # Update account status
            if hasattr(self, 'account_status'):
                self.account_status['server_processed'] = True
                self.account_status['last_processed'] = datetime.now()
                self.account_status['account_number'] = account
            
        except Exception as e:
            self.logger.error(f"Handle account info processed error: {e}")
    
    async def _enhanced_connection_monitoring(self):
        """Enhanced connection monitoring for both WebSocket and Redpanda channels"""
        try:
            self.logger.info("üîç Starting enhanced dual-channel connection monitoring")
            
            # Monitor WebSocket connection
            ws_healthy = self.ws_client and self.ws_client.is_connected
            self.logger.info(f"üåê WebSocket Status: {'‚úÖ Healthy' if ws_healthy else '‚ùå Unhealthy'}")
            
            # Monitor Redpanda connection
            redpanda_healthy = False  # Skip client-side Redpanda (handled server-side)
            self.logger.info(f"üî• Redpanda Status: {'‚úÖ Healthy' if redpanda_healthy else '‚ùå Unhealthy'}")
            
            # Report monitoring results
            monitoring_result = {
                "websocket_healthy": ws_healthy,
                "redpanda_healthy": redpanda_healthy,
                "dual_channel_status": "optimal" if ws_healthy and redpanda_healthy else "degraded",
                "timestamp": datetime.now().isoformat()
            }
            
            if redpanda_healthy:
                monitoring_event = {
                    "event_type": "dual_channel_health_check",
                    "monitoring_result": monitoring_result,
                    "timestamp": datetime.now().isoformat(),
                    "source": "hybrid_bridge_monitoring"
                }
                await self.redpanda_manager.send_trading_event("system_monitoring", monitoring_event)
            
            self.logger.info(f"üè• Dual-channel health: {monitoring_result['dual_channel_status']}")
            
        except Exception as e:
            self.logger.error(f"Enhanced connection monitoring error: {e}")


# Global bridge instance for integration
_bridge_instance = None

async def get_heartbeat_logger():
    """Get the heartbeat logger instance from the bridge"""
    global _bridge_instance
    if _bridge_instance and hasattr(_bridge_instance, 'heartbeat_logger'):
        return _bridge_instance.heartbeat_logger
    return None

async def main():
    """Main entry point for Hybrid MT5 Bridge"""
    global _bridge_instance
    try:
        # Start Hybrid MT5 Bridge with centralized configuration
        _bridge_instance = HybridMT5Bridge()
        await _bridge_instance.start()
        
    except KeyboardInterrupt:
        print("Application interrupted by user")
    except Exception as e:
        print(f"Application error: {e}")
    finally:
        _bridge_instance = None
        print("Hybrid MT5 Bridge finished")


if __name__ == "__main__":
    asyncio.run(main())