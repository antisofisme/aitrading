# Data Ingestion Architecture Review - Executive Summary

**Date:** 2025-10-12
**Reviewed Components:** Polygon Historical Downloader, Data Bridge, Tick Aggregator, NATS, TimescaleDB, ClickHouse
**Overall Grade:** B+ (Good with Critical Improvements Needed)

---

## Quick Overview

### System Flow
```
Polygon.io ‚Üí [Live/Historical Collectors] ‚Üí NATS ‚Üí Data-bridge ‚Üí [TimescaleDB + ClickHouse]
                                                          ‚Üì
                                                   Tick Aggregator
```

### Current Performance
- **Live Data:** 40 msg/sec (14 symbols)
- **Historical Burst:** 50,000 msg/sec
- **Data Coverage:** 2.8 years (2023-2025)
- **Storage:** TimescaleDB (2-3 days), ClickHouse (unlimited)

---

## Critical Issues Found

### 1. NATS "Slow Consumer" Warnings ‚ö†Ô∏è CRITICAL
**Problem:** Data-bridge can't keep up with historical download bursts
- Single consumer processing 1,000 msg/batch
- Publisher sends 50,000 msg/sec during 3-month chunks
- 50x mismatch creates queue buildup

**Fix:** Scale data-bridge to 3 instances with NATS queue groups
```yaml
docker-compose up --scale data-bridge=3
```
**Impact:** 3x throughput, eliminates warnings
**Effort:** 2 days

---

### 2. TimescaleDB Retention vs Gap Lookback Mismatch ‚ö†Ô∏è HIGH
**Problem:** Gap detector checks 30 days, but TimescaleDB only retains 2-3 days
- Creates edge case where gaps can't be backfilled
- Old data expired before gap detection runs

**Fix:** Either reduce gap lookback to 1 day OR increase retention to 7 days
```python
# Option 1: Reduce gap lookback
gaps = gap_detector.detect_gaps(symbol, days=1, max_gap_minutes=60)

# Option 2: Increase TimescaleDB retention (RECOMMENDED)
SELECT add_retention_policy('market_ticks', INTERVAL '7 days');
```
**Impact:** Prevents edge case data loss
**Effort:** 1 hour

---

### 3. Historical Bursts Starve Live Data ‚ö†Ô∏è MEDIUM
**Problem:** NATS FIFO queue processes historical messages before live ticks
- Live tick arrives during historical download
- Gets queued behind 200,000 historical messages
- 200-second delay for live data

**Fix:** Implement message prioritization
```python
# Separate subjects for live vs historical
if source == 'live':
    subject = f"market.live.{symbol}.{tf}"  # High priority
else:
    subject = f"market.historical.{symbol}.{tf}"  # Low priority
```
**Impact:** Live data never blocked by historical processing
**Effort:** 3 days

---

## Architecture Strengths

### What's Working Well ‚úÖ
1. **Resilient Design**
   - Disk buffer prevents data loss during NATS downtime
   - ClickHouse verification catches data-bridge crashes
   - Period tracker prevents duplicate downloads

2. **Scalable Infrastructure**
   - NATS has 20,000x capacity headroom (11M msg/sec vs 50k actual)
   - ClickHouse handles 1M inserts/sec (5M potential)
   - 3-month chunking prevents OOM during historical downloads

3. **Clean Architecture**
   - Clear separation of concerns (collect ‚Üí route ‚Üí persist ‚Üí aggregate)
   - Centralized configuration via Central Hub
   - Proper error handling with circuit breakers

4. **Gap Detection**
   - Comprehensive gap detection finds missing data
   - Automatic backfill repairs holes
   - Multi-layer verification (period tracker + ClickHouse)

---

## Scalability Assessment

### Can It Handle 10 Years of Historical Data?

**Answer:** YES, with Phase 1-2 fixes

**Calculation:**
```
Current: 2.8 years = 11 chunks √ó 7.3 hours = ~80 hours (3.3 days)
10 years: 40 chunks √ó 7.3 hours = ~292 hours (12 days)

With 3x data-bridge scaling:
10 years: 292 hours / 3 = ~97 hours (4 days) ‚úÖ
```

**Bottleneck Analysis:**

| Component | Current | Capacity | 10-Year Load | Status |
|-----------|---------|----------|--------------|--------|
| NATS Server | 50k/sec | 11M/sec | 1M/sec | ‚úÖ OK (11x headroom) |
| Data-bridge | 1k/sec | 10k/sec | 1M/sec burst | ‚ö†Ô∏è BOTTLENECK |
| TimescaleDB | 100/sec | 100k/sec | 500k/sec | ‚úÖ OK (200x headroom) |
| ClickHouse | 500/sec | 1M/sec | 5M/sec | ‚úÖ OK (2000x headroom) |

**Verdict:** Data-bridge is ONLY bottleneck, easily fixed by horizontal scaling

---

## Performance Analysis

### Latency Breakdown (Live Data)
```
Polygon API ‚Üí Live Collector: 50ms (network)
    ‚Üì
Live Collector ‚Üí NATS: <1ms (publish)
    ‚Üì
NATS ‚Üí Data-bridge: <1ms (subscribe)
    ‚Üì
Data-bridge batch wait: 2400ms ‚Üê BOTTLENECK
    ‚Üì
Database insert: 30ms
    ‚Üì
TOTAL: ~2500ms (2.5 seconds)
```

**Optimization:** Reduce batch size for live data
```python
# Dynamic batching
batch_size = 10 if source == 'live' else 500
# New latency: 50ms + 1ms + 1ms + 100ms + 30ms = 182ms ‚úÖ
```

### Throughput (Historical Backfill)
```
3-month chunk: 1,814,400 bars (14 symbols)
Download time: 7.3 hours per chunk
Publisher rate: 250,000 bars/hour (limited by Polygon API)

‚úÖ OPTIMAL: 3-month chunks balance RAM (1.8 GB) vs API efficiency
```

---

## Priority Roadmap

### Phase 1: Critical Fixes (Week 1-2) - MUST DO BEFORE 10-YEAR DOWNLOAD

#### Fix #1: Scale Data-bridge (P0 - CRITICAL)
```yaml
# docker-compose.yml
data-bridge:
  deploy:
    replicas: 3

# Add NATS queue group
await nats.subscribe("market.>", queue="data-bridge-workers", cb=handler)
```
**Impact:** 3x throughput, eliminates slow consumer warnings
**Effort:** 2 days

#### Fix #2: Fix TimescaleDB Retention (P0 - CRITICAL)
```sql
SELECT add_retention_policy('market_ticks', INTERVAL '7 days');
```
**Impact:** Prevents edge case data loss
**Effort:** 1 hour

#### Fix #3: Dynamic Batch Sizing (P1 - HIGH)
```python
batch_size = 10 if message['_source'] == 'live' else 500
```
**Impact:** 10x lower latency for live data (2.5s ‚Üí 0.2s)
**Effort:** 4 hours

---

### Phase 2: Performance (Week 3-4) - RECOMMENDED

#### Fix #4: Implement Backpressure (P1 - HIGH)
```python
# Publisher checks if consumer is falling behind
response = await nats.request(subject, payload, timeout=5)
if response.data != b"ACK":
    await asyncio.sleep(0.1)  # Slow down
```
**Impact:** Prevents queue buildup during bursts
**Effort:** 2 days

#### Fix #5: Message Prioritization (P2 - MEDIUM)
```python
# Separate subjects for live vs historical
subject = f"market.live.{symbol}.{tf}" if live else f"market.historical.{symbol}.{tf}"
```
**Impact:** Live data never blocked by historical processing
**Effort:** 3 days

---

### Phase 3: Observability (Week 5-6) - NICE TO HAVE

#### Add Prometheus Metrics
```python
nats_publish_total = Counter('nats_publish_total', 'Total publishes')
nats_queue_depth = Gauge('nats_queue_depth', 'Queue depth')
db_insert_latency = Histogram('db_insert_latency', 'DB latency')
```
**Impact:** Proactive monitoring, early problem detection
**Effort:** 2 days

---

## Key Recommendations

### Immediate Actions (Before 10-Year Download)
1. ‚úÖ **Scale data-bridge to 3 instances** (MUST DO)
2. ‚úÖ **Increase TimescaleDB retention to 7 days** (MUST DO)
3. ‚úÖ **Implement dynamic batch sizing** (RECOMMENDED)

### Architecture Improvements
1. ‚úÖ **Add backpressure mechanism** (prevents queue buildup)
2. ‚úÖ **Implement message prioritization** (live data never starved)
3. ‚úÖ **Add Prometheus metrics** (observability)

### Configuration Changes
```yaml
# TimescaleDB retention
retention: 7 days  # Was 2-3 days

# Gap detection
lookback_days: 1  # Was 30 days

# Data-bridge batch
tick_batch_size: 100  # Was 1000 (for live data)

# Gap check frequency
interval_hours: 0.25  # Was 1 hour (15-minute checks)
```

---

## Risk Assessment

### Without Fixes
- üî¥ **High Risk:** Slow consumer warnings will escalate during 10-year download
- üî¥ **High Risk:** Live data delayed by 200+ seconds during historical bursts
- üü° **Medium Risk:** Edge case data loss due to retention mismatch
- üü° **Medium Risk:** Single point of failure (data-bridge crash = data loss)

### With Phase 1 Fixes
- üü¢ **Low Risk:** System handles 10-year download without issues
- üü¢ **Low Risk:** Live data latency <1 second even during bursts
- üü¢ **Low Risk:** No data loss from retention conflicts
- üü° **Medium Risk:** Still single point of failure (mitigated by 3 instances)

### With Phase 1-2 Fixes
- üü¢ **Low Risk:** Production-ready, resilient, scalable
- üü¢ **Low Risk:** Can handle 100+ symbols, 10+ years historical
- üü¢ **Low Risk:** Sub-second latency for live data
- üü¢ **Low Risk:** Automatic recovery from failures

---

## Conclusion

**Current State:** System works well for current load (2.8 years, 14 symbols)

**Scalability:** Can handle 10+ years with P0-P1 fixes (3x data-bridge + config tweaks)

**Recommended Action:**
1. Implement P0 fixes (2 days effort)
2. Start 10-year historical download
3. Monitor metrics during download
4. Implement P1-P2 fixes if issues arise

**Timeline:**
- Week 1: P0 fixes (2 days) + start 10-year download (4 days runtime)
- Week 2: Monitor download, implement P1 fixes (3 days)
- Week 3-4: P2 optimizations (optional, based on monitoring)

**Confidence Level:** HIGH
With Phase 1 fixes, system is production-ready for 10+ year historical data ingestion.

---

**Review Status:** ‚úÖ APPROVED WITH CONDITIONS
**Next Review:** After P0 fixes implementation + 10-year download completion

---

## Appendix: Quick Reference

### System Metrics
- **Current Load:** 40 msg/sec live, 50k msg/sec historical bursts
- **Target Load:** 500 msg/sec live, 1M msg/sec historical sustained
- **Capacity Headroom:** 20,000x NATS, 200x TimescaleDB, 2000x ClickHouse

### Key Files to Modify
```
Phase 1 (P0 Fixes):
- docker-compose.yml (scale data-bridge)
- data-bridge/src/main.py (NATS queue group)
- TimescaleDB init script (retention policy)
- main.py line 651 (gap lookback days)

Phase 2 (P1 Fixes):
- publisher.py (dynamic batch sizing)
- publisher.py (backpressure mechanism)
- publisher.py (priority subjects)
```

### Monitoring Queries
```bash
# Check NATS slow consumers
curl http://localhost:8222/varz | grep slow_consumers

# Check data-bridge throughput
docker logs suho-data-bridge | grep "Published.*messages"

# Check ClickHouse data count
clickhouse-client -q "SELECT count(*) FROM aggregates WHERE source='polygon_historical'"
```

---

**Document Version:** 1.0
**Author:** System Architecture Designer
**Classification:** Internal Review
**Distribution:** Technical Team

